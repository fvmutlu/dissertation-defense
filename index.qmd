---
title: "Cost-aware Joint Caching and Forwarding in Networks with Diverse Cache Resources"
subtitle: "Faruk Volkan Mutlu - PhD Dissertation Defense Presentation"
author: "Research Advisor: Edmund Yeh<br>Committee Members: Elif Uysal, Stratis Ioannidis"
---

## Outline

- Primary Contribution
    -   **Introduction**: Motivation, Challenges, Related Work
    -   **Technical**: System Model, Optimization Framework
    -   **Practical**: Strategy, Experiments, Results
    -   **Proposed Work**
- Other Contributions
- Conclusion and Acknowledgements

::: {.footer}
Outline
:::

::: {.hidden}
$$
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\nin}{n \in \mc{N}}
\newcommand{\kin}{k \in \mc{K}}
\newcommand{\jin}{j \in \mc{J}_n}
\newcommand{\knt}{^k_n(t)}
\newcommand{\knjt}{^k_{n_j}(t)}
\newcommand{\kjin}{(k,j) \in \mc{B}_{n,i}}
\newcommand{\iin}{i \in \mc{I}_n}
\newcommand{\ain}{a \in \mc{N}}
\newcommand{\bin}{b \in \mc{N}}
\newcommand{\abin}{(a,b) \in \mc{L}}
\newcommand{\about}{(a,b) \not\in \mc{L}^k}
\newcommand{\betasum}{\sum\limits ^{\sigma_n}_{i=1} \beta_{n,i}}
\newcommand{\betasumnl}{\sum ^{\sigma_n}_{i=1} \beta_{n,i}}
\newcommand{\betasumind}{\sum\limits ^{\sigma_n}_{i=1} \beta_{n,i}\mathbf{1}_{[\kjin]}}
\newcommand{\minpen}{\Psi(\boldsymbol{\lambda})}
\newcommand{\drift}{\Delta(\mathbf{V}(t))}
\newcommand{\norm}[1]{\lVert{#1}\rVert^2_2}
\newcommand{\pen}{\mathbb{E}[p(t)|\mathbf{V}(t)]}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\definecolor{neured}{RGB}{200, 16, 46}
$$
:::

# Introduction

{{< include _intro.qmd >}}

# System Model

::: {.notes}
That concludes the introduction and leads me into the system model that we're dealing with.
:::

{{< include _model.qmd >}}

# Virtual Plane Optimization

::: {.notes}
With the system model described, we will now look at our control algorithm in the virtual plane.
:::

## Optimization Goal
*"Operate close to VIP network stability region boundary while keeping sum penalty small."*

::: {.fragment fragment-index=1}
Minimize upper bound of [drift-plus-penalty]{.body-highlight}[^dpp] expression:
```{=tex}
\begin{equation}
    \drift + \omega \pen
\end{equation}
```
```{=tex}
\begin{equation}
    \drift \triangleq \mathbb{E}[\mathcal{L}(\mathbf{V}(t+1))-\mc{L}(\mathbf{V}(t))|\mathbf{V}(t)]
\end{equation}
```
```{=tex}
\begin{equation}
    \mc{L}(\mathbf{V}(t)) \triangleq \sum\limits_{\nin, \kin} (V^k_n(t))^2
\end{equation}
```
:::

[^dpp]: ["Energy optimal control for time-varying wireless networks.", M. J. Neely, IEEE Trans. Inf. Theory, 2006.]{.fragment fragment-index=1}

::: {.footer}
Optimization Framework
:::

::: {.notes}
- The goal of our optimization in the virtual plane, is to operate as close as possible to the boundary of the stability region of the network of VIP queues, which we will define formally in a bit, while keeping penalties accumulated small.
- (Fragment in) In particular, we're trying to minimize the upper bound on this drift-plus-penalty expression you see on screen, which is based on an extension to Lyapunov optimization devised by Neely.
- So in this way, we're not necessarily trying to make a precisely optimal decision each time slot, but trying to minimize the expected value of an upper bound.
:::

## Virtual Plane Caching
Beginning of each slot $t$, at each node $n$, observe $(V^k_n(t))_{k \in \mathcal{K}, n \in \mathcal{N}}$ and perform [caching]{.body-highlight} by choosing $s^k_{n_j}(t)$ for each $\kin$ and each $j \in \mathcal{J}_n$ with capacity $L_{n_j}$ to:

::: {style="font-size: 80%;"}
```{=tex}
\begin{align}
    \text{maximize}
        & \quad \sum\limits_{\kin} \sum\limits_{\jin} r_{n_j} V^k_{n}(t) s^k_{n_j}(t) - \omega p^k_{n_j}(t) \\
    \text{subject to}
        & \quad \sum\limits_{\kin} s^k_{n_j}(t) \leq L_{n_j}, \; \forall \, \jin \\
        & \quad \sum\limits_{\jin} s^k_{n_j}(t) \leq 1, \; \forall \, \kin \\
        & \quad s^k_{n_j}(t) \in \{0, 1\}, \; \forall \, \kin, \; \jin
\end{align}
```
:::

::: {.footer}
Optimization Framework
:::

::: {.notes}
- And here's how we tackle this optimization. We devise an algorithm that's made of two steps (caching and forwarding), which chooses the respective variables every time slot, to optimize a certain objective based on the VIP counts. We're looking at the caching step right now.
- Obviously there is a lot of math to get to this expression for the objective function, but I'll give you the intuition briefly.
- Because we want to remove as many VIPs from the network as fast as possible, we want to cache the higher VIP count objects in the faster tiers, which is why the read rate multiplies the VIP count here.
- However, we also want to make sure that our choices in caching actions do not add large penalties that outweigh the benefits. Of course the importance of that is determined by omega.
- And of course we have the cache tier capacity constraints and the cache exclusivity constraints
:::

## Virtual Plane Forwarding
Perform [forwarding]{.body-highlight} as in (Yeh, 2014), by choosing $\mu^k_{ab}(t)$ for each $\kin$ and $\abin^k$:

::: {style="font-size: 70%;"}
```{=tex}
\begin{equation}
   \mu^k_{ab}(t) =
   \left\{ \begin{array}{ll}
      C_{ba}, & \text{if} \; k = k^*_{ab}(t) \; \text{and} \; W^k_{ab}(t) > 0 \\
      0, & \text{otherwise}
   \end{array} \right.
\end{equation}
```
```{=tex}
\begin{equation}
\begin{split}
   W^k_{ab}(t) & \triangleq V^k_a(t) - V^k_b(t), \\
   k^*_{ab}(t) & \triangleq \argmax\limits_{\{k: \abin^k\}} W^k_{ab}(t)
\end{split}
\end{equation}
```
:::
<hr>

::: columns
::: {.column width="50%" style="font-size: 70%;"}
$C_{ab}$: Capacity of link $(a,b)$
:::
::: {.column width="50%" style="font-size: 70%;"}
$\mc{L}^k$: Set of links allowed to transmit VIPs of $k$, determined by routing policy
:::
:::

::: {.footer}
Optimization Framework
:::

::: {.notes}
- And here's the forwarding step. The neat thing about our extension here is that we don't actually need to alter the results for forwarding from the original VIP paper, which were based on the backpressure technique.
- The intuition here again comes from thinking of VIP counts as potential representing unsatisfied demand. So with the forwarding step, we're making sure VIP flows follow large differences in potential, essentially steering interests toward sinks, meaning sources or caches.
:::

## Theorem: Stability Region

VIP [stability region]{.body-highlight} $\Lambda$ consists of all arrival rates $\boldsymbol{\lambda} = (\lambda^k_n)_{\kin,\nin}$ such that:

::: {style="font-size: 80%;"}
```{=tex}
\begin{equation}
    \lambda^k_n \leq \sum\limits_{\bin} f^k_{nb} - \sum\limits_{\ain} f^k_{an} + \sum\limits_{\jin} r_{n_j} \betasumind
\end{equation}
```
:::
<hr>

::: {style="font-size: 70%; text-align: center"}
$f^k_{ab}$: Time-average VIP flow for $k$ over $(a,b)$
:::

::: columns
::: {.column width="50%" style="font-size: 70%;"}
$\mc{B}_{n,i}$: i-th among all $\sigma_n$ possible cache placement sets at $n$; if $(k,j) \in \mc{B}_{n,i}$ during $t$, $s^k_{n_j}(t)$ = 1
:::
::: {.column width="50%" style="font-size: 70%;"}
$\beta_{n,i}$: Fraction of time objects at $n$ are placed according to $\mc{B}_{n,i}$; $0 \leq \beta_{n,i} \leq 1$ and $\sum^{\sigma_n}_{i=1} \beta_{n,i} = 1$
:::
:::

::: {.footer}
Optimization Framework
:::

::: {.notes}
- The analysis of our approach comes in two steps. We're following the structure of Lyapunov drift-based analysis here, so we first define the stability region of the virtual plane network.
- The inequality on the screen essentially defines the boundary of that stability region, relating the arrival rates that can be satisfied to the long term average VIP flow over links, as well as VIPs removed via cache tiers.
:::

## Theorem: Trade-off {.smaller}

For arrival rate vector $\boldsymbol{\lambda}$, if there exists $\boldsymbol{\epsilon}$ such that $\boldsymbol{\lambda} + \boldsymbol{\epsilon} \in \Lambda$, then the network of VIP queues under the proposed algorithm satisfies the following:

::: {style="font-size: 110%;"}
```{=tex}
\begin{equation}
    \lim\limits_{T \rightarrow \infty} \frac{1}{T} \sum\limits^{T-1}_{t=0} \sum\limits_{\nin, \kin} \mathbb{E}[V^k_n(t)] \leq \frac{NB}{\epsilon} + \frac{\omega}{2\epsilon} \minpen
\end{equation}
```
```{=tex}
\begin{equation}
    \lim\limits_{T \rightarrow \infty} \frac{1}{T}\sum\limits^{T-1}_{t=0} \mathbb{E}[p(t)] \leq \frac{2NB}{\omega} + \minpen
\end{equation}
```
:::

::: {.fragment style="font-size: 80%;"}
```{=tex}
\begin{equation}
    B \triangleq \frac{1}{2N} \sum_{\nin} \bigg((\mu^{out}_{n,max})^2 + 2(\mu^{out}_{n,max})r_{n,max} + \big(\textstyle \sum_{\kin}A^k_{n,max} + \mu^{in}_{n,max} + r_{n,max})^2 \bigg)
\end{equation}
```
```{=tex}
\begin{equation}
\epsilon \triangleq \textstyle \min_{\nin,\kin} \epsilon^k_n
\end{equation}
```

$\Psi(\boldsymbol{\lambda})$: Minimum time-average sum penalty achievable by a feasible and stabilizing randomized policy.
:::

::: {.footer}
Optimization Framework
:::

::: {.notes}
- The second step of our analysis is what actually shows the guarantees that our algorithm can provide. Here, we use the definition of the stability region, which is why we needed the previous step.
- These two expressions define the upper bounds for time average VIP queue sizes and accumulated penalty in the virtual plane network.
- The essential observation here is the trade-off between the bounds. As you can see, the omega value controls which bound we wish to prioritize, because setting it small means lower average queue sizes which translates to better performance, but at the cost of potentially much larger penalties. Setting it large is the inverse of that situation.
:::

## Lemma: Caching Solution
We can rewrite [caching]{.body-highlight} step optimization problem.

::: {style="font-size: 100%;"}
```{=tex}
\begin{align}
    \text{maximize}
        & \quad \sum\limits_{\kin} \sum\limits_{\jin} r_{n_j} V^k_{n}(t) s^k_{n_j}(t) - \omega p^k_{n_j}(t) \\
    \text{subject to}
        & \quad \sum\limits_{\kin} s^k_{n_j}(t) \leq L_{n_j}, \; \forall \, \jin \\
        & \quad \sum\limits_{\jin} s^k_{n_j}(t) \leq 1, \; \forall \, \kin \\
        & \quad s^k_{n_j}(t) \in \{0, 1\}, \; \forall \, \kin, \; \jin
\end{align}
```
:::

::: {.footer}
Optimization Framework
:::

::: {.notes}
- Now of course, the validity of our approach hinges on whether we can solve the optimization problem from the caching step.
- To do so, we will first rewrite it to clean up this unpleasant form we've shown before. And to do that, we will use a two-step transformation of variables.
:::

## Lemma: Caching Solution
We can rewrite [caching]{.body-highlight} step optimization problem.

::: {style="font-size: 80%;"}
```{=tex}
\begin{align}
    \text{maximize}
        & \quad \sum\limits_{\kin} \sum\limits_{\jin} b^k_{n_j}(t) s^k_{n_j}(t) \\
    \text{subject to}
        & \quad \sum\limits_{\kin} s^k_{n_j}(t) \leq L_{n_j}, \; \forall \, \jin \\
        & \quad \sum\limits_{\jin} s^k_{n_j}(t) \leq 1, \; \forall \, \kin \\
        & \quad s^k_{n_j}(t) \in \{0, 1\}, \; \forall \, \kin, \; \jin
\end{align}
```
```{=tex}
\begin{equation}
    b^k_{n_j}(t) \triangleq \left\{ \begin{array}{ll}
        r_{n_j} V^k_n(t) - \omega c^a_{n_j}, & \text{if} \; s^k_{n_j}(t-1) = 0 \\
        r_{n_j} V^k_n(t) + \omega c^e_{n_j}, & \text{if} \; s^k_{n_j}(t-1) = 1
    \end{array} \right.
\end{equation}
```
:::

::: {.footer}
Optimization Framework
:::

::: {.notes}
- The first step here is defining an auxiliary "benefit" term and using that to simplify the objective function.
- You'll notice that the problem now looks to be in the form of a generalized assignment problem.
:::

## Lemma: Caching Solution
Use equal object sizes, decide variables for each one-object [cache slot]{.body-highlight}. Rewrite problem as [linear assignment problem]{.body-highlight}. 

::: {style="font-size: 70%;"}
```{=tex}
\begin{align}
    \text{maximize}
        & \quad \sum\limits_{\kin} \sum\limits_{\iin} b^k_{n_i}(t) s^k_{n_i}(t) \\
    \text{subject to} 
        & \quad \sum\limits_{\kin} s^k_{n_i}(t) \leq 1, \; \forall \, \iin \\
        & \quad \sum\limits_{\iin} s^k_{n_i}(t) \leq 1, \; \forall \, \kin \\
        & \quad s^k_{n_i}(t) \in \{0, 1\}, \; \forall \, \kin, \; \iin
\end{align}
```
:::

::: {style="font-size: 60%;"}
```{=tex}
\begin{equation}
\mc{I}_n \triangleq \{1, 2, ..., \sum_{\jin} L_{n_j}\}, \; \mc{I}_{n_j} \triangleq \{1 + \sum^{j - 1}_{\ell = 1} L_{n_\ell}, ..., \sum^{j}_{\ell = 1} L_{n_\ell}\}
\end{equation}
```

If $i \in \mc{I}_{n_j}$, then $b^k_{n_j}(t) = b^k_{n_i}(t)$ and $s^k_{n_j}(t) = s^k_{n_i}(t)$.
:::

::: {.footer}
Optimization Framework
:::

::: {.notes}
- Now recall the assumption we made in our model that every data object has the same size.
- Using that assumption, we can think of each cache tier as made up of several "cache slots" that can each take one object. The number of these per tier depend on the total capacity of that tier. These slots have the same read rate and cost parameters as the tier they belong in.
- Now, also recall the cache exclusivity constraint of the original problem. We can easily see that exclusivity across tiers directly translates to exclusivity across these cache slots.
- Using these facts we can rewrite the problem again, this time in the form of a linear assignment problem.
- Luckily, there are methods that can solve this problem in polynomial time. We specifically rely on a modification of the Jonker-Volgenant algorithm.
:::

{{< include _dp.qmd >}}

# Experiments

::: {.notes}
- Alright, so how does our strategy fare in experimental evaluations
:::

## Simulation Setup

::: {.incremental}
-  Object-level, DES framework built with Python using SimPy library
-  Modular design to allow future extension
-  Thread-per-experiment multi-threading
-  Detailed statistic logging
-  Parameter and output data in JSON format for easy parsing
:::

::: {.footer}
Experiments
:::

::: {.notes}
- Firstly, when I say experiments here I strictly mean simulation experiments
- I built a simulator from scratch for this purpose, which took a bit of time and care
- (Talk about bullet points)
- I needed to build this because existing simulators didn't cover what I needed, and modifying them would've taken even more time
- Of course this is also far from perfect, but I think it turned out to be something very useful and flexible
:::

## Topologies {.smaller}

::: columns
::: {.column width="50%"}

![Abilene (11 nodes)](images/abilene.svg)

![GEANT (34 nodes)](images/geant.svg)

:::
::: {.column width="50%"}

![4x4 Grid (16 nodes)](images/grid.svg)

![3-Regular (50 nodes)](images/3reg.svg)

:::
:::

::: {.footer}
Experiments
:::

::: {.notes}
- Alright, so when it comes to experiment settings, I first have to stress that the experimentation space for this work is enormous
- So the results I'll show here are those that I think most succinctly show that our approach can deliver on our goals
- To start off, these are the four topologies I conducted experiments over
- On the left are Abilene and Geant which are real network topologies, and on the right are two stylized graphs
:::

## Experiment Setting

::: {.incremental}
-  $\mc{S}(k)$ are selected uniformly at random
-  Routing: *Any* shortest path (in number of hops) between $n$ and $\mc{S}(k)$
-  Two cache tiers at every node
-  Requests generated for 100 simulation seconds, run terminates when all requests are resolved
-  Virtual plane algorithm slot length of 1 second, sliding window of size 100 slots
:::

::: {.footer}
Experiments
:::

::: {.notes}
- There are some additional assumptions that pertain to the experiment setting so let's go over those
:::

## Baselines

::: {.incremental}
-  Adapted replacements: LRU, LFU, FIFO, RANDOM
   -  LRU, FIFO and RANDOM are cost-unaware, paired with Leave Copy Everywhere admission
   -  LFU admission based on frequency
      - Cost-aware LFU adaptation uses similar benefit metric
-  All baseline caching policies paired with Least Response Time forwarding
:::

::: {.footer}
Experiments
:::

::: {.notes}
- Because there aren't really any policies, novel or established, we could directly compare against, we adapted our own baselines from well-known single-cache policies
- We adapted LRU, FIFO and RANDOM replacement but for these there isn't a good metric to incorporate costs, so those are cost-unaware and for admission they're paired with the basic Leave Copy Everywhere policy
- We adapted LFU similarly, but also made a cost-aware LFU adaptation which uses a cache benefit metric similar to the one we devised for our aproach
- Least response time forwarding simply chooses the link with smallest delay for last satisfied request
:::

## Parameter Reference Table {.smaller}

| Parameter | Description | Value |
|:-:|:-:|:-:|
|$K$|Number of objects in catalog $\mc{K}$|1000|
|$\sum_{\kin} \lambda^k_n$|Total request arrival rate at each $n$ in objects/sec|10|
|$\alpha$|Zipf's law parameter for object popularity distribution|0.75|
|$C_{ab}$|Capacity of each link $\abin$, in objects|10|
|$L_{n_1}$|Capacity of tier 1 at each $n$, in objects|5|
|$r_{n_1}$|Read rate of tier 1 at each $n$, in objects/sec|20|
|$c^a_{n_1}$, $c^e_{n_1}$| Admission and eviction costs of tier 1 at each $n$|4, 2|
|$r_{n_2}$|Read rate of tier 2 at each $n$, in objects/sec|10|
|$c^a_{n_2}$, $c^e_{n_2}$| Admission and eviction costs of tier 2 at each $n$|2, 1|
: Common parameters for all scenarios {tbl-colwidths="[10,80,10]"}

::: {.footer}
Experiments
:::

::: {.notes}
- As I said, there are a great number of parameters for these experiments and I'm not going to cover each one in the interest of time, but we have this table for reference if needed
:::

## Results I {.smaller}
**Scenario**: No penalties ($\omega=0$), $L_{n_1} = 5$ and $L_{n_2} = 100$ at each $n$.

![&emsp; Total delay, as fraction of total delay without any caching](images/tops_delay_v3.svg){width=75% height=75% fig-align="center"}

![Cache hits, percentage of hits in the first tier on the left, total number of hits on the right](images/tops_hits_v3.svg){width=75% height=75% fig-align="center"}

::: {.footer}
Experiments
:::

::: {.notes}
- First, I'll show results from a scenario where we actually ignore penalties, which we can easily do for our approach by setting omega to be zero
- We're comparing our strategy to cost-unaware baselines here and we're just concerned about the capacity challenge
- The figure up top shows the total delay in the network, normalized to the total delay achieved without any caching
- This figure illustrates our point about how managing larger caches is difficult, because as you can see, the "naive" policies are struggling quite a bit, at times showing worse performance than would be achieved with no caches at all
- Our VIP-based approach performs the best across all experiments, though LFU is not that far behind
- The bottom figure reveals more insights since it shows the distribution of cache hits among the two tiers as well as total cache hits
- Now note the cache capacities up top, the difference is 20-fold
- However, with our approach, anywhere between 15% to 20% of cache hits are occurring on the first tier, much larger than other baselines, showing that we're able to balance the amount of cache hits better
- You'll notice in most cases our approach doesn't even have the highest total number of cache hits, and that's because it is set up to handle the rate of cache hits properly
:::

## Results II {.smaller}
**Scenario**: Cost-unaware policies excluded, $\omega > 0$. $L_{n_1}=5$, best value of $L_{n_2}$ picked.

::: {.body-centered}
Delay vs. penalty ($\omega$ decreases to the right)
:::

::: columns
::: {.column width="50%"}
![&emsp;&emsp; Abilene](images/pen_vs_delay_abilene_v2.svg)
:::
::: {.column width="50%"}
![&emsp;&emsp;&emsp;&emsp; 4x4 Grid](images/pen_vs_delay_grid_v2.svg)
:::
:::

::: {.footer}
Experiments
:::

::: {.notes}
- Now we move on to a scenario with penalties, dropping the "naive" policies and keeping only the cost-aware adaptation of LFU as a baseline
- We'll be looking at how each policy balances the delay vs. penalty trade-off; total delay on the y-axis, total penalty on the x-axis
- Now I want to point out here that different second tier capacities are used for the two policies. I experimented with a range of values for each and picked the best performing round value.
- The reason for this connects to how well policies can actually use the larger capacities. As we saw from the last slide, not every policy can handle the same cache capacity as well.
- The gist is that I want to represent the best of the competing policy and show that we're still ahead.
- As we can see, in the Abilene and Grid topologies our approach is generally outperforming the adapted LFU
:::

## Results II {.smaller}
**Scenario**: Cost-unaware policies excluded, $\omega > 0$. Best value of $L_{n_2}$ picked.

::: {.body-centered}
Delay vs. penalty ($\omega$ decreases to the right)
:::

::: columns
::: {.column width="50%"}
![&emsp;&emsp;&emsp; GEANT](images/pen_vs_delay_geant_v2.svg)
:::
::: {.column width="50%"}
![&emsp;&emsp;&emsp; 3-Regular](images/pen_vs_delay_regular_v2.svg)
:::
:::

::: {.footer}
Experiments
:::

::: {.notes}
- The situation is similar for the Geant and 3-regular topologies as well, though in the latter, there is a small operating region where LFU overtakes our approach
:::

# Cache Read Rate Control

{{< include _sbw.qmd >}}

# Proposed Work

## Improvements

There is still room for improvement:

::: {.fragment}
![](images/big-room.webp){width=65% height=65% fig-align="center"}

... and it's a big room
:::

::: {.footer}
Super Funny Joke
:::

::: {.notes}
- So as much as I'd like to say I'll be pursuing a shiny new goal for the remainder of my time here, there are still aspects of this work that needs to be covered and improved
- So this section will outline those improvements I'm planning on pursuing
:::

## Improvements

There is still room for improvement:

::: {.incremental}
- Modeling of cache read rates
- Equal object sizes assumption
- Virtual plane - data plane gap
- Cache exclusivity constraint
- Better scalable simulator
:::

::: {.footer}
Proposed Work
:::

## Cache Read Bandwidth
Model assumes read rate of each object is independent

::: {style="font-size: 70%;"}
```{=tex}
\begin{equation}
    V^k_n(t+1) \leq \Bigg(\Big(V^k_n(t) - \sum\limits_{b \in \mathcal{N}}\mu^k_{nb}(t)\Big)^+ + A^k_n(t)
    + \sum\limits_{a \in \mathcal{N}}\mu^k_{an}(t) - \sum\limits_{j \in \mathcal{J}_n} \color{#C8102E}{r_{n_j}} s^k_{n_j}(t) \Bigg)^+
\end{equation}
```
:::

::: {.footer}
Proposed Work
:::

::: {.notes}
- We treat the read rate parameter as if it is independent for each object, which indicates that there is a guaranteeable rate that can be provided per object, no matter how many objects are cached in that tier
- This is an okay assumption, but given the strict rate limitations we're working with, is not great
:::

## Cache Read Bandwidth
Model assumes read rate of each object is independent

::: {style="font-size: 70%;"}
```{=tex}
\begin{equation}
    \color{#C0C0C0}{V^k_n(t+1) \leq \Bigg(\Big(V^k_n(t) - \sum\limits_{b \in \mathcal{N}}\mu^k_{nb}(t)\Big)^+ + A^k_n(t)
    + \sum\limits_{a \in \mathcal{N}}\mu^k_{an}(t) - \sum\limits_{j \in \mathcal{J}_n}} \color{#C8102E}{r_{n_j}} \color{#C0C0C0}{s^k_{n_j}(t) \Bigg)^+}    
\end{equation}
```
:::

Need to be more accurate for larger and slower caches

::: {style="font-size: 70%;"}
```{=tex}
\begin{equation}
   \color{#C0C0C0}{V^k_n(t+1) \leq \Bigg(\Big(V^k_n(t) - \sum\limits_{b \in \mathcal{N}}\mu^k_{nb}(t)\Big)^+ + A^k_n(t)
   + \sum\limits_{a \in \mathcal{N}}\mu^k_{an}(t) - \sum\limits_{j \in \mathcal{J}_n}} \color{#C8102E}{r^k_{n_j}(t)} \color{#C0C0C0}{s^k_{n_j}(t) \Bigg)^+}
\end{equation}
```
:::

```{=tex}
\begin{equation}
   \sum_{\kin} r^k_{n_j}(t) \leq r_{n_j}   
\end{equation}
```

::: {.footer}
Proposed Work
:::

::: {.notes}
- The way we'll do that is by enforcing a cache read bandwidth constraint
- Additional control action each time slot, in product form with another control action
- Stability region and its analysis changes significantly
- Complexity of caching problem solution reduces
:::

## Different Object Sizes

::: {.incremental}
- Equal object sizes assumption enables polynomial-time solution, but is not realistic
- Without it, we'll need to solve a GAP (NP-hard)
- Need to reevaluate how we treat VIP counts:
   -  VIPs for smaller objects can be drained faster
   -  Service may prioritize smaller objects
:::

::: {.footer}
Proposed Work
:::

## Virtual Plane - Data Plane Gap {.smaller}

::: {.fragment}
![... oops, didn't see you there](images/elephant.webp){width=65% height=65% fig-align="center"}

<audio data-autoplay id="elephant" src="audio/elephant.mp3"></audio>

<script>
  var audio = document.getElementById("elephant");
  audio.volume = 0.1;
</script>
:::

::: {.footer}
Super Funny Callback
:::

::: {.notes}
- So this is the elephant in our room for improvement
:::

## Virtual Plane - Data Plane Gap

Disparities between the planes lead to a gap:

::: {.incremental}
-   Virtual plane assumes nodes can immediately cache any object
-   [Upward migrations]{.body-highlight} not possible in data plane
-   Data plane doesn't allow [redirecting]{.body-highlight} of cache hits
-   [Interest aggregation]{.body-highlight} extends this gap
-   Amending the gap is difficult, but it can be quantified
:::

::: {.footer}
I also wanted to make "mind the gap" joke but there wasn't a good place for it
:::

## Cache Exclusivity

This constraint is reasonable, but very restrictive:

-  Duplicates across tiers could extend bandwidth (SPDK, peer-to-peer DMA)
-  Minimal analytical implications, true impact can only be evaluated via experimentation

::: {.footer}
Proposed Work
:::

## Better Experiments

::: {.incremental}
-  Python is convenient but not highly-scalable for the task. Julia is a great alternative.
-  Simulator core should be multi-threaded to improve [scale-up]{.body-highlight}; per-experiment execution should [scale-out]{.body-highlight}.
-  [Extendibility]{.body-highlight} and [reproducibility]{.body-highlight} can still be improved.
:::

::: {.footer}
Proposed Work
:::

# Conclusion & Other Work

## Conclusion

::: {.incremental}
-   We address a critical and practical problem in scaling high-throughput caching networks
-   Our proposed model and approach achieves our goals and performs decently, but has shortcomings
-   Some improvements can be made by reiterating on technical details, further fine tuning requires experimentation
:::

::: {.footer}
Conclusion & Other Work
:::

::: {.notes}
- I should also note that we submitted a paper on this to MobiHoc last year which was rejected
- We're currently revising the paper for the upcoming ICC
:::

{{< include _hetnet.qmd >}}

# Appendices {visibility="uncounted"}

## Appendix A: Pseudocode

::: {style="font-size: 50%;"}
```pseudocode
\begin{algorithm}
\caption{Quicksort}
\begin{algorithmic}
\Procedure{Quicksort}{$A, p, r$}
  \If{$p < r$}
    \State $q = $ \Call{Partition}{$A, p, r$}
    \State \Call{Quicksort}{$A, p, q - 1$}
    \State \Call{Quicksort}{$A, q + 1, r$}
  \EndIf
\EndProcedure
\Procedure{Partition}{$A, p, r$}
  \State $x = A[r]$
  \State $i = p - 1$
  \For{$j = p$ \To $r - 1$}
    \If{$A[j] < x$}
      \State $i = i + 1$
      \State exchange
      $A[i]$ with     $A[j]$
    \EndIf
    \State exchange $A[i]$ with $A[r]$
  \EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}
```
:::

::: {.footer}
Appendices
:::

## Appendix B: Model Notation {.smaller visibility="uncounted"}

| Notation | Definition |
|:--:|:--------|
|$\mathcal{G}$|Directed graph representing the network topology|
|$(\mathcal{N},\mathcal{L})$|Set of nodes and links in $\mathcal{G}$|
|$C_{ab}$|Transmission capacity (in objects/sec) of link $(a,b)$|
|$\mathcal{K}$|Set of data objects in the network|
|$\mc{S}(k)$|Content source node for $k \in \mathcal{K}$|
|$\mathcal{J}_n$ | Set of cache tiers at node $n \in \mathcal{N}$|
|$L_{n_j}$ | Size (in objects) of cache tier $j \in \mathcal{J}_n$ at $n$|
|$r_{n_j}$ | Read rate of tier $j$ at node $n$|
|$c^a_{n_j}$| Admission cost of tier $j$ at node $n$|
|$c^e_{n_j}$| Eviction cost of tier $j$ at node $n$|
: Table of Notations {tbl-colwidths="[20,80]"}

::: {.footer}
Appendices
:::

## Appendix B: Model Notation {.smaller visibility="uncounted"}

| Notation | Definition |
|:--:|:--------|
|$\lambda^k_n$|Exogenous request arrival rate for $k$ at $n$|
|$t$|Time slot referring to time interval $[t, t+1)$|
|$A^k_n(t)$|Number of exogenous requests for $k$ at $n$ during $t$|
|$s^k_{n_j}(t)$ | Caching state of $k$ in tier $j$ at $n$ at the beginning of $t$|
|$p^k_{n_j}(t)$ | Penalty incurred by the choice of $s^k_{n_j}(t)$|
|$p(t)$|Sum penalty incurred during $t$|
|$\omega$|Penalty importance weight|
|$V^k_n(t)$|VIP count for $k$ at $n$ during $t$|
|$\mathbf{V}(t)$|Vector of VIP queue states during $t$|
|$\mu^k_{ab}(t)$|Allocated rate of VIPs for $k$ over $(a,b)$ during $t$|
: Table of Notations {tbl-colwidths="[20,80]"}

::: {.footer}
Appendices
:::

## Appendix C: Analysis Notation {.smaller visibility="uncounted"}

| Notation | Definition |
|:--:|:--------|
|$A^k_{n,max}$| Finite value such that $A^k_n(t) \leq A^k_{n,max}$ for all $t$|
|$A_{n,max}$| Maximum total exogenous arrivals at node $n$ during $t$ across all $\kin$, i.e. $A_{n,max} \triangleq \sum_{\kin}A^k_{n,max}$|
|$\mu^{out}_{n,max}$| Maximum total allocated transmission rate for VIPs across all $(n,b) \in \mc{L}$, i.e. $\mu^{out}_{n,max} \triangleq \sum_{\bin}C_{nb}$|
|$\mu^{in}_{n,max}$| Maximum total allocated transmission rate for VIPs across all $(a,n) \in \mc{L}$, i.e. $\mu^{in}_{n,max} \triangleq \sum_{\bin}C_{an}$|
|$r_{n,max}$| Maximum total rate that can be served by all cache tiers at $n$|
|$\Psi(\boldsymbol{\lambda})$|Minimum time-average sum penalty achievable by a feasible and stabilizing randomized policy|

::: {.footer}
Appendices
:::