---
title: "Cost-aware Joint Caching and Forwarding in Networks with Diverse Cache Resources"
subtitle: "<s>Farouk Multi</s> Faruk Volkan Mutlu - PhD Proposal Review"
author: "Research Advisor: Edmund Yeh<br>Committee Members: Elif Uysal, Stratis Ioannidis"
format:
  revealjs:
    slide-number: true
    theme: white
    logo: images/neu_logo.svg
    css: styles/styles.css
    highlight-style: atom-one
    header-includes: <link href='https://fonts.googleapis.com/css?family=Lato' rel='stylesheet'>
    include-in-header:
      - text: |
          <script>
          window.MathJax = {
            loader: {
              load: ['[tex]/color']
            },
            tex: {
              packages: {'[+]': ['color']}
            }
          };
          </script>
mainfont: Lato
fig-cap-location: top
---

## Outline

- General Context
- Primary Contribution and Proposed Work
    -   **Introduction**: Motivation, Challenges, Related Work
    -   **Technical**: System Model, Optimization Framework
    -   **Practical**: Strategy, Experiments, Results
    -   **Extensions**: Proposed Work, Out-of-scope Outlook*
- Other Contributions
- Conclusion and Acknowledgements

::: {.footer}
Outline
:::

::: {.hidden}
$$
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\nin}{n \in \mc{N}}
\newcommand{\kin}{k \in \mc{K}}
\newcommand{\jin}{j \in \mc{J}_n}
\newcommand{\kjin}{(k,j) \in \mc{B}_{n,i}}
\newcommand{\iin}{i \in \mc{I}_n}
\newcommand{\ain}{a \in \mc{N}}
\newcommand{\bin}{b \in \mc{N}}
\newcommand{\abin}{(a,b) \in \mc{L}}
\newcommand{\about}{(a,b) \not\in \mc{L}^k}
\newcommand{\betasum}{\sum\limits ^{\sigma_n}_{i=1} \beta_{n,i}}
\newcommand{\betasumnl}{\sum ^{\sigma_n}_{i=1} \beta_{n,i}}
\newcommand{\betasumind}{\sum\limits ^{\sigma_n}_{i=1} \beta_{n,i}\mathbf{1}_{[\kjin]}}
\newcommand{\minpen}{\Psi(\boldsymbol{\lambda})}
\newcommand{\drift}{\Delta(\mathbf{V}(t))}
\newcommand{\pen}{\mathbb{E}[p(t)|\mathbf{V}(t)]}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\definecolor{neured}{RGB}{200, 16, 46}
$$
:::

## Information Centric Networking

- **Motivation**: The Internet is primarily a data distribution network, yet its principles are those of a communication network
- **Premise**: Make uniquely named data the primary entity of the network, instead of endpoints
- **Advantages**: Improve efficiency and scalability by decoupling data from location and session

::: {.footer}
Context
:::

## In-network Caching

- **Motivation**: Current caching infrastructure is mostly proprietary and centralized
- **Premise**: Enable every router in the network to maintain its own cache
- **Advantages**: Caching becomes decentralized, reducing network congestion and improving scalability

::: {.footer}
Context
:::

# Introduction

## Overview {.smaller}

-  **Motivation**: Data volume grows exponentially, cache capacities are stagnant
-  **Challenge**: Enabling larger caches in a cost-effective way is difficult
   -  How beneficial are larger caches?
   -  Are slower and larger storage elements viable as caches?
   -  What are the costs of operating large caches?
   -  How can we get the most out of caches in the network?
-  **Goal**: Develop a policy that observes network traffic patterns and diverse cache device characteristics to make intelligent caching and forwarding decisions

::: {.footer}
Introduction
:::

## Use Case: Data-Intensive Science

::: {.incremental}
-   Data-intensive science experiments (high-energy physics, genomics etc.) process huge amounts of data
    -   In one year of LHC running, over an exabyte of data is accessed
    -   In November 2018, 15.8 petabytes of data were written on tape at CERN
-   Data is continuously distributed across the world for research
:::

::: {.footer}
Introduction
:::

## Use Case: Data-Intensive Science

::: {.incremental}
-   *"N-DISE: NDN-based Data Distribution for Large-Scale Data-Intensive Science"*, Y. Wu, F. V. Mutlu, et al.
    -   NDN-based high-throughput data distribution, caching and access system
    -   Each router allocated 20 gigabytes DRAM cache space
-   *"NDN-DPDK: NDN Forwarding at 100 Gbps on Commodity Hardware"*, J. Shi, D. Pesavento, L. Benmohamed
    -   Increasing capacities via NVMe SSDs require novel caching strategies due to their slow speeds
:::

::: {.footer}
Introduction
:::

## The Capacity Challenge

::: {.incremental}
-  DRAM is the primary cache device since it is fast, i.e. *line rate* (12.8-25.6 GB/s for DDR4, 32-64 GB/s for DDR5)
-  However, DRAM offers small cache sizes at a high and exponentially increasing cost ($3 to $12 / GB)
-  Next best storage element is the NVMe SSD, offering large cache sizes at significantly lower costs ($0.1 / GB)
-  NVMe SSDs can be much slower than DRAM (11.7 GB/s ideal read rate for fastest PCIe 5.0)
:::

::: {.footer}
Introduction
:::

## Toy Example {.smaller}

![](images/toynet.svg){width=60% height=60% fig-align="center"}

-  Catalog of 1000 objects, ranked in popularity according to Zipf's law
-  Forwarder responds to requests from cache or forwards them to server
-  Caching at the forwarder is fixed: most popular $L_{f}$ objects are cached
<hr>

::: columns
::: {.column width="50%"}
| Notation | Definition |
|:-:|:---------------|
|$C_{ab}$|Capacity of link $(a,b)$|
|$\mathcal{N}_c$|Set of consumer nodes|
|$L_{a}$|Cache size (in packets) at node $a$|
:::
::: {.column width="50%"}
| Notation | Definition |
|:-:|:---------------|
|$\mathcal{K}$|Set of data objects|
|$\alpha$|Zipf's law parameter|
|$\lambda$|Total incoming request rate at forwarder|
:::
:::

::: {.footer}
Introduction
:::

## Case for Larger Caches {.smaller}

::: columns
::: {.column width="50%"}
![](images/zipf_1k.svg)

::: {.fragment}
-  Most popular 1% of objects make up one fifth of all requests
-  Most popular 10% of objects make up half of all requests
:::
:::
::: {.column width="50%"}
![](images/cache_size.svg)

::: {.fragment}
-  Increasing cache sizes can help reduce delay significantly
-  Note that the gain from size increase has diminishing returns
:::
:::
:::

::: {.footer}
Motivation
:::

## Extended Toy Example {.smaller}

![](images/toynet_cache.svg){width=75% height=75% fig-align="center"}

::: columns
::: {.column width="50%"}
| Notation | Definition |
|:-:|:---------------|
|$r_{n_j}$|Read rate of tier $j$ at $n$|
|$L_{n_j}$|Cache size of tier $j$ at $n$|
:::
::: {.column width="50%"}
-   Numbered nodes connected by green edges represent potential cache tiers
-   Most popular 10 objects are cached in tier 1, objects ranked 11-40 are cached in tier 2, objects ranked 41-100 are cached in tier 3
:::
:::

::: {.footer}
Motivation
:::

## Case for Additional, Slower Caches {.smaller}

::: columns
::: {.column width="50%"}
![](images/two_tiers.svg)

::: {.fragment}
-  Second tier operates slower but improves delay for high request rates
-  This is due to the diminishing returns emerging from Zipf's law
:::
:::
::: {.column width="50%"}
![](images/three_tiers.svg)

::: {.fragment}
-  Addition of the third tier impacts performance negatively
-  This is due to trading link delay with cache read delay
:::
:::
:::

::: {.footer}
Motivation
:::

## The Cost Challenge

::: {.incremental}
-  Caches are not free to use, every admission and replacement has a cost
-  With additional cache devices, these are more pronounced:
   -  SSDs can wear out over time
   -  Their slower rates mean cache state transitions take longer
   -  They introduce additional power consumption
:::

::: {.footer}
Motivation
:::

## Coupling Caching with Forwarding

::: {.incremental}
-  Benefits of caching more pronounced when caching and forwarding decisions are tightly coupled
   -  "*Coupling caching and forwarding: benefits, analysis, and implementation*", G. Rossini, D. Rossi, ACM ICN 2014
-  This is even more critical with larger, slower caches
   -  Should we direct a cache miss toward a node where we might hit a slow cache or a more distant node where we might hit a faster cache?
:::

::: {.footer}
Motivation
:::

## Goal*

Placeholder.

::: {.footer}
Motivation
:::

## Related Work {.smaller}

-  *"Multi-Terabyte and multi-Gbps information centric routers"*, G. Rossini, D. Rossi, M. Garetto, E. Leonardi, INFOCOM 2014
   -  Two-layer cache (SSD masked behind DRAM), utilizes "prefetching", trace-driven and synthetic simulation results
-  *"Hierarchical Content Stores in High-Speed ICN Routers: Emulation and Prototype Implementation"*, R. B. Mansilha, L. Saino, M. Barcellos, M. Gallo, E. Leonardi, D. Perino, D. Rossi, ICN 2015
   -  Follow-up to above work, prototype implementation
-  *"Exploiting parallelism in hierarchical content stores for high-speed ICN routers"*, R. B. Mansilha, M. Barcellos, E. Leonardi, D. Rossi, COMNET 2017
   -  Follow-up to above works, integrates HCS design with NDN Forwarding Daemon (NFD)

::: {.footer}
Related Work
:::

## Related Work*

-  *"VIP: a framework for joint dynamic forwarding and caching in named data networks"*, E. M. Yeh, T. Ho, Y. Cui, M. Burd, R. Liu, D. Leong, ACM ICN 2014
    -   Model already includes read rate of caches
    -   Lyapunov drift analysis integrates easily with costs

::: {.footer}
Related Work
:::

# System Model

## Data Plane Model {.smaller}
Network model is based on general ICN principles:

-   Unit of content is *data object*; each object has equal size, and a unique source
-   Requests can enter network at any node; sources or caching nodes can respond to requests with data
-   Data responses follow reverse of request path back to requester
<hr>

| Notation | Definition |
|:-:|:---------|
|$\mathcal{G}$|Directed graph representing network topology|
|$(\mathcal{N},\mathcal{L})$|Set of nodes and links in $\mathcal{G}$|
|$C_{ab}$|Transmission capacity (in objects/sec) of link $(a,b)$|
|$\mathcal{K}$|Set of data objects in network|
|$\mc{S}(k)$|Source node for $k \in \mathcal{K}$|
|$\lambda^k_n$|Exogenous request arrival rate for $k$ at $n$|

::: {.footer}
System Model
:::

## Data Plane Model {.smaller}
Multi-tiered caching model has following properties:

-   Any node can have cache tiers and can cache an object unless it is already the source for that object
-   A node can cache an object in at most one of its tiers (*cache exclusivity*)
-   An object evicted from one tier can migrate to another tier at the same node
<hr>

| Notation | Definition |
|:-:|:---------|
|$\mathcal{J}_n$ | Set of cache tiers at node $n \in \mathcal{N}$|
|$L_{n_j}$ | Size (in objects) of cache tier $j \in \mathcal{J}_n$ at $n$|
|$r_{n_j}$ | Read rate of tier $j$ at node $n$|
|$c^a_{n_j}$| Admission cost of tier $j$ at node $n$|
|$c^e_{n_j}$| Eviction cost of tier $j$ at node $n$|

::: {.footer}
System Model
:::

## Virtual Plane Model {.smaller}
Virtual plane model is based on (Yeh, 2014):

-  Integer time slots depicted as $t = \{1,2,...\}$
-  A *virtual interest packet* (*VIP*) is generated alongside each exogenous request entering network; each node maintains a *VIP counter* for each object in the network
-  At the beginning of each time slot, each node decides where and how to forward its VIPs
-  VIPs are removed at object sources and caching nodes
<hr>

| Notation | Definition |
|:--:|:--------|
|$t$|Time slot referring to time interval $[t, t+1)$|
|$V^k_n(t)$|VIP count for $k$ at $n$ during $t$|
|$A^k_n(t)$|Number of exogenous requests for $k$ at $n$ during $t$|
|$\mu^k_{ab}(t)$|Allocated rate of VIPs for $k$ over $(a,b)$ during $t$|

::: {.footer}
System Model
:::

## Virtual Plane Model {.smaller}
Model is expanded with multiple cache tiers and *penalties*:

-   Each cache admission and eviction in virtual plane incurs a penalty
-   A configurable weight $\omega$ lets network determine importance of penalties in objective
<hr>

| Notation | Definition |
|:--:|:--------|
|$s^k_{n_j}(t)$ | Caching state of $k$ in tier $j$ at $n$ at the beginning of $t$|
|$p^k_{n_j}(t)$ | Penalty incurred by choice of $s^k_{n_j}(t)$|
|$p(t)$|Sum penalty incurred during $t$|
|$\omega$|Penalty importance weight|

::: {.footer}
System Model
:::

## VIP Queue and Penalty Dynamics {.smaller}
The VIP queue evolution for object $k$ at node $n$ can be described with the following, where $(x)^+ = max(x,0)$:
```{=tex}
\begin{equation}
\begin{split}
    V^k_n(t+1) \leq & \Bigg(\Big(V^k_n(t) - \sum\limits_{b \in \mathcal{N}}\mu^k_{nb}(t)\Big)^+ + A^k_n(t) 
    + \sum\limits_{a \in \mathcal{N}}\mu^k_{an}(t) - \sum\limits_{j \in \mathcal{J}_n} r_{n_j} s^k_{n_j}(t) \Bigg)^+    
\end{split}
\end{equation}
```

The penalty incurred by the caching action $s^k_{n_j}(t)$ can be defined in terms of utilization costs as follows:
```{=tex}
\begin{equation}
    p^k_{n_j}(t) \triangleq 
    \left\{ \begin{array}{ll}
        c^a_{n_j}, & \text{if} \; s^k_{n_j}(t) - s^k_{n_j}(t-1) = 1 \\
        c^e_{n_j}, & \text{if} \; s^k_{n_j}(t) - s^k_{n_j}(t-1) = -1 \\
       0, & \text{otherwise}
   \end{array} \right.
\end{equation}
```

With the sum penalty incurred during slot $t$ denoted as $p(t) = \sum_{\kin, \nin, \jin} p^k_{n_j}(t)$.

::: {.footer}
System Model
:::

# Virtual Plane Optimization

## Optimization Goal {.smaller}
Operate close to VIP network stability region boundary while keeping sum penalty small, thus achieving a balance between user demand rate satisfied and costs incurred.

More precisely, our objective is to minimize the following *drift-plus-penalty* expression:
```{=tex}
\begin{equation}
    \drift + \omega \pen
\end{equation}
```
where $\drift$ is the drift component defined as:
```{=tex}
\begin{equation}
    \drift = \mathbb{E}[\mathcal{L}(\mathbf{V}(t+1))-\mc{L}(\mathbf{V}(t))|\mathbf{V}(t)]
\end{equation}
```
and $\mc{L}(\mathbf{V}(t))$ is the Lyapunov function defined as:
```{=tex}
\begin{equation}
    \mc{L}(\mathbf{V}(t)) = \sum\limits_{\nin, \kin} (V^k_n(t))^2
\end{equation}
```

::: {.footer}
Optimization Framework
:::

## Virtual Plane Caching Algorithm {.smaller}
At the beginning of each time slot $t$, at each node $n$, observe queueues $(V^k_n(t))_{k \in \mathcal{K}, n \in \mathcal{N}}$ then perform **caching** as follows:

- Choose $s^k_{n_j}(t)$ for each $\kin$ and $j \in \mathcal{J}_n$ to:
```{=tex}
\begin{align}
    \text{maximize}
        & \quad \sum\limits_{\kin} \sum\limits_{\jin} r_{n_j} V^k_{n}(t) s^k_{n_j}(t) - p^k_{n_j}(t) \\
    \text{subject to}
        & \quad \sum\limits_{\kin} s^k_{n_j}(t) \leq L_{n_j}, \; \jin \\
        & \quad \sum\limits_{\jin} s^k_{n_j}(t) \leq 1, \; \kin \\
        & \quad s^k_{n_j}(t) \in \{0, 1\}, \; \kin, \; \jin
\end{align}
```

::: {.footer}
Optimization Framework
:::

## Virtual Plane Forwarding Algorithm {.smaller}
At the beginning of each time slot $t$, at each node $n$, observe queueues $(V^k_n(t))_{k \in \mathcal{K}, n \in \mathcal{N}}$ then perform **forwarding** as follows:

- Let $\mc{L}^k$ be the set of links which are allowed to transmit VIPs of object k, determined by a routing policy. For each $\kin$ and $\abin^k$, choose $\mu^k_{ab}(t)$ to:
```{=tex}
\begin{align}
    \text{maximize}
        & \quad \sum\limits_{\kin} \sum\limits_{\abin^k} \mu^k_{ab}(t) (V^k_a(t) - V^k_b(t)) \\
    \text{subject to} 
        & \quad \sum\limits_{\kin} \mu^k_{ab}(t) \leq C_{ba}, \; \abin \\
        & \quad \mu^k_{ab}(t) \geq 0, \; \kin, \; \abin
\end{align}
```

::: {.footer}
Optimization Framework
:::

## Analysis {.smaller}

The VIP **stability region** $\Lambda$ of the network $\mc{G} = (\mc{N},\mc{L})$, is the set $\Lambda$ consisting of all VIP arrival rates $\boldsymbol{\lambda} = (\lambda^k_n)_{\kin,\nin}$ such that the following holds:
```{=tex}
\begin{equation}
    \lambda^k_n \leq \sum\limits_{\bin} f^k_{nb} - \sum\limits_{\ain} f^k_{an} + \sum\limits_{\jin} r_{n_j} \betasumind, \quad \forall \nin, \; \kin, \; n \not = \mc{S}(k)
\end{equation}
```

| Notation | Definition |
|:--:|:--------|
|$f^k_{ab}$|Time-average VIP flow for $k$ over $(a,b)$|
|$\mc{B}_{n,i}$|i-th among all $\sigma_n$ possible cache placement sets at $n$; if $(k,j) \in \mc{B}_{n,i}$ during $t$, $s^k_{n_j}(t)$ = 1|
|$\beta_{n,i}$|Fraction of time objects at $n$ are placed according to $\mc{B}_{n,i}$; $0 \leq \beta_{n,i} \leq 1$ and $\sum^{\sigma_n}_{i=1} \beta_{n,i} = 1$|


::: {.footer}
Optimization Framework
:::

## Analysis {.smaller}

Given VIP arrival rate vector $\boldsymbol{\lambda}$, if there exists $\boldsymbol{\epsilon}$ such that $\boldsymbol{\lambda} + \boldsymbol{\epsilon} \in \Lambda$, then the network of VIP queues under the proposed algorithm satisfies the following:
```{=tex}
\begin{equation}
    \lim\limits_{T \rightarrow \infty} \frac{1}{T} \sum\limits^{T-1}_{t=0} \sum\limits_{\nin, \kin} \mathbb{E}[V^k_n(t)] \leq \frac{NB}{\epsilon} + \frac{\omega}{2\epsilon} \minpen
\end{equation}
```
```{=tex}
\begin{equation}
    \lim\limits_{T \rightarrow \infty} \frac{1}{T}\sum\limits^{T-1}_{t=0} \mathbb{E}[p(t)] \leq \frac{2NB}{\omega} + \minpen
\end{equation}
```
```{=tex}
\begin{equation}
    B \triangleq \frac{1}{2N} \sum_{\nin} \bigg((\mu^{out}_{n,max})^2 + 2(\mu^{out}_{n,max})r_{n,max} + \big(\textstyle \sum_{\kin}A^k_{n,max} + \mu^{in}_{n,max} + r_{n,max})^2 \bigg)
\end{equation}
```
$\Psi(\boldsymbol{\lambda})$ is the minimum time-average sum penalty achievable by a feasible and stabilizing randomized policy.

::: {.footer}
Optimization Framework
:::

## Solutions to Optimization Problems {.smaller}
We can rewrite the optimization problem for the [caching]{.body-highlight} step.
```{=tex}
\begin{align}
    \text{maximize}
        & \quad \sum\limits_{\kin} \sum\limits_{\jin} r_{n_j} V^k_{n}(t) s^k_{n_j}(t) - p^k_{n_j}(t) \\
    \text{subject to}
        & \quad \sum\limits_{\kin} s^k_{n_j}(t) \leq L_{n_j}, \; \jin \\
        & \quad \sum\limits_{\jin} s^k_{n_j}(t) \leq 1, \; \kin \\
        & \quad s^k_{n_j}(t) \in \{0, 1\}, \; \kin, \; \jin
\end{align}
```
```{=tex}
\begin{equation}
    p^k_{n_j}(t) \triangleq 
    \left\{ \begin{array}{ll}
        c^a_{n_j}, & \text{if} \; s^k_{n_j}(t) - s^k_{n_j}(t-1) = 1 \\
        c^e_{n_j}, & \text{if} \; s^k_{n_j}(t) - s^k_{n_j}(t-1) = -1 \\
       0, & \text{otherwise}
   \end{array} \right.
\end{equation}
```

::: {.footer}
Optimization Framework
:::

## Solutions to Optimization Problems {.smaller}
We can rewrite the optimization problem for the [caching]{.body-highlight} step.
```{=tex}
\begin{align}
    \text{maximize}
        & \quad \sum\limits_{\kin} \sum\limits_{\jin} b^k_{n_j}(t) s^k_{n_j}(t) \\
    \text{subject to}
        & \quad \sum\limits_{\kin} s^k_{n_j}(t) \leq L_{n_j}, \; \jin \\
        & \quad \sum\limits_{\jin} s^k_{n_j}(t) \leq 1, \; \kin \\
        & \quad s^k_{n_j}(t) \in \{0, 1\}, \; \kin, \; \jin
\end{align}
```
```{=tex}
\begin{equation}
    b^k_{n_j}(t) \triangleq \left\{ \begin{array}{ll}
        r_{n_j} V^k_n(t) - \omega c^a_{n_j}, & \text{if} \; s^k_{n_j}(t-1) = 0 \\
        r_{n_j} V^k_n(t) + \omega c^e_{n_j}, & \text{if} \; s^k_{n_j}(t-1) = 1
    \end{array} \right.
\end{equation}
```

::: {.footer}
Optimization Framework
:::

## Solutions to Optimization Problems {.smaller}
Using a transformation of variables, the problem can be written as a [linear assignment problem]{.body-highlight}. We can interpret this as deciding on caching variables for each one-object cache *slot*, rather than for each tier as a whole[^1].
```{=tex}
\begin{align}
    \text{maximize}
        & \quad \sum\limits_{\kin} \sum\limits_{\iin} b^k_{n_i}(t) s^k_{n_i}(t) \\
    \text{subject to} 
        & \quad \sum\limits_{\kin} s^k_{n_i}(t) \leq 1, \; \iin \\
        & \quad \sum\limits_{\iin} s^k_{n_i}(t) \leq 1, \; \kin \\
        & \quad s^k_{n_i}(t) \in \{0, 1\}, \; \kin, \; \iin
\end{align}
```
$\mc{I}_n = \{1, 2, ..., \sum_{\jin} L_{n_j}\}$. If $i \in \mc{I}_{n_j} = \{1 + \sum^{j - 1}_{\ell = 1} L_{n_\ell}, ..., \sum^{j}_{\ell = 1} L_{n_\ell}\}$, then $b^k_{n_j}(t) = b^k_{n_i}(t)$ and $s^k_{n_j}(t) = s^k_{n_i}(t)$.

[^1]: Only possible due to same size objects assumption

::: {.footer}
Optimization Framework
:::

## Solutions to Optimization Problems {.smaller}
**Forwarding**: The solution to the forwarding optimization problem is as in (Yeh, 2014).
```{=tex}
\begin{align}
    \text{maximize}
        & \quad \sum\limits_{\kin} \sum\limits_{\abin^k} \mu^k_{ab}(t) (V^k_a(t) - V^k_b(t)) \\
    \text{subject to} 
        & \quad \sum\limits_{\kin} \mu^k_{ab}(t) \leq C_{ba}, \; \abin \\
        & \quad \mu^k_{ab}(t) \geq 0, \; \kin, \; \abin
\end{align}
```
<hr>

```{=tex}
\begin{equation}
   \mu^k_{ab}(t) =
   \left\{ \begin{array}{ll}
      C_{ba}, & \text{if} \; k = k^*_{ab}(t) \; \text{and} \; W^k_{ab}(t) > 0 \\
      0, & \text{otherwise}
   \end{array} \right.
\end{equation}
```
```{=tex}
\begin{equation}
\begin{split}
   W^k_{ab}(t) & \triangleq V^k_a(t) - V^k_b(t), \\
   k^*_{ab}(t) & \triangleq \argmax\limits_{\{k: \abin^k\}} W^k_{ab}(t)
\end{split}
\end{equation}
```

::: {.footer}
Optimization Framework
:::

# Data Plane Strategy

## Transition to Data Plane

There are several considerations when applying our approach in the data plane:

-  Per-packet forwarding and caching decisions need to be made
-  Assumption that we have immediate access to any object for caching purposes does not hold 
-  Algorithm can lead to oscillatory behavior for caching decisions

::: {.footer}
Data Plane Strategy
:::

## Data Plane Caching {.smaller}

Define the [cache score]{.body-highlight} metric as follows:
```{=tex}
\begin{equation}
    CS^k_n(t) \triangleq \frac{1}{T} \sum^t_{\tau = t - T + 1} \sum_{(a,n) \in \mc{L}^k} v^k_{an}(\tau)
\end{equation}
```

Define the [eviction target]{.body-highlight} for tier $j$ at node $n$ as follows:
```{=tex}
\begin{equation}
   k'_{n_j} = \argmin_{\{k \in \mc{K}_{n_j}\}} CS^k_n(t)
\end{equation}
```
<hr>

| Notation | Definition |
|:-:|:---------|
|$\mc{v}^k_{ab}$|Actual number of VIPs for $k$ transmitted over $(a,b)$ during $t$|
|$T$|Size of sliding window in time slots|
|$\mc{K}_{n_j}(t)$|Set of objects cached in tier $j$ at $n$ during $t$|

::: {.footer}
Data Plane Strategy
:::

## Data Plane Caching {.smaller}

Define the [cache benefit]{.body-highlight} metric as follows:
```{=tex}
\begin{equation}
    CB^k_{n_j}(t) = \begin{cases}
        \begin{array}{l}
             r_{n_j}(CS^k_n(t) - CS^{k'_{n_j}}_n(t)) - \omega(c^a_{n_j} + c^e_{n_j}), \text{if} \, j \, \text{is full} \\
             r_{n_j} CS^k_n(t) - \omega c^a_{n_j}, \; \text{otherwise}
        \end{array}
    \end{cases} 
\end{equation}
```

When a data object $k \not \in \mc{K}_{n_j}$ arrives at $n$ during slot $t$, [data plane caching policy]{.body-highlight} at $n$ behaves as follows:

-  Determine the cache tier which offers the highest cache benefit, i.e. $j^* = \argmax_{\{ \jin \}} CB^k_{n_j}(t)$
-  If $CB^k_{n_{j^*}}(t) > 0$, admit object into tier $j^*$
   -  If $j^*$ is full, i.e. $|\mc{K}_{n_{j^*}}| = L_{n_{j^*}}$, $k$ replaces $k'_{n_{j^*}}$
-  If a replacement happened in $j^* < |\mc{J}_n|$, set $k = k'_{n_{j^*}}$, then start the process over

::: {.footer}
Data Plane Strategy
:::

## Data Plane Forwarding

Forwarding strategy is as in (Yeh, 2014). When a request for object $k \not \in \mc{K}_{n_j}(t), \; \forall \jin$, arrives at $n$ during $t$, the request is forwarded to the following node:

```{=tex}
\begin{equation}
    b^k_n(t) = \argmax_{\{ b:(n,b) \in \mc{L}^k \}} \frac{1}{T} \sum\limits^t_{t'=t-T+1} v^k_{nb}(t)
\end{equation}
```

::: {.footer}
Data Plane Strategy
:::

## Chunk-level Decisions

At chunk-level in data plane, we observe the following principles:

-   If a data object is admitted to (evicted from) a cache tier, all its chunks must be admitted to (evicted from) that tier.
-   Forwarding decision is made upon receiving request for a first chunk. Requests for subsequent chunks are forwarded to the same node.

::: {.footer}
Data Plane Strategy
:::

# Experiments

## Simulation Setup

-  Object-level simulation framework built with Python using the SimPy library
-  Modular, object-oriented design to allow future extension
-  Multi-threading allowing thread-per-experiment execution
-  Experiment parameters and output in JSON format for easy parsing
-  Thorough statistic logging

::: {.footer}
Experiments
:::

## Experiment Setting {.smaller}

::: columns
::: {.column width="50%"}

![Abilene (11 nodes)](images/abilene.svg)

![GEANT (34 nodes)](images/geant.svg)

:::
::: {.column width="50%"}

![4x4 Grid (16 nodes)](images/grid.svg)

![3-Regular (50 nodes)](images/3reg.svg)

:::
:::

::: {.footer}
Experiments
:::

## Experiment Setting {.smaller}

-  Content source for each $k$ is selected uniformly at random; content sources have infinite read rate
-  Routing: Allow $n$ to forward requests for $k$ to any neighbor on *any* shortest path between $n$ and $\mc{S}(k)$ (in number of hops)
-  Adapted replacement baselines: LRU, LFU, FIFO, RANDOM
   -  LRU, FIFO and RANDOM cost-unaware, paired with LCE admission
   -  LFU has both aware and unaware adaptations, admission based on frequency
-  All baseline caching policies paired with Least Response Time forwarding
-  Virtual plane algorithm with slot length of 1 sec and sliding window of size $T=100$

::: {.footer}
Experiments
:::

## Experiment Setting {.smaller}

| Parameter | Description | Value |
|:-:|:-:|:-:|
|$K$|Number of objects in catalog $\mc{K}$|1000|
|$\sum_{\kin} \lambda^k_n$|Total request arrival rate at each $n$ in objects/sec|10|
|$\alpha$|Zipf's law parameter for object popularity distribution|0.75|
|$C_{ab}$|Capacity of each link $\abin$, in objects|10|
|$L_{n_1}$|Capacity of tier 1 at each $n$, in objects|5|
|$r_{n_1}$|Read rate of tier 1 at each $n$, in objects/sec|20|
|$c^a_{n_1}$, $c^e_{n_1}$| Admission and eviction costs of tier 1 at each $n$|4, 2|
|$r_{n_2}$|Read rate of tier 1 at each $n$, in objects/sec|10|
|$c^a_{n_2}$, $c^e_{n_2}$| Admission and eviction costs of tier 1 at each $n$|2, 1|
: Parameters for all topologies {tbl-colwidths="[10,80,10]"}

::: {.footer}
Experiments
:::

## Results I {.smaller}
**Scenario**: No penalties ($\omega=0$) and $L_{n_2} = 100$ at each $n$.

![Total delay, as fraction of total delay without any caching](images/tops_delay_v3.svg){width=75% height=75% fig-align="center"}

![Cache hits, percentage of hits in the first tier on the left, total number of hits on the right](images/tops_hits_v3.svg){width=75% height=75% fig-align="center"}

::: {.footer}
Experiments
:::

## Results II {.smaller}
**Scenario**: Cost-unaware policies excluded, $\omega > 0$. Best value of $L_{n_2}$ picked.

::: {.body-centered}
Delay vs. penalty ($\omega$ decreases to the right)
:::

::: columns
::: {.column width="50%"}
![Abilene](images/pen_vs_delay_abilene_v2.svg)
:::
::: {.column width="50%"}
![4x4 Grid](images/pen_vs_delay_grid_v2.svg)
:::
:::

::: {.footer}
Experiments
:::

## Results II {.smaller}
**Scenario**: Cost-unaware policies excluded, $\omega > 0$. Best value of $L_{n_2}$ picked.

::: {.body-centered}
Delay vs. penalty ($\omega$ decreases to the right)
:::

::: columns
::: {.column width="50%"}
![GEANT](images/pen_vs_delay_geant_v2.svg)
:::
::: {.column width="50%"}
![3-Regular](images/pen_vs_delay_regular_v2.svg)
:::
:::

::: {.footer}
Experiments
:::

# Proposed Work

## Improvements

::: {.fragment}
There is room for improvement
:::

::: {.fragment}
![](images/big-room.webp){width=65% height=65% fig-align="left"}

... and it's a big room
:::

::: {.footer}
Super Funny Joke
:::

## Cache Read Bandwidth {.smaller}
Current model assumes read rate of each object is independent of all others. 
```{=tex}
\begin{equation}
    V^k_n(t+1) \leq \Bigg(\Big(V^k_n(t) - \sum\limits_{b \in \mathcal{N}}\mu^k_{nb}(t)\Big)^+ + A^k_n(t)
    + \sum\limits_{a \in \mathcal{N}}\mu^k_{an}(t) - \sum\limits_{j \in \mathcal{J}_n} \color{#C8102E}{r_{n_j}} s^k_{n_j}(t) \Bigg)^+
\end{equation}
```

## Cache Read Bandwidth {.smaller}
Current model assumes read rate of each object is independent of all others.
```{=tex}
\begin{equation}
    \color{#C0C0C0}{V^k_n(t+1) \leq \Bigg(\Big(V^k_n(t) - \sum\limits_{b \in \mathcal{N}}\mu^k_{nb}(t)\Big)^+ + A^k_n(t)
    + \sum\limits_{a \in \mathcal{N}}\mu^k_{an}(t) - \sum\limits_{j \in \mathcal{J}_n}} \color{#C8102E}{r_{n_j}} \color{#C0C0C0}{s^k_{n_j}(t) \Bigg)^+}    
\end{equation}
```
We need to make this more accurate when considering slower caches.
```{=tex}
\begin{equation}
   \color{#C0C0C0}{V^k_n(t+1) \leq \Bigg(\Big(V^k_n(t) - \sum\limits_{b \in \mathcal{N}}\mu^k_{nb}(t)\Big)^+ + A^k_n(t)
   + \sum\limits_{a \in \mathcal{N}}\mu^k_{an}(t) - \sum\limits_{j \in \mathcal{J}_n}} \color{#C8102E}{r^k_{n_j}(t)} \color{#C0C0C0}{s^k_{n_j}(t) \Bigg)^+}
\end{equation}
```
```{=tex}
\begin{equation}
   \sum_{\kin} r^k_{n_j}(t) \leq r_{n_j}, \; \forall t   
\end{equation}
```

::: {.footer}
Proposed Work
:::

## Cache Read Bandwidth {.smaller}
Additional control action each time slot, in product form with another control action
```{=tex}
\begin{equation}
\text{maximize} \quad \sum\limits_{\kin} \sum\limits_{\jin} V^k_{n}(t) r^k_{n_j}(t) s^k_{n_j}(t) - p^k_{n_j}(t)
\end{equation}
```
Read rate no longer a constant, VIPs drained via caching represented with a flow variable, like VIP flow terms over links
```{=tex}
\begin{equation}
   F^k_{n_j}(t) \triangleq r^k_{n_j}(t) s^k_{n_j}(t), \qquad f^k_{n_j} = \frac{1}{t} \sum^{t}_{\tau=1} r^k_{n_j}(\tau) s^k_{n_j}(\tau)
\end{equation}
```
**Expectation**: Stability region and its analysis changes significantly; complexity of caching problem solution reduces

::: {.footer}
Proposed Work
:::

## Cache Exclusivity {.smaller}
```{=tex}
\begin{equation}
\sum_{\jin} s^k_{n_j}(t) \leq 1, \quad \kin
\end{equation}
```
Useful in view of [interest aggregation]{.body-highlight} and hardware limitations.

Arguments in support of removing it:

-  Cache exclusivity makes [upward migrations]{.body-highlight} difficult in data plane
-  Without this constraint, algorithm operates over individual tiers independently; reduced complexity, adaptations to baselines unnecessary
-  Hardware limitations may not be there for long (peer-to-peer DMA)
Not clear whether removing exclusivity constraint would make algorithm converge to overly redundant cache placements; extensive testing required.

::: {.footer}
Proposed Work
:::

## Different Object Sizes {.smaller}

Equal object sizes assumption makes analysis straightforward, but is not realistic.

Let $z^k$ be the size of object $k$ in bits. Redefine $L_{n_j}$ be the cache capacity of tier $j$ at node $n$ in bits, instead of objects. The caching problem changes as follows:
```{=tex}
\begin{align}
    \text{maximize}
        & \quad \sum\limits_{\kin} \sum\limits_{\jin} r_{n_j} V^k_{n}(t) s^k_{n_j}(t) - p^k_{n_j}(t) \\
    \text{subject to}
        & \quad \sum\limits_{\kin} z^k s^k_{n_j}(t) \leq L_{n_j}, \; \jin \\
        & \quad \sum\limits_{\jin} s^k_{n_j}(t) \leq 1, \; \kin \\
        & \quad s^k_{n_j}(t) \in \{0, 1\}, \; \kin, \; \jin
\end{align}
```
Problem no longer a LAP but a GAP instead (NP-hard).

::: {.footer}
Proposed Work
:::

## Different Object Sizes {.smaller}

-  Combined with cache bandwidth constraints, solution may still be low-complexity
-  We have to reevaluate how we treat VIP counts:
   -  Copies of a larger object are produced at a slower rate
   -  VIPs for smaller objects can be drained at a higher rate
   -  Without adjustments, service may prioritize smaller objects
-  Combined with multi-tiered caches, raises the [integrity]{.body-highlight} question, i.e. should we mandate that nodes cache all chunks of an object in the same tier?
   -  If not, complexity of the caching problem increases dramatically
   -  Chunk-level strategy may serve this scenario better

::: {.footer}
Proposed Work
:::

## Virtual Plane - Data Plane Gap {.smaller}

-   Some disparities between the two planes lead to a gap building over time
    -   Virtual plane assumes nodes can immediately cache any object in the network
    -   Length of time slots, interest aggregation etc. further extend this gap
-   Amending the gap is difficult, but it must be quantified:
    -   Measure delay between a caching decision in the virtual plane, and when it is realized in data plane
    -   Estimate difference between performance and cost metrics under virtual plane assumptions vs. data plane results

::: {.footer}
Proposed Work
:::

## Better Experiments

-  Python is convenient but not highly-scalable for DES frameworks. Julia is a great alternative for both convenience and performance.
-  Per-experiment multi-threading is not effective. Simulator core should be multi-threaded to improve [scale-up]{.body-highlight}; per-experiment [scale-out]{.body-highlight} can also be used.
-  Current codebase has some comments but little documentation and examples. Easily [improvable]{.body-highlight} simulator and [reproducible]{.body-highlight} results crucial.

::: {.footer}
Proposed Work
:::

# Other Contributions & Conclusion

## Power & Caching in Wireless HetNets*

Placeholder

## Other Work*

Placeholder

## Conclusion*

Placeholder

## Acknowledgements*

Placeholder

# Questions

# Appendices {visibility="uncounted"}

## Appendix A: Other References {.smaller visibility="uncounted"}
-  *"Toward terabyte-scale caching with SSD in a named data networking router"*, W. So, T. Chung, H. Yuan, D. Oran, M. Stapp, ANCS 2014
-  *"On Designing Optimal Memory Damage Aware Caching Policies for Content-Centric Networks"*, S. Shukla, A. A. Abouzeid, WiOpt 2016
-  *"HCaching: High-Speed Caching for Information-Centric Networking"*, H. Li, H. Zhou, W. Quan, B. Feng, H. Zhang, S. Yu, GLOBECOM 2017


## Appendix B: Model Notation {.smaller .scrollable visibility="uncounted"}

| Notation | Definition |
|:--:|:--------|
|$\mathcal{G}$|Directed graph representing the network topology|
|$(\mathcal{N},\mathcal{L})$|Set of nodes and links in $\mathcal{G}$|
|$C_{ab}$|Transmission capacity (in objects/sec) of link $(a,b)$|
|$\mathcal{K}$|Set of data objects in the network|
|$\mc{S}(k)$|Content source node for $k \in \mathcal{K}$|
|$\mathcal{J}_n$ | Set of cache tiers at node $n \in \mathcal{N}$|
|$L_{n_j}$ | Size (in objects) of cache tier $j \in \mathcal{J}_n$ at $n$|
|$r_{n_j}$ | Read rate of tier $j$ at node $n$|
|$c^a_{n_j}$| Admission cost of tier $j$ at node $n$|
|$c^e_{n_j}$| Eviction cost of tier $j$ at node $n$|
|$\lambda^k_n$|Exogenous request arrival rate for $k$ at $n$|
|$t$|Time slot referring to time interval $[t, t+1)$|
|$A^k_n(t)$|Number of exogenous requests for $k$ at $n$ during $t$|
|$s^k_{n_j}(t)$ | Caching state of $k$ in tier $j$ at $n$ at the beginning of $t$|
|$p^k_{n_j}(t)$ | Penalty incurred by the choice of $s^k_{n_j}(t)$|
|$p(t)$|Sum penalty incurred during $t$|
|$\omega$|Penalty importance weight|
|$V^k_n(t)$|VIP count for $k$ at $n$ during $t$|
|$\mathbf{V}(t)$|Vector of VIP queue states during $t$|
|$\mu^k_{ab}(t)$|Allocated rate of VIPs for $k$ over $(a,b)$ during $t$|
: Table of Notations {tbl-colwidths="[20,80]"}

## Appendix C: Analysis Notation {.smaller .scrollable visibility="uncounted"}

| Notation | Definition |
|:--:|:--------|
|$f^k_{ab}$|Time-average VIP flow for $k$ over $(a,b)$|
|$F^k_{ab}(t)$|Number of VIPs of $k$ sent over $(a,b)$ during slot $t$|
|$N$|Number of nodes in the network, i.e. $N = |\mc{N}|$|
|$A^k_{n,max}$| Finite value such that $A^k_n(t) \leq A^k_{n,max}$ for all $t$|
|$A_{n,max}$| Maximum total exogenous arrivals at node $n$ during $t$ across all $\kin$, i.e. $A_{n,max} \triangleq \sum_{\kin}A^k_{n,max}$|
|$\mu^{out}_{n,max}$| Maximum total allocated transmission rate for VIPs across all $(n,b) \in \mc{L}$, i.e. $\mu^{out}_{n,max} \triangleq \sum_{\bin}C_{nb}$|
|$\mu^{in}_{n,max}$| Maximum total allocated transmission rate for VIPs across all $(a,n) \in \mc{L}$, i.e. $\mu^{in}_{n,max} \triangleq \sum_{\bin}C_{an}$|
|$r_{n,max}$| Maximum total rate that can be served by all cache tiers at $n$|
|$\Psi(\boldsymbol{\lambda})$|Minimum time-average sum penalty achievable by a feasible and stabilizing randomized policy|

## Appendix D: Simulator Snippets {.smaller visibility="uncounted"}

::: {.panel-tabset}

### Node

```{.python}
class Node(object):
   # ...
   def packetReceiver(self, remote_id):
      while True:
         pkt = yield self.in_links[remote_id].get()
         if pkt.isData():
               yield self.env.timeout(1/self.in_links[remote_id].link_cap)
         if pkt.isData() or pkt.isInterest():
               self.pkt_buffer.put((remote_id, pkt))
   def packetProcessor(self):
      while True:
         remote_id, pkt = yield self.pkt_buffer.get()
         if pkt.isInterest():
               self.receiveInterest(remote_id, pkt.request)
         elif pkt.isData():
               self.receiveData(pkt.request)
   # ...
   def forwardInterest(self, request):
        self.sendInterestPacket(self.fib[request.object_id][0], request)
        return

    def decideCaching(self, object_id):
        yield self.env.timeout(0)
        return
   # ...
```

### Cache

```{.python}
class Cache(object):
   def cacheController(self):
      while True:
         task = yield self.task_queue.get()
         if task.type == 'r':
               obj = yield self.env.process(self.readProcess(task.object_id))
               self.out_buffer.put(obj)
         elif task.type == 'w':
               yield self.env.process(self.writeProcess(task.object_id))
   #...
   def readObject(self, object_id):
      self.stats['reads'] += 1            
      task = CacheTask(type = 'r', object_id = object_id)
      tic = self.env.now
      self.task_queue.put(task)
      obj = yield self.out_buffer.get()
      self.stats['read_delay'] += self.env.now - tic
      return obj
   def cacheObject(self, object_id):
      self.stats['writes'] += 1
      self.contents.append(object_id)
      self.cur_size += 1
      task = CacheTask(type = 'w', object_id = object_id)
      self.task_queue.put(task)
      return True
```

### Link

```{.python}
class Link(object):
   def __init__(self, env, link_cap, prop_delay):
      self.env = env
      self.link_cap = link_cap
      self.pipe = sp.Store(env)
   def put(self, pkt):
      self.pipe.put(pkt)
   def get(self):
      return self.pipe.get()

class VipLink(object):
    def __init__(self, env):
        self.env = env
        self.update_pipe = sp.Store(env)
        self.vip_pipe = sp.Store(env)    
    def pushUpdate(self, pkt):
        self.update_pipe.put(pkt)    
    def getUpdate(self):
        return self.update_pipe.get()    
    def pushVips(self, pkt):
        self.vip_pipe.put(pkt)    
    def getVips(self):
        return self.vip_pipe.get()
```

### Caching Policy

```{.python}
class UNIFNode(Node):
   def decideCaching(self, object_id):
      cache = np.random.choice(self.caches)
      if cache.isFull():
         victim_id = np.random.choice(cache.contents)
         yield self.env.process(cache.replaceObject(victim_id, object_id))
      else:
         cache.cacheObject(object_id)
```

### Forwarding Policy

```{.python}
class RoundRobinNode(Node):
   def addFIB(self, fib):
      super().addFIB(fib)
      self.link_queues = {}
      for k in fib:
         self.link_queues[k] = deque(fib[k])
   
   def forwardInterest(self, request):
      object_id = request.object_id
      remote_id = self.link_queues[object_id][0]
      self.link_queues[object_id].rotate(1)
      self.sendInterestPacket(remote_id, request)
```
:::