---
title: "Cost-aware Joint Caching and Forwarding in Networks with Diverse Cache Resources"
subtitle: "<s>Farouk Multi</s> Faruk Volkan Mutlu - PhD Proposal Review"
author: "Research Advisor: Edmund Yeh<br>Committee Members: Elif Uysal, Stratis Ioannidis"
format:
  revealjs:
    slide-number: true
    theme: white
    logo: images/neu_logo.svg
    css: styles/styles.css
    highlight-style: atom-one
---

## Outline

The outline of this presentation is as follows:

1. General Context
2. Joint Caching and Forwarding in Networks with Hybrid Storage Systems
3. Other Contributions
4. Conclusion and Acknowledgements

::: {.footer}
Outline
:::

::: {.hidden}
$$
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\nin}{n \in \mc{N}}
\newcommand{\kin}{k \in \mc{K}}
\newcommand{\jin}{j \in \mc{J}_n}
\newcommand{\kjin}{(k,j) \in \mc{B}_{n,i}}
\newcommand{\iin}{i \in \mc{I}_n}
\newcommand{\ain}{a \in \mc{N}}
\newcommand{\bin}{b \in \mc{N}}
\newcommand{\abin}{(a,b) \in \mc{L}}
\newcommand{\about}{(a,b) \not\in \mc{L}^k}
\newcommand{\betasum}{\sum\limits ^{\sigma_n}_{i=1} \beta_{n,i}}
\newcommand{\betasumnl}{\sum ^{\sigma_n}_{i=1} \beta_{n,i}}
\newcommand{\betasumind}{\sum\limits ^{\sigma_n}_{i=1} \beta_{n,i}\mathbf{1}_{[\kjin]}}
\newcommand{\minpen}{\Psi(\boldsymbol{\lambda})}
\newcommand{\drift}{\Delta(\mathbf{V}(t))}
\newcommand{\pen}{\mathbb{E}[p(t)|\mathbf{V}(t)]}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
$$
:::

## Information Centric Networking

- **Motivation**: The Internet is primarily a data distribution network, yet its principles are those of a communication network
- **Premise**: Make uniquely named data the primary entity of the network, instead of endpoints
- **Advantages**: Improve efficiency and scalability by decoupling data from location and session

::: {.footer}
Context
:::

## In-network Caching

- **Motivation**: Current caching infrastructure is mostly proprietary and centralized
- **Premise**: Enable every router in the network to maintain its own cache
- **Advantages**: Caching becomes decentralized, reducing network congestion and improving scalability

::: {.footer}
Context
:::

# Introduction

## Overview {.smaller}

::: {.incremental}
-  **Motivation**: Data volume grows exponentially, cache capacities are stagnant
-  **Challenge**: Enabling larger caches in a cost-effective way is difficult
-  **Considerations**:
   -  How beneficial are larger caches?
   -  Are slower and larger storage elements viable as caches?
   -  What are the costs of operating large caches?
   -  How can we get the most out of caches in the network?
-  **Goal**: Develop a policy that observes network traffic patterns and cache device characteristics to make smart caching and forwarding decisions
:::

::: {.footer}
Introduction
:::

## Use Case: Data-Intensive Science

::: {.incremental}
-   Data-intensive science experiments (high-energy physics, genomics etc.) process huge amounts of data
    -   In one year of LHC running, over an exabyte of data is accessed
    -   In November 2018, 15.8 petabytes of data were written on tape at CERN
-   Data is continuously distributed across the world for research
:::

::: {.footer}
Introduction
:::

## Use Case: Data-Intensive Science

::: {.incremental}
-   *"N-DISE: NDN-based Data Distribution for Large-Scale Data-Intensive Science"*, Y. Wu, F. V. Mutlu, et al.
    -   NDN-based high-throughput data distribution, caching and access system
    -   Each router allocated 20 gigabytes DRAM cache space
-   *"NDN-DPDK: NDN Forwarding at 100 Gbps on Commodity Hardware"*, J. Shi, D. Pesavento, L. Benmohamed
    -   Increasing capacities via NVMe SSDs require novel caching strategies due to their slow speeds
:::

::: {.footer}
Introduction
:::

## The Capacity Challenge

::: {.incremental}
-  DRAM is the primary cache device since it is fast, i.e. *line rate* (12.8-25.6 GB/s for DDR4, 32-64 GB/s for DDR5)
-  However, DRAM offers small cache sizes at a high and exponentially increasing cost ($3 to $12 / GB)
-  Next best storage element is the NVMe SSD, offering large cache sizes at significantly lower costs ($0.1 / GB)
-  NVMe SSDs can be much slower than DRAM (11.7 GB/s ideal read rate for fastest PCIe 5.0)
:::

::: {.footer}
Introduction
:::

## Toy Example {.smaller}

![](images/toynet.svg){width=60% height=60% fig-align="center"}

::: columns
::: {.column width="50%"}
| Notation | Definition |
|:-:|:---------------|
|$C_{ab}$|Capacity of link $(a,b)$|
|$\mathcal{N}_c$|Set of consumer nodes|
|$\mathcal{K}$|Set of data objects|
|$\alpha$|Zipf's law parameter|
|$L_{a}$|Cache size (in packets) at node $a$|
|$\lambda$|Total incoming request rate at forwarder|
:::
::: {.column width="50%"}
-  Catalog of $|\mathcal{K}|=1000$ single-packet objects, ranked in popularity according to Zipf's law with parameter $\alpha=0.75$
-  Consumer nodes send requests to the forwarder node, which either responds from its cache or forwards them upstream to server
-  Caching at the forwarder is clairvoyant, i.e. most popular $L_{f}$ objects are cached
:::
:::

::: {.footer}
Motivation
:::

## Case for Larger Caches {.smaller}

::: columns
::: {.column width="50%"}
![](images/zipf_1k.svg)

::: {.fragment}
-  Most popular 1% of objects make up one fifth of all requests
-  Most popular 10% of objects make up half of all requests
:::
:::
::: {.column width="50%"}
![](images/cache_size.svg)

::: {.fragment}
-  Increasing cache sizes can help reduce delay significantly
-  Note that the gain from size increase has diminishing returns
:::
:::
:::

::: {.footer}
Motivation
:::



## Extended Toy Example {.smaller}

![](images/toynet_cache.svg){width=75% height=75% fig-align="center"}

::: columns
::: {.column width="50%"}
| Notation | Definition |
|:-:|:---------------|
|$r_{n_j}$|Read rate of tier $j$ at $n$|
|$L_{n_j}$|Cache size of tier $j$ at $n$|

Numbered nodes connected by green edges represent potential cache tiers
:::
::: {.column width="50%"}
-  Same catalog and popularity distribution, three possible cache tiers
-  Objects are cached in order of popularity, starting from tier 1 and moving onto next tier when current one is full
:::
:::

::: {.footer}
Motivation
:::

## Case for Additional, Slower Caches {.smaller}

::: columns
::: {.column width="50%"}
![](images/two_tiers.svg)

::: {.fragment}
-  Although second tier operates slower than line rate, it improves delay at high request rates
-  This is due to the diminishing returns emerging from Zipf's law
:::
:::
::: {.column width="50%"}
![](images/three_tiers.svg)

::: {.fragment}
-  Addition of the third tier impacts performance negatively
-  **Lesson**: Cache tiers need to outpace their own *hit rate*
:::
:::
:::

::: {.footer}
Motivation
:::

## The Cost Challenge*

::: {.incremental}
-  Caches are not free to use, every admission and replacement has a cost
-  With additional cache devices, these are more pronounced:
   -  SSDs can wear out over time
   -  Their slower rates mean cache state transitions take longer
   -  They introduce additional power consumption
:::

::: {.footer}
Motivation
:::

## Coupling Caching with Forwarding*

::: {.incremental}
-  Benefits of caching more pronounced when caching and forwarding decisions are tightly coupled
   -  "*Coupling caching and forwarding: benefits, analysis, and implementation*", G. Rossini, D. Rossi, ACM ICN 2014
-  This is even more critical with larger, slower caches
   -  Should we direct a cache miss toward a node where we might hit a slow cache or a more distant node where we might hit a faster cache?
:::

::: {.footer}
Motivation
:::

## Goal*

Placeholder.

::: {.footer}
Motivation
:::

## Related Work {.smaller}

-  *"Multi-Terabyte and multi-Gbps information centric routers"*, G. Rossini, D. Rossi, M. Garetto, E. Leonardi, INFOCOM 2014
   -  Two-layer cache (SSD masked behind DRAM), utilizes "prefetching", trace-driven and synthetic simulation results
-  *"Hierarchical Content Stores in High-Speed ICN Routers: Emulation and Prototype Implementation"*, R. B. Mansilha, L. Saino, M. Barcellos, M. Gallo, E. Leonardi, D. Perino, D. Rossi, ICN 2015
   -  Follow-up to above work, prototype implementation
-  *"Exploiting parallelism in hierarchical content stores for high-speed ICN routers"*, R. B. Mansilha, M. Barcellos, E. Leonardi, D. Rossi, COMNET 2017
   -  Follow-up to above works, integrates HCS design with NDN Forwarding Daemon (NFD)

::: {.footer}
Related Work
:::

## Related Work* {.smaller}
-  *"Toward terabyte-scale caching with SSD in a named data networking router"*, W. So, T. Chung, H. Yuan, D. Oran, M. Stapp, ANCS 2014
-  *"On Designing Optimal Memory Damage Aware Caching Policies for Content-Centric Networks"*, S. Shukla, A. A. Abouzeid, WiOpt 2016
-  *"HCaching: High-Speed Caching for Information-Centric Networking"*, H. Li, H. Zhou, W. Quan, B. Feng, H. Zhang, S. Yu, GLOBECOM 2017

::: {.footer}
Related Work
:::

## Related Work*

-  *"VIP: a framework for joint dynamic forwarding and caching in named data networks"*, E. M. Yeh, T. Ho, Y. Cui, M. Burd, R. Liu, D. Leong, ACM ICN 2014
    -   Model already includes read rate of caches
    -   Lyapunov drift analysis integrates easily with costs

::: {.footer}
Related Work
:::

# System Model

## Data Plane Model {.smaller}

::: {.incremental}
-  Unit of content in the network is a *data object* (*object* for short), each object has the same size (in bits), a unique identifier, and a fixed unique source, i.e. an *object source*
-  Exogenous requests for objects can enter the network at any node and are forwarded through the network until reaching an object source or a node that caches the object
-  Responses to requests forward the object back to the requester using the reverse of the path followed by the request
-  Any node can have one or more cache tiers and a cache tier is defined by its capacity (in objects), read rate (in objects/sec) and admission / eviction penalties (arbitrary unit)
-  A node can cache an object in any of its tiers, unless it is the source for that object
-  A node can cache the same object in at most one of its tiers (*cache exclusivity* assumption)
-  An object evicted from a cache tier is allowed to potentially migrate to another tier at the same node
:::

::: {.footer}
System Model
:::

## Control Plane Model {.smaller}

::: {.incremental}
-  Control algorithms operate in a time-slotted *virtual control plane* added above the data plane just described
-  Whenever an exogenous request for an object arrives at a node, a *virtual interest packet* (*VIP*) for that object is generated at that node
-  Each node maintains a *VIP counter* for each object in the network; generating a VIP means incrementing this counter by 1
-  The *VIP count* of an object at a node can be interpreted as a queue that represents the unsatisfied demand for that object at that node
-  VIPs are forwarded through the network in the virtual plane using virtual links; at the beginning of each time slot, each node decides where and how to forward its VIPs
-  VIPs for an object are removed either at the source for that object, or at nodes that cache the object
:::

::: {.footer}
System Model
:::

# Virtual Plane Optimization Framework

## VIP Queue and Penalty Dynamics {.smaller}
The VIP queue evolution for object $k$ at node $n$ can be described with the following, where $(x)^+ = max(x,0)$:
```{=tex}
\begin{equation}
\begin{split}
    V^k_n(t+1) \leq & \Bigg(\Big(V^k_n(t) - \sum\limits_{b \in \mathcal{N}}\mu^k_{nb}(t)\Big)^+ + A^k_n(t) 
    + \sum\limits_{a \in \mathcal{N}}\mu^k_{an}(t) - \sum\limits_{j \in \mathcal{J}_n} r_{n_j} s^k_{n_j}(t) \Bigg)^+    
\end{split}
\end{equation}
```

The penalty incurred by the caching action $s^k_{n_j}(t)$ can be defined in terms of utilization costs as follows:
```{=tex}
\begin{equation}
    p^k_{n_j}(t) \triangleq 
    \left\{ \begin{array}{ll}
        c^a_{n_j}, & \text{if} \; s^k_{n_j}(t) - s^k_{n_j}(t-1) = 1 \\
        c^e_{n_j}, & \text{if} \; s^k_{n_j}(t) - s^k_{n_j}(t-1) = -1 \\
       0, & \text{otherwise}
   \end{array} \right.
\end{equation}
```

With the sum penalty incurred during slot $t$ denoted as $p(t) = \sum_{\kin, \nin, \jin} p^k_{n_j}(t)$.

::: {.footer}
Optimization Framework
:::

## Optimization Goal {.smaller}
We aim to operate as close to the boundary of the VIP network stability region as possible while keeping sum penalty small, thus achieving a balance between user demand rate satisfied and costs incurred.

More precisely, our objective is to minimize the following *drift-plus-penalty* expression:
```{=tex}
\begin{equation}
    \drift + \omega \pen
\end{equation}
```
where $\drift$ is the drift component defined as:
```{=tex}
\begin{equation}
    \drift = \mathbb{E}[\mathcal{L}(\mathbf{V}(t+1))-\mc{L}(\mathbf{V}(t))|\mathbf{V}(t)]
\end{equation}
```
and $\mc{L}(\mathbf{V}(t))$ is the Lyapunov function defined as:
```{=tex}
\begin{equation}
    \mc{L}(\mathbf{V}(t)) = \sum\limits_{\nin, \kin} (V^k_n(t))^2
\end{equation}
```

::: {.footer}
Optimization Framework
:::

## Virtual Plane Caching Algorithm {.smaller}
At the beginning of each time slot $t$, at each node $n$, observe queueues $(V^k_n(t))_{k \in \mathcal{K}, n \in \mathcal{N}}$ then perform **caching** as follows:

- Choose $s^k_{n_j}(t)$ for each $\kin$ and $j \in \mathcal{J}_n$ to:
```{=tex}
\begin{align}
    \text{maximize}
        & \quad \sum\limits_{\kin} \sum\limits_{\jin} r_{n_j} V^k_{n}(t) s^k_{n_j}(t) - p^k_{n_j}(t) \\
    \text{subject to}
        & \quad \sum\limits_{\kin} s^k_{n_j}(t) \leq L_{n_j}, \; \jin \\
        & \quad \sum\limits_{\jin} s^k_{n_j}(t) \leq 1, \; \kin \\
        & \quad s^k_{n_j}(t) \in \{0, 1\}, \; \kin, \; \jin
\end{align}
```

::: {.footer}
Optimization Framework
:::

## Virtual Plane Forwarding Algorithm {.smaller}
At the beginning of each time slot $t$, at each node $n$, observe queueues $(V^k_n(t))_{k \in \mathcal{K}, n \in \mathcal{N}}$ then perform **forwarding** as follows:

- Let $\mc{L}^k$ be the set of links which are allowed to transmit VIPs of object k, determined by a routing policy. For each $\kin$ and $\abin^k$, choose:
```{=tex}
\begin{align}
    \text{maximize}
        & \quad \sum\limits_{\kin} \sum\limits_{\abin^k} \mu^k_{ab}(t) (V^k_a(t) - V^k_b(t)) \\
    \text{subject to} 
        & \quad \sum\limits_{\kin} \mu^k_{ab}(t) \leq C_{ba}, \; \abin \\
        & \quad \mu^k_{ab}(t) \geq 0, \; \kin, \; \abin
\end{align}
```

::: {.footer}
Optimization Framework
:::

## Analysis {.smaller}

The VIP **stability region** $\Lambda$ of the network $\mc{G} = (\mc{N},\mc{L})$, is the set $\Lambda$ consisting of all VIP arrival rates $\boldsymbol{\lambda} = (\lambda^k_n)_{\kin,\nin}$ such that the following holds:
```{=tex}
\begin{equation}
    \lambda^k_n \leq \sum\limits_{\bin} f^k_{nb} - \sum\limits_{\ain} f^k_{an} + \sum\limits_{\jin} r_{n_j} \betasumind, \quad \forall \nin, \; \kin, \; n \not = \mc{S}(k)
\end{equation}
```

| Notation | Definition |
|:--:|:--------|
|$f^k_{ab}$|Time-average VIP flow for $k$ over $(a,b)$|
|$\mc{B}_{n,i}$|i-th among all $\sigma_n$ possible cache placement sets at $n$; if $(k,j) \in \mc{B}_{n,i}$ during $t$, $s^k_{n_j}(t)$ = 1|
|$\beta_{n,i}$|Fraction of time objects at $n$ are placed according to $\mc{B}_{n,i}$; $0 \leq \beta_{n,i} \leq 1$ and $\sum^{\sigma_n}_{i=1} \beta_{n,i} = 1$|


::: {.footer}
Optimization Framework
:::

## Analysis {.smaller}

Given VIP arrival rate vector $\boldsymbol{\lambda}$, if there exists $\boldsymbol{\epsilon}$ such that $\boldsymbol{\lambda} + \boldsymbol{\epsilon} \in \Lambda$, then the network of VIP queues under the proposed algorithm satisfies the following:
```{=tex}
\begin{equation}
    \lim\limits_{T \rightarrow \infty} \frac{1}{T} \sum\limits^{T-1}_{t=0} \sum\limits_{\nin, \kin} \mathbb{E}[V^k_n(t)] \leq \frac{NB}{\epsilon} + \frac{\omega}{2\epsilon} \minpen
\end{equation}
```
```{=tex}
\begin{equation}
    \lim\limits_{T \rightarrow \infty} \frac{1}{T}\sum\limits^{T-1}_{t=0} \mathbb{E}[p(t)] \leq \frac{2NB}{\omega} + \minpen
\end{equation}
```
```{=tex}
\begin{equation}
    B \triangleq \frac{1}{2N} \sum_{\nin} \bigg((\mu^{out}_{n,max})^2 + 2(\mu^{out}_{n,max})r_{n,max} + \big(\textstyle \sum_{\kin}A^k_{n,max} + \mu^{in}_{n,max} + r_{n,max})^2 \bigg)
\end{equation}
```
$\Psi(\boldsymbol{\lambda})$ is the minimum time-average sum penalty achievable by a feasible and stabilizing randomized policy.

::: {.footer}
Optimization Framework
:::

## Solutions to Optimization Problems {.smaller}
**Caching**: We can rewrite the optimization problem for the caching step.
```{=tex}
\begin{align}
    \text{maximize}
        & \quad \sum\limits_{\kin} \sum\limits_{\jin} r_{n_j} V^k_{n}(t) s^k_{n_j}(t) - p^k_{n_j}(t) \\
    \text{subject to}
        & \quad \sum\limits_{\kin} s^k_{n_j}(t) \leq L_{n_j}, \; \jin \\
        & \quad \sum\limits_{\jin} s^k_{n_j}(t) \leq 1, \; \kin \\
        & \quad s^k_{n_j}(t) \in \{0, 1\}, \; \kin, \; \jin
\end{align}
```
```{=tex}
\begin{equation}
    p^k_{n_j}(t) \triangleq 
    \left\{ \begin{array}{ll}
        c^a_{n_j}, & \text{if} \; s^k_{n_j}(t) - s^k_{n_j}(t-1) = 1 \\
        c^e_{n_j}, & \text{if} \; s^k_{n_j}(t) - s^k_{n_j}(t-1) = -1 \\
       0, & \text{otherwise}
   \end{array} \right.
\end{equation}
```

::: {.footer}
Optimization Framework
:::

## Solutions to Optimization Problems {.smaller}
**Caching**: We can rewrite the optimization problem for the caching step.
```{=tex}
\begin{align}
    \text{maximize}
        & \quad \sum\limits_{\kin} \sum\limits_{\jin} b^k_{n_j}(t) s^k_{n_j}(t) \\
    \text{subject to}
        & \quad \sum\limits_{\kin} s^k_{n_j}(t) \leq L_{n_j}, \; \jin \\
        & \quad \sum\limits_{\jin} s^k_{n_j}(t) \leq 1, \; \kin \\
        & \quad s^k_{n_j}(t) \in \{0, 1\}, \; \kin, \; \jin
\end{align}
```
```{=tex}
\begin{equation}
    b^k_{n_j}(t) \triangleq \left\{ \begin{array}{ll}
        r_{n_j} V^k_n(t) - \omega c^a_{n_j}, & \text{if} \; s^k_{n_j}(t-1) = 0 \\
        r_{n_j} V^k_n(t) + \omega c^e_{n_j}, & \text{if} \; s^k_{n_j}(t-1) = 1
    \end{array} \right.
\end{equation}
```

::: {.footer}
Optimization Framework
:::

## Solutions to Optimization Problems {.smaller}
**Caching**: Using a transformation of variables, the problem can be written as a linear assignment problem. We can interpret this as deciding on caching variables per one-object cache space associated with each cache tier, rather than per-tier[^1].
```{=tex}
\begin{align}
    \text{maximize}
        & \quad \sum\limits_{\kin} \sum\limits_{\iin} b^k_{n_i}(t) s^k_{n_i}(t) \\
    \text{subject to} 
        & \quad \sum\limits_{\kin} s^k_{n_i}(t) \leq 1, \; \iin \\
        & \quad \sum\limits_{\iin} s^k_{n_i}(t) \leq 1, \; \kin \\
        & \quad s^k_{n_i}(t) \in \{0, 1\}, \; \kin, \; \iin
\end{align}
```
$\mc{I}_n = \{1, 2, ..., \sum_{\jin} L_{n_j}\}$. If $i \in \mc{I}_{n_j} = \{1 + \sum^{j - 1}_{\ell = 1} L_{n_\ell}, ..., \sum^{j}_{\ell = 1} L_{n_\ell}\}$, then $b^k_{n_j}(t) = b^k_{n_i}(t)$ and $s^k_{n_j}(t) = s^k_{n_i}(t)$.

[^1]: Only possible due to same size objects assumption

::: {.footer}
Optimization Framework
:::

## Solutions to Optimization Problems {.smaller}

![](images/gap.svg)

::: {.footer}
Optimization Framework
:::

## Solutions to Optimization Problems {.smaller}

![](images/rlap.svg)

::: {.footer}
Optimization Framework
:::

## Solutions to Optimization Problems {.smaller}
**Forwarding**: The solution to the forwarding optimization problem is as in (Yeh, 2014).
```{=tex}
\begin{align}
    \text{maximize}
        & \quad \sum\limits_{\kin} \sum\limits_{\abin^k} \mu^k_{ab}(t) (V^k_a(t) - V^k_b(t)) \\
    \text{subject to} 
        & \quad \sum\limits_{\kin} \mu^k_{ab}(t) \leq C_{ba}, \; \abin \\
        & \quad \mu^k_{ab}(t) \geq 0, \; \kin, \; \abin
\end{align}
```

::: {.footer}
Optimization Framework
:::

## Solutions to Optimization Problems {.smaller}
**Forwarding**: The solution to the forwarding optimization problem is as in (Yeh, 2014).
```{=tex}
\begin{equation}
   \mu^k_{ab}(t) =
   \left\{ \begin{array}{ll}
      C_{ba}, & \text{if} \; k = k^*_{ab}(t) \; \text{and} \; W^k_{ab}(t) > 0 \\
      0, & \text{otherwise}
   \end{array} \right.
\end{equation}
```
```{=tex}
\begin{equation}
\begin{split}
   W^k_{ab}(t) & \triangleq V^k_a(t) - V^k_b(t), \\
   k^*_{ab}(t) & \triangleq \argmax\limits_{\{k: \abin^k\}} W^k_{ab}(t)
\end{split}
\end{equation}
```

::: {.footer}
Optimization Framework
:::

# Data Plane Strategy

## Transition to Data Plane

There are several considerations when applying our approach in the data plane:

-  Per-packet forwarding and caching decisions need to be made
-  Assumption that we have immediate access to any object for caching purposes does not hold 
-  Algorithm can lead to oscillatory behavior for caching decisions

::: {.footer}
Data Plane Strategy
:::

## Data Plane Caching {.smaller}

Define the *cache score* metric as follows:
```{=tex}
\begin{equation}
    CS^k_n(t) \triangleq \frac{1}{T} \sum^t_{\tau = t - T + 1} \sum_{(a,n) \in \mc{L}^k} v^k_{an}(\tau)
\end{equation}
```

Define the *eviction target* for tier $j$ at node $n$ as follows:
```{=tex}
\begin{equation}
   k'_{n_j} = \argmin_{\{k \in \mc{K}_{n_j}\}} CS^k_n(t)
\end{equation}
```

| Notation | Definition |
|:--:|:--------|
|$\mc{v}^k_{ab}$|Actual number of VIPs for $k$ transmitted over $(a,b)$ during $t$|
|$T$|Size of a sliding window of time slots|
|$\mc{K}_{n_j}$|Set of objects currently cached in tier $j$ at $n$|

::: {.footer}
Data Plane Strategy
:::

## Data Plane Caching {.smaller}

Define the *cache benefit* metric as follows:
```{=tex}
\begin{equation}
    CB^k_{n_j}(t) = \begin{cases}
        \begin{array}{l}
             r_{n_j}(CS^k_n(t) - CS^{k'_{n_j}}_n(t)) - \omega(c^a_{n_j} + c^e_{n_j}), \text{if} \, j \, \text{is full} \\
             r_{n_j} CS^k_n(t) - \omega c^a_{n_j}, \; \text{otherwise}
        \end{array}
    \end{cases} 
\end{equation}
```

**Data Plane Caching**: When a data object $k \not \in \mc{K}_{n_j}$ arrives at $n$ during slot $t$, caching policy at $n$ behaves as follows.

-  Determine the cache tier which offers the highest cache benefit, i.e. $j^* = \argmax_{\{ \jin \}} CB^k_{n_j}(t)$
-  If $CB^k_{n_{j^*}}(t) > 0$, admit object into tier $j^*$
   -  If $j^*$ is full, i.e. $|\mc{K}_{n_{j^*}}| = L_{n_{j^*}}$, $k$ replaces $k'_{n_{j^*}}$
-  If a replacement happened in $j^* < |\mc{J}_n|$, set $k = k'_{n_{j^*}}$, then start the process over

::: {.footer}
Data Plane Strategy
:::

## Data Plane Forwarding

**Data Plane Forwarding**: Forwarding strategy is as in (Yeh, 2014). When a request for object $k \not \in \mc{K}_{n_j}, \; \forall \jin$, arrives at $n$, the request is forwarded to the following node.

```{=tex}
\begin{equation}
    b^k_n(t) = \argmax_{\{ b:(n,b) \in \mc{L}^k \}} \frac{1}{T} \sum\limits^t_{t'=t-T+1} v^k_{nb}(t)
\end{equation}
```

::: {.footer}
Data Plane Strategy
:::

## Chunk-level Decisions

At the chunk-level in the data plane, we observe the following principles:

-   If a data object is admitted to (evicted from) a cache tier, all its chunks must be admitted to (evicted from) that tier.
-   Forwarding decision is made upon receiving request for a first chunk. Requests for subsequent chunks are forwarded to the same node.

::: {.footer}
Data Plane Strategy
:::

# Experiments

## Simulation Setup

-  Object-level simulation environment built with Python using the SimPy library
-  Modular, object-oriented design to allow future extension
-  Multi-threading allowing for thread-per-experiment execution
-  Experiment parameters and output in JSON format for easy parsing
-  Thorough statistic logging

::: {.footer}
Experiments
:::

## Simulation Setup {.smaller}

::: {.panel-tabset}

### Node

```{.python}
class Node(object):
   # ...
   def packetReceiver(self, remote_id):
      while True:
         pkt = yield self.in_links[remote_id].get()
         if pkt.isData():
               yield self.env.timeout(1/self.in_links[remote_id].link_cap)
         if pkt.isData() or pkt.isInterest():
               self.pkt_buffer.put((remote_id, pkt))
   def packetProcessor(self):
      while True:
         remote_id, pkt = yield self.pkt_buffer.get()
         if pkt.isInterest():
               self.receiveInterest(remote_id, pkt.request)
         elif pkt.isData():
               self.receiveData(pkt.request)
   # ...
   def forwardInterest(self, request):
        self.sendInterestPacket(self.fib[request.object_id][0], request)
        return

    def decideCaching(self, object_id):
        yield self.env.timeout(0)
        return
   # ...
```

### Cache

```{.python}
class Cache(object):
   def cacheController(self):
      while True:
         task = yield self.task_queue.get()
         if task.type == 'r':
               obj = yield self.env.process(self.readProcess(task.object_id))
               self.out_buffer.put(obj)
         elif task.type == 'w':
               yield self.env.process(self.writeProcess(task.object_id))
   #...
   def readObject(self, object_id):
      self.stats['reads'] += 1            
      task = CacheTask(type = 'r', object_id = object_id)
      tic = self.env.now
      self.task_queue.put(task)
      obj = yield self.out_buffer.get()
      self.stats['read_delay'] += self.env.now - tic
      return obj
   def cacheObject(self, object_id):
      self.stats['writes'] += 1
      self.contents.append(object_id)
      self.cur_size += 1
      task = CacheTask(type = 'w', object_id = object_id)
      self.task_queue.put(task)
      return True
```

### Link

```{.python}
class Link(object):
   def __init__(self, env, link_cap, prop_delay):
      self.env = env
      self.link_cap = link_cap
      self.pipe = sp.Store(env)
   def put(self, pkt):
      self.pipe.put(pkt)
   def get(self):
      return self.pipe.get()

class VipLink(object):
    def __init__(self, env):
        self.env = env
        self.update_pipe = sp.Store(env)
        self.vip_pipe = sp.Store(env)    
    def pushUpdate(self, pkt):
        self.update_pipe.put(pkt)    
    def getUpdate(self):
        return self.update_pipe.get()    
    def pushVips(self, pkt):
        self.vip_pipe.put(pkt)    
    def getVips(self):
        return self.vip_pipe.get()
```

### Caching Policy

```{.python}
class UNIFNode(Node):
   def decideCaching(self, object_id):
      cache = np.random.choice(self.caches)
      if cache.isFull():
         victim_id = np.random.choice(cache.contents)
         yield self.env.process(cache.replaceObject(victim_id, object_id))
      else:
         cache.cacheObject(object_id)
```

### Forwarding Policy

```{.python}
class RoundRobinNode(Node):
   def addFIB(self, fib):
      super().addFIB(fib)
      self.link_queues = {}
      for k in fib:
         self.link_queues[k] = deque(fib[k])
   
   def forwardInterest(self, request):
      object_id = request.object_id
      remote_id = self.link_queues[object_id][0]
      self.link_queues[object_id].rotate(1)
      self.sendInterestPacket(remote_id, request)
```
:::

::: {.footer}
Experiments
:::

## Experiment Setting {.smaller}

-  Experiments over four topologies: Abilene, GEANT, 4x4 Grid, 3-regular graph
-  All nodes have caches, can be content sources and can generate requests
-  For each $k$ a node is selected as $\mc{S}(k)$ uniformly at random
-  Content sources have infinite read rate*
-  Routing scheme: Allow $n$ to forward requests for $k$ to any neighbor on *any* shortest path between $n$ and $\mc{S}(k)$ (in number of hops)*
-  Multi-tier adapted cost-unaware replacement baselines: LRU, LFU, FIFO, RANDOM
   -  LRU, FIFO and RANDOM paired with LCE admission
-  Multi-tier adapted cost-aware baseline: Penalty Aware LFU
-  All baseline caching policies paired with Least Response Time forwarding
-  Multi-tier VIP ran with slot length of 1 sec and sliding window of size $T=100$

::: {.footer}
Experiments
:::

## Experiment Setting {.smaller .scrollable}

| Parameter | Description | Value |
|:-:|:-:|:-:|
|$K$|Number of objects in catalog $\mc{K}$|1000|
|$\sum_{\kin} \lambda^k_n$|Total request arrival rate at each $n$ in objects/sec|10|
|$\alpha$|Zipf's law parameter for object popularity distribution|0.75|
|$C_{ab}$|Capacity of each link $\abin$, in objects|10|
|$L_{n_1}$|Capacity of tier 1 at each $n$, in objects|5|
|$r_{n_1}$|Read rate of tier 1 at each $n$, in objects/sec|20|
|$c^a_{n_1}$, $c^e_{n_1}$| Admission and eviction costs of tier 1 at each $n$|4, 2|
|$r_{n_2}$|Read rate of tier 1 at each $n$, in objects/sec|10|
|$c^a_{n_2}$, $c^e_{n_2}$| Admission and eviction costs of tier 1 at each $n$|2, 1|
: Parameters for all topologies {tbl-colwidths="[10,80,10]"}

::: {.footer}
Experiments
:::

## Results I {.smaller}
**Scenario**: No penalties ($\omega=0$) and $L_{n_2} = 100$ at each $n$.

Total delay, as fraction of total delay without any caching

![](images/tops_delay_v3.svg){width=75% height=75% fig-align="center"}

Cache hits, percentage of hits in the first tier on the left, total number of hits on the right

![](images/tops_hits_v3.svg){width=75% height=75% fig-align="center"}

::: {.footer}
Experiments
:::

## Results II {.smaller}
**Scenario**: Exclude cost-unaware policies, vary $\omega$. Pick best-performing $L_{n_2}$ value for each of LFU and VIP.

Delay vs. penalty ($\omega$ decreasing to the right)

::: columns
::: {.column width="33%"}
![](images/pen_vs_delay_abilene_v2.svg)
:::
::: {.column width="33%"}
![](images/pen_vs_delay_grid_v2.svg)
:::
::: {.column width="33%"}
![](images/pen_vs_delay_regular_v2.svg)
:::
:::

::: {.footer}
Experiments
:::

# Proposed Work

## Improvements

::: {.fragment}
There is room for improvement
:::

::: {.fragment}
![... and it's a big room](images/big-room.webp){width=65% height=65% fig-align="left"}
:::

::: {.footer}
Super Funny Joke
:::

## Cache Read Bandwidth {.smaller}

**Cache Bandwidth Constraint**: Current model assumes read rate of each object is independent of all others.
```{=tex}
\begin{equation}
    V^k_n(t+1) \leq \Bigg(\Big(V^k_n(t) - \sum\limits_{b \in \mathcal{N}}\mu^k_{nb}(t)\Big)^+ + A^k_n(t)
    + \sum\limits_{a \in \mathcal{N}}\mu^k_{an}(t) - \sum\limits_{j \in \mathcal{J}_n} \color{red}{r_{n_j}} s^k_{n_j}(t) \Bigg)^+    
\end{equation}
```
We need to make this more accurate when considering slower caches.
```{=tex}
\begin{equation}
   \small{V^k_n(t+1) \leq \Bigg(\Big(V^k_n(t) - \sum\limits_{b \in \mathcal{N}}\mu^k_{nb}(t)\Big)^+ + A^k_n(t)
   + \sum\limits_{a \in \mathcal{N}}\mu^k_{an}(t) - \sum\limits_{j \in \mathcal{J}_n} \color{red}{r^k_{n_j}(t)} s^k_{n_j}(t) \Bigg)^+}    
\end{equation}
```
```{=tex}
\begin{equation}
   \sum_{\kin} r^k_{n_j}(t) \leq r_{n_j}, \; \forall t   
\end{equation}
```

::: {.footer}
Proposed Work
:::

## Cache Read Bandwidth {.smaller}

**Cache Bandwidth Constraint**: This seemingly small change has the following consequences we have to reanalyze:

-  We now have a third control action each time slot, which is in product form with another control action
```{=tex}
\begin{equation}
\text{maximize} \quad \sum\limits_{\kin} \sum\limits_{\jin} V^k_{n}(t) r^k_{n_j}(t) s^k_{n_j}(t) - p^k_{n_j}(t)
\end{equation}
```
-  This changes the stability region analysis significantly, as it involves computing expected values for VIP flows and cache placements using a randomized policy

::: {.footer}
Proposed Work
:::

## Cache Read Bandwidth {.smaller}

**Cache Bandwidth Constraint**: This seemingly small change has the following consequences we have to reanalyze:

-  As the read rate for each object each slot is no longer a constant, terms in various expressions that relate to VIPs that are drained via caching will now resemble a flow variable, much like other terms
```{=tex}
\begin{equation}
   F^k_{n_j}(t) \triangleq r^k_{n_j}(t) s^k_{n_j}(t), \qquad f^k_{n_j} = \frac{1}{t} \sum^{t}_{\tau=1} r^k_{n_j}(\tau) s^k_{n_j}(\tau)
\end{equation}
```
-  While this change makes the stability region analysis a bit more complicated, it is likely to reduce the complexity of the solution to the caching problem (*currently a strong hypothesis*)

::: {.footer}
Proposed Work
:::

## Cache Exclusivity {.smaller}

**Cache Exclusivity Constraint**: There is good reason to enforce this constraint in view of interest aggregation and hardware limitations. However, there are strong arguments in support of removing it:

-  Under cache exclusivity, the data plane strategy can't naturally handle *upward migrations*
-  Lifting this constraint would make the algorithm less complex as it would simply operate over individual tiers independently
   -  This also has the benefit of not requiring any adaptations to baselines
-  As peer-to-peer DMA becomes more widely available and reliable, the assumption that multiple cache devices can supply the link simultaneously and increase read rates may be better supported

While this alternative does not require significant reanalysis, it does require extensive testing as it is not immediately clear whether it would converge to overly redundant cache placements.

::: {.footer}
Proposed Work
:::

## Unequal Object Sizes {.smaller}

**Equal Object Sizes Assumption**: While this assumption makes our analysis and solution much more straightforward, it is hardly realistic.

Let $z^k$ be the size of object $k$ in bits. Redefine $L_{n_j}$ be the cache capacity of tier $j$ at node $n$ in bits, instead of objects. The caching problem changes as follows:
```{=tex}
\begin{align}
    \text{maximize}
        & \quad \sum\limits_{\kin} \sum\limits_{\jin} r_{n_j} V^k_{n}(t) s^k_{n_j}(t) - p^k_{n_j}(t) \\
    \text{subject to}
        & \quad \sum\limits_{\kin} z^k s^k_{n_j}(t) \leq L_{n_j}, \; \jin \\
        & \quad \sum\limits_{\jin} s^k_{n_j}(t) \leq 1, \; \kin \\
        & \quad s^k_{n_j}(t) \in \{0, 1\}, \; \kin, \; \jin
\end{align}
```

::: {.footer}
Proposed Work
:::

The problem can no longer be rewritten as a LAP and, as a GAP, it is known to be NP-hard.

## Unequal Object Sizes {.smaller}

**Equal Object Sizes Assumption**: This change brings with it a set of other considerations:

-  If combined with the enforcing of cache bandwidth constraints, the problem may reveal a low-complexity "greedy" solution (*currently a hypothesis*)
-  However, we also have to reevaluate how we treat VIPs for objects
   -  Copies of a larger object are produced at a slower rate
   -  Therefore, VIPs for a smaller object can be drained at a higher rate
   -  If we do not alter the VIP metric to account for object sizes, this may skew network service (both caching and forwarding) to prioritize smaller objects

::: {.fragment .semi-fade-out}
-  Combining multi-tiered caches with varying object sizes also raises questions about *cache integrity*, i.e. should we mandate that nodes cache all chunks of an object in the same tier?
   -  If not, the complexity of the caching problem increases dramatically
   -  It may be better to approach this version of the problem with a chunk-level strategy
:::

::: {.footer}
Proposed Work
:::

## Gap Between Planes {.smaller}

**Control Plane - Data Plane Gap**: The algorithm and its analysis in the virtual control plane does not directly transfer to the data plane.

-  In the virtual plane, it is assumed that a node can immediately retrieve and cache any object in the network
-  The backpressure-based forwarding strategy in the virtual plane is prone to making impractical choices at a per-time-slot granularity, specifically in low traffic scenarios
-  These and other disparities lead to a *gap* between the two planes that is difficult to amend
-  While the data plane strategy works decently in practice, I believe this gap must be studied and quantified, even if it can't be practically amended. For instance:
   -  Measure the delay between a caching decision being made in the virtual plane, and if/when that decision is realized in the data plane
   -  Estimate the difference between performance and cost metrics that would be obtained under virtual plane assumptions vs. experiment results

::: {.footer}
Proposed Work
:::

## Uh, Better Experiments, Obviously {.smaller}

**Simulation Performance and Reproducibility**: The simulation framework performs decently and is relatively clean, but it can be improved significantly.

-  Python, while convenient, is not the best choice to build highly-scalable DES frameworks. Julia is a great alternative that maintains convenience while providing much better performance
-  Per-experiment multi-threading is not capturing parallelism effectively. Building the core components of the simulator in a multi-threaded way to improve *scale-up*, then enabling per-experiment *scale-out* will greatly improve performance.
-  The current codebase has useful comments but little documentation and examples. Overall it doesn't offer the best basis for reproducibility and future improvements.
-  Addressing these will result in a robust codebase that allows me to run a more thorough experimental study, improve the credibility of my results and potentially give other researchers a tool to build upon

::: {.footer}
Proposed Work
:::

# Appendices

## Appendix A: Model Notation {.smaller .scrollable}

| Notation | Definition |
|:--:|:--------|
|$\mathcal{G}$|Directed graph representing the network topology|
|$(\mathcal{N},\mathcal{L})$|Set of nodes and links in $\mathcal{G}$|
|$C_{ab}$|Transmission capacity (in objects/sec) of link $(a,b)$|
|$\mathcal{K}$|Set of data objects in the network|
|$\mc{S}(k)$|Content source node for $k \in \mathcal{K}$|
|$\mathcal{J}_n$ | Set of cache tiers at node $n \in \mathcal{N}$|
|$L_{n_j}$ | Size (in objects) of cache tier $j \in \mathcal{J}_n$ at $n$|
|$r_{n_j}$ | Read rate of tier $j$ at node $n$|
|$c^a_{n_j}$| Admission cost of tier $j$ at node $n$|
|$c^e_{n_j}$| Eviction cost of tier $j$ at node $n$|
|$\lambda^k_n$|Exogenous request arrival rate for $k$ at $n$|
|$t$|Time slot referring to time interval $[t, t+1)$|
|$A^k_n(t)$|Number of exogenous requests for $k$ at $n$ during $t$|
|$s^k_{n_j}(t)$ | Caching state of $k$ in tier $j$ at $n$ at the beginning of $t$|
|$p^k_{n_j}(t)$ | Penalty incurred by the choice of $s^k_{n_j}(t)$|
|$p(t)$|Sum penalty incurred during $t$|
|$\omega$|Penalty importance weight|
|$V^k_n(t)$|VIP count for $k$ at $n$ during $t$|
|$\mathbf{V}(t)$|Vector of VIP queue states during $t$|
|$\mu^k_{ab}(t)$|Allocated rate of VIPs for $k$ over $(a,b)$ during $t$|
: Table of Notations {tbl-colwidths="[20,80]"}

::: {.footer}
System Model Notation
:::

## Appendix B: Analysis Notation {.smaller .scrollable}

| Notation | Definition |
|:--:|:--------|
|$f^k_{ab}$|Time-average VIP flow for $k$ over $(a,b)$|
|$F^k_{ab}(t)$|Number of VIPs of $k$ sent over $(a,b)$ during slot $t$|
|$N$|Number of nodes in the network, i.e. $N = |\mc{N}|$|
|$A^k_{n,max}$| Finite value such that $A^k_n(t) \leq A^k_{n,max}$ for all $t$|
|$A_{n,max}$| Maximum total exogenous arrivals at node $n$ during $t$ across all $\kin$, i.e. $A_{n,max} \triangleq \sum_{\kin}A^k_{n,max}$|
|$\mu^{out}_{n,max}$| Maximum total allocated transmission rate for VIPs across all $(n,b) \in \mc{L}$, i.e. $\mu^{out}_{n,max} \triangleq \sum_{\bin}C_{nb}$|
|$\mu^{in}_{n,max}$| Maximum total allocated transmission rate for VIPs across all $(a,n) \in \mc{L}$, i.e. $\mu^{in}_{n,max} \triangleq \sum_{\bin}C_{an}$|
|$r_{n,max}$| Maximum total rate that can be served by all cache tiers at $n$|
|$\Psi(\boldsymbol{\lambda})$|Minimum time-average sum penalty achievable by a feasible and stabilizing randomized policy|