---
title: "Placeholder Title"
subtitle: "Faruk Volkan Mutlu - PhD Proposal Review"
author: "Research Advisor: Edmund Yeh<br>Committee Members: Elif Uysal, Stratis Ioannidis"
format:
  revealjs:
    slide-number: true
    theme: white
    logo: images/neu_logo.svg
    css: styles/logo.css
---

## Outline

1. Introduction
   i. Background
   ii. Research Problems
   iii. Related Work
2. Work So Far
   i. Multi-tier (*Working title*)
   ii. N-DISE (*Working title*)
   iii. HetNet (*Working title*)
3. Proposed Work

::: {.footer}
Outline
:::

::: {.hidden}
$$
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\nin}{n \in \mc{N}}
\newcommand{\kin}{k \in \mc{K}}
\newcommand{\jin}{j \in \mc{J}_n}
\newcommand{\iin}{i \in \mc{I}_n}
\newcommand{\ain}{a \in \mc{N}}
\newcommand{\bin}{b \in \mc{N}}
\newcommand{\abin}{(a,b) \in \mc{L}}
\newcommand{\about}{(a,b) \not\in \mc{L}^k}
\DeclareMathOperator*{\argmax}{arg\,max}
$$
:::

# Introduction
<h3>Background</h3>

## Information Centric Networking

- **Motivation**: Internet today is primarily a data distribution network, yet its principles are those of a communication network
- **Core Premise**: Make uniquely named data the primary entity of the network, instead of endpoints
- **Advantages**: Improve efficiency and scalability by decoupling data from location and session

::: {.footer}
Background
:::

## In-network Caching

- **Motivation**: Current caching infrastructure is mostly proprietary and centralized
- **Core Premise**: Enable every router in the network to maintain its own cache
- **Advantages**: Caching becomes decentralized, reducing network congestion and improving scalability

::: {.footer}
Background
:::

## Wireless Edge & HetNets

- Interface between local devices and wider network infrastructure
- Many edge devices are connected wirelessly and through various access technologies (HetNet)
- **Importance**: Edge-computing is gaining prevalance, driving more computation towards the network edge

::: {.footer}
Background
:::

# Introduction
<h3>Research Problems</h3>

# Introduction
<h3>Related Work</h3>

# Work So Far
<h3>Multi-tier</h3>

## Motivation - Toy Example {.smaller}

![](images/toynet.svg){width=60% height=60% fig-align="center"}

::: columns
::: {.column width="50%"}
| Notation | Definition |
|:-:|:---------------|
|$C_{ab}$|Capacity of link $(a,b)$|
|$\mathcal{N}_c$|Set of consumer nodes|
|$\mathcal{K}$|Set of data objects|
|$\alpha$|Zipf's law parameter|
|$L_{a}$|Cache size (in packets) at node $a$|
|$\lambda$|Total incoming request rate at forwarder|
:::
::: {.column width="50%"}
-  Catalog of $|\mathcal{K}|=1000$ single-packet objects, ranked in popularity according to Zipf's law with parameter $\alpha=0.75$
-  Consumer nodes send requests to the forwarder node, which either responds from its cache or forwards them upstream to server
-  Caching at the forwarder is clairvoyant, i.e. most popular $L_{f}$ objects are cached
:::
:::

## Motivation - Toy Example {.smaller}

::: columns
::: {.column width="50%"}
![](images/zipf_1k.svg){.fragment}

::: {.fragment}
-  Most popular 10% of objects make up half of all requests
-  Most popular 1% of objects make up one fifth of all requests
:::
:::
::: {.column width="50%"}
![](images/cache_size.svg){.fragment}

::: {.fragment}
-  Increasing cache sizes can help reduce delay significantly
-  But the gain from size increase has diminishing returns
:::
:::
:::

## Core Challenge {.incremental}
-  DRAM is the primary cache device since it is fast, i.e. can operate at *line rate*
-  However, DRAM offers small cache sizes at a high and exponentially increasing cost ($3 to $12 / GB)
-  Next best storage element is the NVMe SSD, offering large cache sizes at significantly lower costs ($0.1 / GB)
-  NVMe SSDs can be an order of magnitude slower than DRAM

## Return to Toy Example {.smaller}

![](images/toynet_cache.svg){width=75% height=75% fig-align="center"}

::: columns
::: {.column width="50%"}
| Notation | Definition |
|:-:|:---------------|
|$r_{n_j}$|Read rate of tier $j$ at $n$|
|$L_{n_j}$|Cache size of tier $j$ at $n$|
:::
::: {.column width="50%"}
-  Same catalog and popularity distribution, three possible cache tiers
-  Objects are cached in order of popularity, starting from tier 1 and moving onto next tier when current one is full
:::
:::

## Key Observations {.incremental .smaller}

::: columns
::: {.column width="50%"}
![](images/two_tiers.svg){.fragment}

::: {.fragment}
-  Although second tier operates slower than line rate, it improves delay at high request rates
-  This is due to the diminishing returns emerging from Zipf's law
:::
:::
::: {.column width="50%"}
![](images/three_tiers.svg){.fragment}

::: {.fragment}
-  Addition of the third tier impacts performance negatively
-  **Lesson**: Cache tiers need to outpace their own *hit rate*
:::
:::
:::

## Multi-tiered Caching Policy {.incremental}
-  **Goal**: A caching policy suitable for systems with a combination of different storage elements as caches
-  This policy needs to:
   -  Balance frequency of cache hits served by cache tiers based on their transfer rates
   -  Be adaptable to changing traffic patterns in the network
   -  Offer distributed implementation with performance guarantees

## Coupling Caching and Forwarding {.incremental}
-  **Approach**:

## VIP Framework
-  Couples backpressure-forwarding with caching
-  Provides throughput optimality guarantee*
-  Incorporates transfer rate of cache in optimization framework

## System Model {.smaller .scrollable}

| Notation | Definition |
|:--:|:--------|
|$\mathcal{G}$|Directed graph underlining the network topology|
|$(\mathcal{N},\mathcal{L})$|Set of nodes and links in $\mathcal{G}$|
|$C_{ab}$|Transmission capacity of link $(a,b)$|
|$\mathcal{K}$|Set of data objects in the network|
|$src(k)$|Content source node for $k \in \mathcal{K}$|
|$\mathcal{J}_n$ | Set of cache tiers at node $n \in \mathcal{N}$|
|$L_{n_j}$ | Size of cache tier $j \in \mathcal{J}_n$ at $n$|
|$r_{n_j}$ | Read rate of tier $j$ at node $n$|
|$t$|Time slot referring to interval $[t, t+1)$|
|$V^k_n(t)$|VIP count for $k$ at $n$ during $t$|
|$A^k_n(t)$|Number of exogenous requests for $k$ at $n$ during $t$|
|$\lambda^k_n$|Exogenous VIP arrival rate for $k$ at $n$|
|$\mu^k_{ab}(t)$|Allocated rate of VIPs for $k$ over $(a,b)$ during $t$|
|$s^k_{n_j}(t)$ | Caching state of $k$ in tier $j$ at $n$ at the beginning of $t$|
: Table of Notations {tbl-colwidths="[20,80]"}

## Queue Dynamics
The VIP queue evolution for object $k$ at node $n$ can be described with the following, where $(x)^+ = max(x,0)$:
```{=tex}
\begin{equation}
\begin{split}
    V^k_n(t+1) \leq & \Bigg(\Big(V^k_n(t) - \sum\limits_{b \in \mathcal{N}}\mu^k_{nb}(t)\Big)^+ + A^k_n(t) \\ 
    & + \sum\limits_{a \in \mathcal{N}}\mu^k_{an}(t) - \sum\limits_{j \in \mathcal{J}_n} r_{n_j} s^k_{n_j}(t) \Bigg)^+    
\end{split}
\end{equation}
```

## Algorithm {.smaller .scrollable}
At the beginning of each time slot $t$, at each node $n$, observe queueues $(V^k_n(t))_{k \in \mathcal{K}, n \in \mathcal{N}}$ then perform caching and forwarding as follows:

-  **Caching**: Choose $s^k_{n_j}(t)$ for each $\kin$ and $j \in \mathcal{J}_n$ to:
```{=tex}
\begin{align}
    \text{maximize}
        & \quad \sum\limits_{\kin} \sum\limits_{\jin} r_{n_j} V^k_n(t) s^k_{n_j}(t) \label{eq:gap_obj} \\
    \text{subject to}
        & \quad \sum\limits_{\kin} s^k_{n_j}(t) \leq L_{n_j}, \; \jin \label{eq:gap_const1} \\
        & \quad \sum\limits_{\jin} s^k_{n_j}(t) \leq 1, \; \kin \label{eq:gap_const2} \\
        & \quad s^k_{n_j}(t) \in \{0, 1\}, \; \kin, \; \jin \label{eq:gap_const3}
\end{align}
```
-  **Forwarding**: Let $\mc{L}^k$ be the set of links which are allowed to transmit VIPs of object k, determined by a routing policy. For each $\kin$ and $\abin^k$, choose:
```{=tex}
\begin{equation}
   \mu^k_{ab}(t) =
   \left\{ \begin{array}{ll}
      C_{ba}, & \text{if} \; k = k^*_{ab}(t) \; \text{and} \; W^k_{ab}(t) > 0 \\
      0, & \text{otherwise}
   \end{array} \right.
\end{equation}
```
```{=tex}
\begin{equation}
\begin{split}
   W^k_{ab}(t) & \triangleq V^k_a(t) - V^k_b(t), \\
   k^*_{ab}(t) & \triangleq \argmax\limits_{\{k: \abin^k\}} W^k_{ab}(t)
\end{split}
\end{equation}
```