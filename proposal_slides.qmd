---
title: "Cost-aware Joint Caching and Forwarding in Networks with Diverse Cache Resources"
subtitle: "<s>Farouk Multi</s> Faruk Volkan Mutlu - PhD Proposal Review"
author: "Research Advisor: Edmund Yeh<br>Committee Members: Elif Uysal, Stratis Ioannidis"
format:
  revealjs:
    slide-number: true
    theme: white
    logo: images/neu_logo.svg
    css: styles/styles.css
    highlight-style: atom-one
    header-includes: <link href='https://fonts.googleapis.com/css?family=Lato' rel='stylesheet'>
mainfont: Lato
fig-cap-location: top
---

## Outline

- Primary Contribution
    -   **Introduction**: Motivation, Challenges, Related Work
    -   **Technical**: System Model, Optimization Framework
    -   **Practical**: Strategy, Experiments, Results
    -   **Proposed Work**
- Other Contributions
- Conclusion and Acknowledgements

::: {.footer}
Outline
:::

::: {.hidden}
$$
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\nin}{n \in \mc{N}}
\newcommand{\kin}{k \in \mc{K}}
\newcommand{\jin}{j \in \mc{J}_n}
\newcommand{\kjin}{(k,j) \in \mc{B}_{n,i}}
\newcommand{\iin}{i \in \mc{I}_n}
\newcommand{\ain}{a \in \mc{N}}
\newcommand{\bin}{b \in \mc{N}}
\newcommand{\abin}{(a,b) \in \mc{L}}
\newcommand{\about}{(a,b) \not\in \mc{L}^k}
\newcommand{\betasum}{\sum\limits ^{\sigma_n}_{i=1} \beta_{n,i}}
\newcommand{\betasumnl}{\sum ^{\sigma_n}_{i=1} \beta_{n,i}}
\newcommand{\betasumind}{\sum\limits ^{\sigma_n}_{i=1} \beta_{n,i}\mathbf{1}_{[\kjin]}}
\newcommand{\minpen}{\Psi(\boldsymbol{\lambda})}
\newcommand{\drift}{\Delta(\mathbf{V}(t))}
\newcommand{\pen}{\mathbb{E}[p(t)|\mathbf{V}(t)]}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\definecolor{neured}{RGB}{200, 16, 46}
$$
:::

# Introduction

## Overview

- **Context**: Information-centric networking (ICN), in-network caching, content delivery networks (CDNs)
- **Motivation**: Data volume grows exponentially, cache capacities are stagnant
- **Challenge**: Operating larger caches in a efficiently is difficult
    - Are slower storage elements viable as caches?
    - What are the costs of operating large caches?
    - How can we get the most out of caches in the network?

::: {.footer}
Introduction
:::

::: {.notes}
- To begin, here's a brief overview of my primary research that I will present today.
- First off, for context, my work is based on the Information-centric Networking paradigm, or ICN for short.
- The Internet today is built on communication network principles, but its primary use is data distribution, so ICN aims to steer it in that direction by making data the primary entity of the network, rather than endpoints.
- In particular, I'm targeting a core functionality of ICNs called in-network caching, which lets every router in the network maintain its own cache, making caching decentralized and more scalable.
- However, my work is also applicable within today's Internet, especially for systems like CDNs, where it could be used as an overlay.
- The main motivation for my work is simply that the volume of data continuously grows, but the capacities of our caches are somewhat stagnant.
- Of course there are many obstacles in the way of scaling cache capacities. We'll specifically look at three considerations: (1) are slower storage elements viable as caches? (2) what are the costs of operating large caches? (3) how can we get the most out of caches in the network?
:::

## Overview

- **Context**: Information-centric networking, content delivery networks, in-network caching
- **Motivation**: Data volume grows exponentially, cache capacities are stagnant
- **Challenge**: Operating larger caches in a efficiently is difficult
- **Goal**: Caching policy that addresses these considerations

::: {.footer}
Introduction
:::

::: {.notes}
- By the end, our goal is to come up with a policy that addresses these considerations
:::

## Use Case: Data-Intensive Science

-   Data-intensive science experiments (high-energy physics, genomics etc.) process huge amounts of data
    -   In one year of LHC running, over an exabyte of data is accessed
    -   In November 2018, 15.8 petabytes of data were written on tape at CERN
-   Data is continuously distributed across the world for research

::: {.footer}
Introduction
:::

::: {.notes}
- To expand on my motivation, I'll first talk about the use case that initially inspired my work.
- As you know, large science programs in fields like high-energy physics, genomics etc. deal with experiments that produce huge amounts of data.
- For instance, for the LHC, in one year, over an exabyte of data is accessed.
- This data also has to constantly be distributed across the world for research, which is a significant networking challenge.
:::

## Use Case: Data-Intensive Science

-   *"N-DISE: NDN-based Data Distribution for Large-Scale Data-Intensive Science"*, Y. Wu, F. V. Mutlu, et al.
    -   NDN-based high-throughput data distribution, caching and access system
    -   Each router allocated 20 GB DRAM cache space, each object in catalog around 5 GB
    -   "Increasing capacities via NVMe SSDs require novel caching strategies" (paper on NDN-DPDK forwarder)

::: {.footer}
Introduction
:::

::: {.notes}
- To address this, our lab led an effort to build a state-of-the-art, Named Data Networking based data distribution system, which you can read about in the paper we published in ICN 2022.
- The point here is, even in this state-of-the-art prototype, we were only able to allocate a small amount of cache space at each router. As you can see it is a drop in the bucket compared to the volumes we expect to be working with.
- I also want to briefly mention that, in a separate paper that describes a core technology that enabled the N-DISE system (the NDN-DPDK forwarder), the problem of scaling cache capacities and the need for specialized policies for operating larger caches was specifically outlined as part of future outlook.
- So there's a gap here that motivated my work.
:::

## The Capacity Challenge

-  DRAM is the primary cache device since it is fast, i.e. *line rate* (12.8-25.6 GB/s for DDR4, 32-64 GB/s for DDR5)
-  However, DRAM offers small capacities at a high and exponentially increasing cost ($3 to $12 / GB)
-  Next best storage element is the NVMe SSD, offering large capacities at significantly lower costs ($0.1 / GB)
-  NVMe SSDs can be much slower than DRAM (11.7 GB/s ideal read rate for fastest PCIe 5.0)

::: {.footer}
Introduction
:::

::: {.notes}
- Now I want to give you a better idea about why scaling cache capacities is such a challenge.
:::

## Capacity Challenge: Toy Example {.smaller}

![](images/toynet_cache.svg){width=75% height=75% fig-align="center"}

::: columns
::: {.column width="50%"}
| Notation | Definition |
|:-:|:---------------|
|$C_{ab}$|Capacity of link $(a,b)$|
|$\mathcal{N}_c$|Set of consumer nodes|
|$r_{n_j}$|Read rate of tier $j$ at $n$|
|$L_{n_j}$|Cache size of tier $j$ at $n$|
:::
::: {.column width="50%"}
- Catalog of 1000 objects, ranked in popularity by [Zipf's law]{.body-highlight}
- Objects cached by rank:
    - Tier 1: 1-10
    - Tier 2: 11-40
    - Tier 3: 41-100
:::
:::

::: {.footer}
Introduction
:::

::: {.notes}
- Now, to both illustrate the capacity challenge more precisely, and to underline how slower caches may help us here, I brought in a toy example.
- Here we're looking at a simple network with a catalog of objects, all hosted by one server. We assume the popularity distribution of these objects are governed by Zipf's law. We have a set of consumers making requests for these objects, which go through a forwarder. If the forwarder can respond to a request from one its cache tiers, it does so instead of forwarding it to the server. There is a fixed caching policy at the forwarder, according to a priori knowledge of object popularities, as you can see on the screen.
- We have three potential tiers of cache, with the first being a fast and small tier, the middling being a lot larger but slower, then the last one being even larger but much slower. 
:::

## Case for Additional, Slower Caches {.smaller}

::: columns
::: {.column width="50%"}
![](images/zipf_1k.svg)

-  Most popular 1% of objects make up one fifth of all requests
-  Most popular 10% of objects make up half of all requests
:::
::: {.column width="50%"}
![](images/two_tiers.svg)

-  Second tier operates slower but improves delay for high request rates
-  This is due to the diminishing returns emerging from Zipf's law
:::
:::

::: {.footer}
Introduction
:::

::: {.notes}
- First off, on the left hand side, we see the CDF for Zipf's law. The key observation there is that, in this network, 1% of objects will make up roughly one fifth of all requests, and 10% of objects will make up half of all requests.
- Now, on the right hand side, we see how adding the middling tier on top of the first tier impacts the total delay in the network. Even though the second tier is slower, it improves delay when there's more traffic. This can be linked directly to our observation from Zipf's law, because even though the second tier has a larger amount of objects cached, the frequency of requests that hit the second tier does not exceed its read capacity, since those objects are less popular.
:::

## Case for Additional, Slower Caches {.smaller}

::: columns
::: {.column width="50%"}
![](images/zipf_1k.svg)

-  Most popular 1% of objects make up one fifth of all requests
-  Most popular 10% of objects make up half of all requests
:::
::: {.column width="50%"}
![](images/three_tiers.svg)

-  Addition of the third tier impacts performance negatively
-  This is due to trading link delay with cache read delay
:::
:::

::: {.footer}
Introduction
:::

::: {.notes}
- However, adding the last tier, even though it expands our total cache capacity, impacts delay negatively, because what I just explained does not hold for the third tier since its much larger and much slower, so we end up trading delay on the server link with worse read delay on the third tier.
:::

## The Cost Challenge

-  Caches are not free to use: admissions, replacements, even idle operation has an energy cost

::: columns
::: {.column width="50%"}
![](images/dram_power.png)
:::
::: {.column width="50%"}
![](images/ssd_power.png)
:::
:::

::: {.footer}
Introduction
:::

::: {.notes}
- Now of course the capacity challenge is not the only obstacle in our way. There's another major consideration which is operational costs.
- Caches are not free to use, each admission and replacement, and even idle operation has a real cost in energy so if we're adding even more cache devices we have to be mindful of this cost.
:::

## The Cost Challenge

-  Caches are not free to use: admissions, replacements, even idle operation has an energy cost
-  SSDs also wear out over time as they're written

![](images/ssd_endurance.png)

::: {.footer}
Introduction
:::

::: {.notes}
- And on top of that, SSDs also wear out as they're written, which, although modern SSDs are very durable, is still a significant consideration because dynamic caching operates at a small time scale so storage will be rewritten quite frequently
:::

## Coupling Caching with Forwarding

-  Benefits of caching more pronounced when caching and forwarding decisions are tightly coupled
   -  "*Coupling caching and forwarding: benefits, analysis, and implementation*", G. Rossini, D. Rossi, ACM ICN 2014
-  This is even more critical with larger, slower caches

::: {.footer}
Introduction
:::

::: {.notes}
- Finally, we look at the question of, we can handle caching adequately, how can we make sure we're actually making the most of those caches in the network?
- This is where the act of tying caching and forwarding together comes in. 
:::

## Goal

Develop a joint forwarding and caching policy that:

-   Tracks user demand for objects as basis for decisions
-   Utilizes diverse cache resources effectively:
    -   Ensures devices are receiving [sustainable hit rates]{.body-highlight}
    -   Makes [high-value]{.body-highlight} replacement decisions
-   Utilizes bandwidth resources effectively:
    -   Steers requests toward caching points
    -   Forwards requests avoiding congestion

::: {.footer}
Introduction
:::

::: {.notes}
- Now, having looked at these considerations, let's reiterate our goal.
:::

## Related Work {.smaller}

-  *"Multi-Terabyte and multi-Gbps information centric routers"*, G. Rossini, D. Rossi, M. Garetto, E. Leonardi, INFOCOM 2014
   -  Two-layer cache (SSD masked behind DRAM), utilizes "prefetching", trace-driven and synthetic simulation results
-  *"Hierarchical Content Stores in High-Speed ICN Routers: Emulation and Prototype Implementation"*, R. B. Mansilha, L. Saino, M. Barcellos, M. Gallo, E. Leonardi, D. Perino, D. Rossi, ICN 2015
   -  Follow-up to above work, prototype implementation
-  *"Exploiting parallelism in hierarchical content stores for high-speed ICN routers"*, R. B. Mansilha, M. Barcellos, E. Leonardi, D. Rossi, COMNET 2017
   -  Follow-up to above works, integrates HCS design with NDN Forwarding Daemon (NFD)

::: {.footer}
Related Work
:::

::: {.notes}
- Now, to conclude the introduction, I want to briefly point out some related work.
- This connected series of papers you see on the screen, which come from the same group that authored the one about coupling caching and forwarding that I just discussed, is the closest related work in my opinion.
- These papers lay out similar concerns about cache capacities and the relevant challenges.
- However, their work targets the system layer. They deal with lower level implementation details of how to integrate SSDs into the actual data structures that make up the caches in ICNs.
- In these works, while the authors rely on traditional cache admission and replacement policies, they also point out that specialized policies may result in better performant systems.
- They also do not consider the coupling of forwarding decisions.
:::

## Related Work: VIP

*"VIP: a framework for joint dynamic forwarding and caching in named data networks"*, E. M. Yeh, T. Ho, Y. Cui, M. Burd, R. Liu, D. Leong, ACM ICN 2014

-   Couples caching and forwarding
-   Models read rate of caches
-   Virtual and data plane separation makes algorithm simpler to operate
-   Lyapunov drift analysis integrates easily with costs

::: {.footer}
Related Work
:::

::: {.notes}
- Lastly, perhaps the single most important paper for my work is the VIP paper by Prof. Yeh and co-authors, as my work directly extends this paper.
- So let me go over why that is.
- Recall the earlier paper I mentioned about coupling caching and forwarding; coincidentally these two papers appeared in the same conference and the VIP paper proposed a strategy that does exactly that, which suits our goals as well.
- Furthermore, this paper includes the read capacities of cache devices in its system model, which is one of the parameters of primary concern to us.
- The core feature of this paper is a separation of the data plane mechanisms from the "virtual plane", where control decisions take place, which we also adopt as it makes building these strategies simpler.
- Finally, there is a technical benefit here because the analysis for VIP is based on Lyapunov drift, which has an extension called drift-plus-penalty, that gives us a direct venue for incorporating cache utilization costs.
:::

# System Model

::: {.notes}
That concludes the introduction and leads me into the system model that we're dealing with.
:::

## System Model

![](images/system_model.svg){fig-align="center" width="60%" height="60%"}

::: {.footer}
System Model
:::

::: {.notes}
- Here's a simple illustration of the system model the separation and interaction between the virtual and data planes we're adopting from the VIP paper.
- The data plane is where actual network functionality is performed. The virtual plane uses the request arrival information from the data plane; its where control decisions are made.
:::

## Data Plane Model {.smaller}
Data plane network model is based on general ICN principles:

-   Unit of content is *data object*; each object has a unique source and is of equal size 
-   Requests can enter network at any node; sources or caching nodes can respond to requests with data
-   Data responses follow request path back to requester
<hr>

| Notation | Definition |
|:-:|:---------|
|$\mathcal{G}$|Directed graph representing network topology|
|$(\mathcal{N},\mathcal{L})$|Set of nodes and links in $\mathcal{G}$|
|$C_{ab}$|Transmission capacity (in objects/sec) of link $(a,b)$|
|$\mathcal{K}$|Set of data objects in network|
|$\mc{S}(k)$|Source node for $k \in \mathcal{K}$|
|$\lambda^k_n$|Exogenous request arrival rate for $k$ at $n$|

::: {.footer}
System Model
:::

::: {.notes}
- The principles of the data plane are based on those of a classical ICN.
- We use "data object" as the unit of content in the network and assume that each object has a unique source node.
- We particularly assume every object has the same size, which is a crucial assumption we'll need to remember for later.
- Requests can enter network at any node. A request for an object can be met with a data response at the source node for that object or at any node that caches that object.
- Those data responses will follow the path of the request back to the requester (inverse of the path, of course)
:::

## Data Plane Model {.smaller}
Multi-tiered caching model has following properties:

-   Any node can have cache tiers and can cache an object unless it is already the source for that object
-   A node can cache an object in at most one of its tiers (*cache exclusivity*)
-   An object evicted from one tier can migrate to another tier at the same node
<hr>

| Notation | Definition |
|:-:|:---------|
|$\mathcal{J}_n$ | Set of cache tiers at node $n \in \mathcal{N}$|
|$L_{n_j}$ | Size (in objects) of cache tier $j \in \mathcal{J}_n$ at $n$|
|$r_{n_j}$ | Read rate of tier $j$ at node $n$|
|$c^a_{n_j}$| Admission cost of tier $j$ at node $n$|
|$c^e_{n_j}$| Eviction cost of tier $j$ at node $n$|

::: {.footer}
System Model
:::

::: {.notes}
- The multi-tiered caching model I propose adds the following properties to those basic mechanisms.
- Any node can have cache tiers and can cache any object in one of those tiers unless it already sources that object.
- A node can cache an object in at most one of its tiers. This means that we don't duplicate objects across cache tiers. This is a crucial constraint we'll also need to remember for later.
- An object evicted from one tier can migrate to another tier at the same node. So an eviction from one tier is not necessarily final for an object.
:::

## Virtual Plane Model {.smaller}
Virtual plane model is based on (Yeh, 2014):

-  A *virtual interest packet* (*VIP*) is generated alongside each exogenous request entering network; each node maintains a *VIP counter* for each object in the network
-  VIPs are removed at object sources and caching nodes
-  Integer time slots depicted as $t = \{1,2,...\}$
-  Beginning of each time slot, each node decides where and how to forward its VIPs, as well as how to cache objects
<hr>

| Notation | Definition |
|:--:|:--------|
|$t$|Time slot referring to time interval $[t, t+1)$|
|$V^k_n(t)$|VIP count for $k$ at $n$ during $t$|
|$A^k_n(t)$|Number of exogenous requests for $k$ at $n$ during $t$|
|$\mu^k_{ab}(t)$|Allocated rate of VIPs for $k$ over $(a,b)$ during $t$|

::: {.footer}
System Model
:::

::: {.notes}
- Now, on to the virtual plane model, which is largely based on the original VIP paper. We'll go over it briefly.
- First off, anytime an exogenous request enters the data plane network, a virtual interest packet (VIP for short) is generated in the virtual plane at that node. These VIPs can only be removed at sources or caching points.
- Each node maintains a VIP counter for each object in the network. We can interpret these counts as queues that describe unsatisfied demand for objects. Another useful way to think of VIP counts is as a "potential" value, where the potential is high at request entry points and low at sources and caching points. We'll refer back to this interpretation later.
- We're considering integer time slots in the virtual plane where caching and forwarding decisions for each time slot are made at the beginning of that time slot and network state changes via events that happen in the data plane within the window from the beginning of one time slot to the next.
:::

## Virtual Plane Model {.smaller}
Model is expanded with multiple cache tiers and [penalties]{.body-highlight}:

-   Each cache admission and eviction in virtual plane incurs a penalty
-   A configurable weight $\omega$ lets network determine importance of penalties in objective
<hr>

| Notation | Definition |
|:--:|:--------|
|$s^k_{n_j}(t)$ | Caching state of $k$ in tier $j$ at $n$ at the beginning of $t$|
|$p^k_{n_j}(t)$ | Penalty incurred by choice of $s^k_{n_j}(t)$|
|$p(t)$|Sum penalty incurred during $t$|
|$\omega$|Penalty importance weight|

::: {.footer}
System Model
:::

::: {.notes}
- The multi-tiered caching model that I propose, which incorporates cache utilization costs, extends the virtual plane model.
- Each cache admission and eviction in virtual plane incurs a penalty, and a configurable weight parameter, $\omega$, lets the network determine the importance of penalties in its overall objective
:::

## VIP Queue and Penalty Dynamics {.smaller}
The VIP queue evolution for object $k$ at node $n$ can be described with the following, where $(x)^+ = max(x,0)$:
```{=tex}
\begin{equation}
\begin{split}
    V^k_n(t+1) \leq & \Bigg(\Big(V^k_n(t) - \sum\limits_{b \in \mathcal{N}}\mu^k_{nb}(t)\Big)^+ + A^k_n(t) 
    + \sum\limits_{a \in \mathcal{N}}\mu^k_{an}(t) - \sum\limits_{j \in \mathcal{J}_n} r_{n_j} s^k_{n_j}(t) \Bigg)^+    
\end{split}
\end{equation}
```

The penalty incurred by the caching action $s^k_{n_j}(t)$ can be defined in terms of utilization costs as follows:
```{=tex}
\begin{equation}
    p^k_{n_j}(t) \triangleq 
    \left\{ \begin{array}{ll}
        c^a_{n_j}, & \text{if} \; s^k_{n_j}(t) - s^k_{n_j}(t-1) = 1 \\
        c^e_{n_j}, & \text{if} \; s^k_{n_j}(t) - s^k_{n_j}(t-1) = -1 \\
       0, & \text{otherwise}
   \end{array} \right.
\end{equation}
```

Sum penalty incurred during slot $t$ denoted as $p(t) = \sum_{\kin, \nin, \jin} p^k_{n_j}(t)$ .

::: {.footer}
System Model
:::

::: {.notes}
- The inequality at the top shows how VIP counts change from slot to slot in the virtual plane
- First term is VIPs forwarded away from the node, then we have the exogenous arrivals, then VIPs that arrive from other nodes and finally, VIPs removed due to caching. Notice that this term is dependent on the read rate of the cache tier
- The definition at the bottom shows how the caching actions translate to penalties incurred
:::

# Virtual Plane Optimization

::: {.notes}
With the system model described, we will now look at our control algorithm in the virtual plane.
:::

## Optimization Goal {.smaller}
Operate close to VIP network stability region boundary while keeping sum penalty small, thus achieving a balance between satisfied user demand rate and incurred costs.

More precisely, our objective is to minimize the upper bound of the following [drift-plus-penalty]{.body-highlight} expression:
```{=tex}
\begin{equation}
    \drift + \omega \pen
\end{equation}
```
where $\drift$ is the drift component defined as:
```{=tex}
\begin{equation}
    \drift = \mathbb{E}[\mathcal{L}(\mathbf{V}(t+1))-\mc{L}(\mathbf{V}(t))|\mathbf{V}(t)]
\end{equation}
```
and $\mc{L}(\mathbf{V}(t))$ is the Lyapunov function defined as:
```{=tex}
\begin{equation}
    \mc{L}(\mathbf{V}(t)) = \sum\limits_{\nin, \kin} (V^k_n(t))^2
\end{equation}
```

::: {.footer}
Optimization Framework
:::

::: {.notes}
- The goal of our optimization in the virtual plane, is to operate as close as possible to the boundary of the stability region (which we will define formally later) of the network of VIP queues, while keeping penalties accumulated small
- So in this way, we're not trying to make a precisely optimal decision each time slot, but rather we're trying to minimize the upper bound on this drift-plus-penalty expression you see on screen, which is based on an extension to Lyapunov optimization devised by Neely
:::

## Virtual Plane Caching Algorithm {.smaller}
At the beginning of each time slot $t$, at each node $n$, observe queues $(V^k_n(t))_{k \in \mathcal{K}, n \in \mathcal{N}}$ then perform [caching]{.body-highlight} as follows:

- Choose $s^k_{n_j}(t)$ for each $\kin$ and $j \in \mathcal{J}_n$ to:
```{=tex}
\begin{align}
    \text{maximize}
        & \quad \sum\limits_{\kin} \sum\limits_{\jin} r_{n_j} V^k_{n}(t) s^k_{n_j}(t) - \omega p^k_{n_j}(t) \\
    \text{subject to}
        & \quad \sum\limits_{\kin} s^k_{n_j}(t) \leq L_{n_j}, \; \forall \, \jin \\
        & \quad \sum\limits_{\jin} s^k_{n_j}(t) \leq 1, \; \forall \, \kin \\
        & \quad s^k_{n_j}(t) \in \{0, 1\}, \; \forall \, \kin, \; \jin
\end{align}
```

::: {.footer}
Optimization Framework
:::

::: {.notes}
- And here's how we tackle this optimization. We devise an algorithm that's made of two steps (caching and forwarding), which chooses the respective variables every time slot, to optimize a certain objective based on the VIP counts. We're looking at the caching step right now.
- Obviously there is a lot of math to get to this expression for the objective function, but I'll give you the intuition briefly.
- Because we want to remove as many VIPs from the network as fast as possible, we want to cache the higher VIP count objects in the faster tiers, which is why the read rate multiplies the VIP count here.
- However, we also want to make sure that our choices in caching actions do not add large penalties that outweigh the benefits. Of course the importance of that is determined by omega.
:::

## Virtual Plane Forwarding Algorithm {.smaller}
At the beginning of each time slot $t$, at each node $n$, observe queues $(V^k_n(t))_{k \in \mathcal{K}, n \in \mathcal{N}}$ then perform [forwarding]{.body-highlight} as follows:

- Let $\mc{L}^k$ be the set of links which are allowed to transmit VIPs of object k, determined by a routing policy. For each $\kin$ and $\abin^k$, choose $\mu^k_{ab}(t)$ to:
```{=tex}
\begin{align}
    \text{maximize}
        & \quad \sum\limits_{\kin} \sum\limits_{\abin^k} \mu^k_{ab}(t) (V^k_a(t) - V^k_b(t)) \\
    \text{subject to} 
        & \quad \sum\limits_{\kin} \mu^k_{ab}(t) \leq C_{ba}, \; \forall \, \abin \\
        & \quad \mu^k_{ab}(t) \geq 0, \; \forall \, \kin, \; \abin \\
        & \quad \mu^k_{ab}(t) = 0, \; \forall \, \kin, \; (a,b) \not \in \mc{L}^k
\end{align}
```

::: {.footer}
Optimization Framework
:::

::: {.notes}
- And here's the forwarding step.
- The intuition here again comes from thinking of VIP counts as potential representing unsatisfied demand. So with the forwarding step, we're making sure VIP flows follow large differences in potential, essentially steering interests toward sinks, meaning sources or caches.
:::

## Analysis {.smaller}

The VIP [stability region]{.body-highlight} $\Lambda$ of the network $\mc{G} = (\mc{N},\mc{L})$, is the set $\Lambda$ consisting of all VIP arrival rates $\boldsymbol{\lambda} = (\lambda^k_n)_{\kin,\nin}$ such that the following holds:
```{=tex}
\begin{equation}
    \lambda^k_n \leq \sum\limits_{\bin} f^k_{nb} - \sum\limits_{\ain} f^k_{an} + \sum\limits_{\jin} r_{n_j} \betasumind, \; \forall \nin, \; \kin, \; n \not = \mc{S}(k)
\end{equation}
```

| Notation | Definition |
|:--:|:--------|
|$f^k_{ab}$|Time-average VIP flow for $k$ over $(a,b)$|
|$\mc{B}_{n,i}$|i-th among all $\sigma_n$ possible cache placement sets at $n$; if $(k,j) \in \mc{B}_{n,i}$ during $t$, $s^k_{n_j}(t)$ = 1|
|$\beta_{n,i}$|Fraction of time objects at $n$ are placed according to $\mc{B}_{n,i}$; $0 \leq \beta_{n,i} \leq 1$ and $\sum^{\sigma_n}_{i=1} \beta_{n,i} = 1$|


::: {.footer}
Optimization Framework
:::

::: {.notes}
- The analysis of our approach comes in two steps. We're following the structure of Lyapunov drift-based analysis, which was also used in the VIP paper, to first define the stability region of the network of VIP counts (which can be seen as queues) in the virtual plane.
- The inequality on the screen essentially defines the boundary of that stability region, relating the arrival rate that can be satisfied contingent on the long term average VIP flow over links, as well as VIPs removed via cache tiers.
:::

## Analysis {.smaller}

Given VIP arrival rate vector $\boldsymbol{\lambda}$, if there exists $\boldsymbol{\epsilon}$ such that $\boldsymbol{\lambda} + \boldsymbol{\epsilon} \in \Lambda$, then the network of VIP queues under the proposed algorithm satisfies the following:
```{=tex}
\begin{equation}
    \lim\limits_{T \rightarrow \infty} \frac{1}{T} \sum\limits^{T-1}_{t=0} \sum\limits_{\nin, \kin} \mathbb{E}[V^k_n(t)] \leq \frac{NB}{\epsilon} + \frac{\omega}{2\epsilon} \minpen
\end{equation}
```
```{=tex}
\begin{equation}
    \lim\limits_{T \rightarrow \infty} \frac{1}{T}\sum\limits^{T-1}_{t=0} \mathbb{E}[p(t)] \leq \frac{2NB}{\omega} + \minpen
\end{equation}
```
```{=tex}
\begin{equation}
    B \triangleq \frac{1}{2N} \sum_{\nin} \bigg((\mu^{out}_{n,max})^2 + 2(\mu^{out}_{n,max})r_{n,max} + \big(\textstyle \sum_{\kin}A^k_{n,max} + \mu^{in}_{n,max} + r_{n,max})^2 \bigg)
\end{equation}
```
$\Psi(\boldsymbol{\lambda})$ is the minimum time-average sum penalty achievable by a feasible and stabilizing randomized policy.

::: {.footer}
Optimization Framework
:::

::: {.notes}
- The second step of our analysis is what actually shows the guarantees that our algorithm can provide. Here, we use the definition of the stability region, which is why we needed the previous step.
- These two expressions define the upper bounds for time average VIP queue sizes and accumulated penalty in the virtual plane network.
- The essential observation here is the trade-off between the bounds. As you can see, the omega value controls which bound we wish to prioritize, because setting it small means lower average queue sizes which translates to better performance, but at the cost of potentially much larger penalties. Setting it large is the inverse of that situation.
:::

## Solutions to Optimization Problems {.smaller}
We can rewrite the optimization problem for the [caching]{.body-highlight} step.
```{=tex}
\begin{align}
    \text{maximize}
        & \quad \sum\limits_{\kin} \sum\limits_{\jin} r_{n_j} V^k_{n}(t) s^k_{n_j}(t) - \omega p^k_{n_j}(t) \\
    \text{subject to}
        & \quad \sum\limits_{\kin} s^k_{n_j}(t) \leq L_{n_j}, \; \forall \, \jin \\
        & \quad \sum\limits_{\jin} s^k_{n_j}(t) \leq 1, \; \forall \, \kin \\
        & \quad s^k_{n_j}(t) \in \{0, 1\}, \; \forall \, \kin, \; \jin
\end{align}
```
```{=tex}
\begin{equation}
    p^k_{n_j}(t) \triangleq 
    \left\{ \begin{array}{ll}
        c^a_{n_j}, & \text{if} \; s^k_{n_j}(t) - s^k_{n_j}(t-1) = 1 \\
        c^e_{n_j}, & \text{if} \; s^k_{n_j}(t) - s^k_{n_j}(t-1) = -1 \\
       0, & \text{otherwise}
   \end{array} \right.
\end{equation}
```

::: {.footer}
Optimization Framework
:::

::: {.notes}
- Now of course, the validity of our approach still hinges on whether we can solve the two optimization problems from the two steps of our algorithm.
- To do so for the caching step, we will first rewrite it to clean up this unpleasant form we've shown before. And to do that, we will use a two-step transformation of variables.
:::

## Solutions to Optimization Problems {.smaller}
We can rewrite the optimization problem for the [caching]{.body-highlight} step.
```{=tex}
\begin{align}
    \text{maximize}
        & \quad \sum\limits_{\kin} \sum\limits_{\jin} b^k_{n_j}(t) s^k_{n_j}(t) \\
    \text{subject to}
        & \quad \sum\limits_{\kin} s^k_{n_j}(t) \leq L_{n_j}, \; \forall \, \jin \\
        & \quad \sum\limits_{\jin} s^k_{n_j}(t) \leq 1, \; \forall \, \kin \\
        & \quad s^k_{n_j}(t) \in \{0, 1\}, \; \forall \, \kin, \; \jin
\end{align}
```
```{=tex}
\begin{equation}
    b^k_{n_j}(t) \triangleq \left\{ \begin{array}{ll}
        r_{n_j} V^k_n(t) - \omega c^a_{n_j}, & \text{if} \; s^k_{n_j}(t-1) = 0 \\
        r_{n_j} V^k_n(t) + \omega c^e_{n_j}, & \text{if} \; s^k_{n_j}(t-1) = 1
    \end{array} \right.
\end{equation}
```

::: {.footer}
Optimization Framework
:::

::: {.notes}
- The first step here is defining an auxiliary "benefit" term and using that to simplify the objective function.
- You'll notice that the problem now looks to be in the form of a generalized assignment problem.
:::

## Solutions to Optimization Problems {.smaller}
Using a transformation of variables, the problem can be written as a [linear assignment problem]{.body-highlight}. We can interpret this as deciding on caching variables for each one-object cache *slot*, rather than for each tier as a whole[^1].
```{=tex}
\begin{align}
    \text{maximize}
        & \quad \sum\limits_{\kin} \sum\limits_{\iin} b^k_{n_i}(t) s^k_{n_i}(t) \\
    \text{subject to} 
        & \quad \sum\limits_{\kin} s^k_{n_i}(t) \leq 1, \; \forall \, \iin \\
        & \quad \sum\limits_{\iin} s^k_{n_i}(t) \leq 1, \; \forall \, \kin \\
        & \quad s^k_{n_i}(t) \in \{0, 1\}, \; \forall \, \kin, \; \iin
\end{align}
```
$\mc{I}_n = \{1, 2, ..., \sum_{\jin} L_{n_j}\}$. If $i \in \mc{I}_{n_j} = \{1 + \sum^{j - 1}_{\ell = 1} L_{n_\ell}, ..., \sum^{j}_{\ell = 1} L_{n_\ell}\}$, then $b^k_{n_j}(t) = b^k_{n_i}(t)$ and $s^k_{n_j}(t) = s^k_{n_i}(t)$.

[^1]: Only possible due to equal object size assumption

::: {.footer}
Optimization Framework
:::

::: {.notes}
- Now please recall the assumption we made in our model that every data object has the same size, because it is critical here.
- Using that assumption, we can think of each cache tier as made up of several "cache slots" that can each take one object, depending of course on the total capacity of the tier. Each of these cache slots of course have the same read rate and cost parameters as the tier it belongs in.
- Now, also recall the cache exclusivity constraint of the original problem. We can easily see that exclusivity across tiers directly translates to exclusivity across these cache slots.
- Using these facts we can rewrite the problem again, this time in the form of a linear assignment problem.
- Luckily, there are methods that can solve this problem in polynomial time. We specifically rely on a modification of the Jonker-Volgenant algorithm.
:::

## Solutions to Optimization Problems {.smaller}
**Forwarding**: The solution to the forwarding optimization problem is as in (Yeh, 2014).
```{=tex}
\begin{align}
    \text{maximize}
        & \quad \sum\limits_{\kin} \sum\limits_{\abin^k} \mu^k_{ab}(t) (V^k_a(t) - V^k_b(t)) \\
    \text{subject to} 
        & \quad \sum\limits_{\kin} \mu^k_{ab}(t) \leq C_{ba}, \; \forall \, \abin \\
        & \quad \mu^k_{ab}(t) \geq 0, \; \forall \, \kin, \; \abin
\end{align}
```
<hr>

```{=tex}
\begin{equation}
   \mu^k_{ab}(t) =
   \left\{ \begin{array}{ll}
      C_{ba}, & \text{if} \; k = k^*_{ab}(t) \; \text{and} \; W^k_{ab}(t) > 0 \\
      0, & \text{otherwise}
   \end{array} \right.
\end{equation}
```
```{=tex}
\begin{equation}
\begin{split}
   W^k_{ab}(t) & \triangleq V^k_a(t) - V^k_b(t), \\
   k^*_{ab}(t) & \triangleq \argmax\limits_{\{k: \abin^k\}} W^k_{ab}(t)
\end{split}
\end{equation}
```

::: {.footer}
Optimization Framework
:::

::: {.notes}
- As for the solution to the forwarding step, a neat thing about our extension here is that we don't actually alter the results for forwarding from the original VIP paper.
- So we can use the backpressure solution that was devised there directly.
:::

# Data Plane Strategy

## Transition to Data Plane

There are several considerations when applying our approach in the data plane:

-  Per-packet forwarding and caching decisions
-  Assumption of immediate access to any object for caching purposes does not hold 
-  [Oscillatory]{.body-highlight} nature of VIP counts impractical for data plane caching

::: {.footer}
Data Plane Strategy
:::

::: {.notes}
- So first off, there are several things about using the virtual plane algorithm with the mechanics of the data plane
- The virtual plane decisions are made in discrete time slots, but of course the data plane decisions have to be reactive and continuous
- Another major point is that, for caching decisions, when we change the relevant variables in the virtual plane, we inherently assume that we can have access to that object immediately, this obviously won't hold in the data plane
- Finally, there's an observation made in the original VIP paper about VIP counts, that also applies here, which is that they are oscillatory. This means that, because of the decisions we make in the virtual plane, when the VIP count for an object becomes large, we immediately make decisions that will quickly drive the count down. Basing caching on this behavior directly is impractical in the data plane.
:::

## Data Plane Caching {.smaller}

Define the [cache score]{.body-highlight} metric as in (Yeh, 2014):
```{=tex}
\begin{equation}
    CS^k_n(t) \triangleq \frac{1}{T} \sum^t_{\tau = t - T + 1} \sum_{(a,n) \in \mc{L}^k} v^k_{an}(\tau)
\end{equation}
```

Define the [eviction target]{.body-highlight} for tier $j$ at node $n$ as follows:
```{=tex}
\begin{equation}
   k^{min}_{n_j}(t) = \argmin_{\{k \in \mc{K}_{n_j}(t)\}} CS^k_n(t)
\end{equation}
```
<hr>

| Notation | Definition |
|:-:|:---------|
|$\mc{v}^k_{ab}$|Actual number of VIPs for $k$ transmitted over $(a,b)$ during $t$|
|$T$|Size of sliding window in time slots|
|$\mc{K}_{n_j}(t)$|Set of objects cached in tier $j$ at $n$ during $t$|

::: {.footer}
Data Plane Strategy
:::

::: {.notes}
- So to address all of these, we begin by adopting the cache score metric, which is based on a sliding window average of received VIPs
- Then we define the eviction target for each cache tier, as shown in the second expression on screen
:::

## Data Plane Caching {.smaller}

Define the [cache benefit]{.body-highlight} metric as follows:
```{=tex}
\begin{equation}
    CB^k_{n_j}(t) = \begin{cases}
        \begin{array}{l}
             r_{n_j}(CS^k_n(t) - CS^{k^{min}_{n_j}(t)}_n(t)) - \omega(c^a_{n_j} + c^e_{n_j}), \text{if} \, j \, \text{is full} \\
             r_{n_j} CS^k_n(t) - \omega c^a_{n_j}, \; \text{otherwise}
        \end{array}
    \end{cases} 
\end{equation}
```

When data object $k \not \in \mc{K}_{n_j}(t)$ arrives at $n$ during slot $t$, [data plane caching policy]{.body-highlight} at $n$ behaves as follows:

-  Determine cache tier which offers highest cache benefit, i.e. $j^* = \argmax_{\{ \jin \}} CB^k_{n_j}(t)$
-  If $CB^k_{n_{j^*}}(t) > 0$, admit object into tier $j^*$
   -  If $j^*$ is full, i.e. $|\mc{K}_{n_{j^*}}(t)| = L_{n_{j^*}}$, $k$ replaces $k^{min}_{n_{j^*}}$
-  If a replacement happened in $j^* < |\mc{J}_n|$, set $k = k^{min}_{n_{j^*}}$, then start the process over

::: {.footer}
Data Plane Strategy
:::

::: {.notes}
- Of course, because we also consider costs, we need a metric that incorporates those, in other words, balances cache scores against weighted penalties, whether it be for admission or replacement; we call this the cache benefit metric
- Now that the defined this, here's the operation of the data plane caching policy
:::

## Data Plane Forwarding

Forwarding strategy is as in (Yeh, 2014). When a request for object $k \not \in \mc{K}_{n_j}(t), \; \forall \jin$, arrives at $n$ during $t$, the request is forwarded to the following node:

```{=tex}
\begin{equation}
    b^k_n(t) = \argmax_{\{ b:(n,b) \in \mc{L}^k \}} \frac{1}{T} \sum\limits^t_{t'=t-T+1} v^k_{nb}(t)
\end{equation}
```

::: {.footer}
Data Plane Strategy
:::

::: {.notes}
- As for the forwarding, again we base it on the findings of the VIP paper, and use a similar sliding window average, this time of outgoing VIP flow, to make forwarding decisions
:::

## Chunk-level Decisions

At chunk-level in data plane, we observe the following principles:

-   If a data object is admitted to (evicted from) a cache tier, all its chunks must be admitted to (evicted from) that tier.
-   Forwarding decision is made upon receiving request for a first chunk. Requests for subsequent chunks are forwarded to the same node.

::: {.footer}
Data Plane Strategy
:::

::: {.notes}
- As a final note, recall that our model and approach is at the object level, but in the real world, the data plane operates at a chunk (or packet) level
- So here are the two principles we would observe when applying our approach at the chunk level
- (Talk about bullet points)
:::

# Experiments

::: {.notes}
- Alright, so how does our strategy fare in experimental evaluations
:::

## Simulation Setup

-  Object-level simulation framework built with Python using the SimPy library
-  Modular, object-oriented design to allow future extension
-  Multi-threading allowing thread-per-experiment execution
-  Experiment parameters and output in JSON format for easy parsing
-  Thorough statistic logging

::: {.footer}
Experiments
:::

::: {.notes}
- Firstly, when I say experiments here I strictly mean simulation experiments
- And I wanted to throw in this slide here at the beginning because it took quite a bit of time and care to build the simulator that I used to run experiments for this work
- (Talk about bullet points)
- And I needed to build this because existing simulators didn't cover what I needed, and were missing some features I wanted and modifying them would've taken even more time
- But of course this is also far from perfect, but we'll get to that at the very end
:::

## Experiment Setting {.smaller}

::: columns
::: {.column width="50%"}

![Abilene (11 nodes)](images/abilene.svg)

![GEANT (34 nodes)](images/geant.svg)

:::
::: {.column width="50%"}

![4x4 Grid (16 nodes)](images/grid.svg)

![3-Regular (50 nodes)](images/3reg.svg)

:::
:::

::: {.footer}
Experiments
:::

::: {.notes}
- Alright, so when it comes to experiment settings, I first have to stress that the experimentation space for this work is quite overwhelming and not straightforward
- So the results I'll show here are those that I think most succinctly show that our approach can deliver on our goals
- To start off, these are the four topologies I conducted experiments over
- On the left are Abilene and Geant which are real network topologies, and on the right are two stylized graphs
:::

## Experiment Setting {.smaller}

-  Content source for each $k$ is selected uniformly at random; content sources have infinite read rate
-  Routing: Allow $n$ to forward requests for $k$ to any neighbor on *any* shortest path between $n$ and $\mc{S}(k)$ (in number of hops)
-  All nodes have the same two cache tiers available
-  Adapted replacement baselines: LRU, LFU, FIFO, RANDOM
   -  LRU, FIFO and RANDOM cost-unaware, paired with LCE admission
   -  LFU has both aware and unaware adaptations, admission based on frequency
-  All baseline caching policies paired with Least Response Time forwarding
-  Nodes generate requests for some number of simulation seconds, simulation run terminates when all requests are resolved
-  Virtual plane algorithm with slot length of 1 sec and sliding window of size $T=100$

::: {.footer}
Experiments
:::

::: {.notes}
- Of course there are some additional assumptions that pertain to the experiment setting so let's go over those
- Firstly, we pick the source for each object uniformly at random and we assume sources have infinite read rate; *this is not real world accurate since sources would also be holding objects in disks, but it helps to do this in simulations so we can focus on the caching side of things*
- We assume routing is pre-determined, since it's not in our scope here, and the given policy is simple: a node can forward requests for an object on any shortest path (in hops) between itself and the source node for that object.
- *This may sound a bit limiting, but if we actually look at the topologies we're dealing with, you can see that for a lot of source-destination pairs there is going to be many same-length shortest paths*
- These experiments all have two tiers of cache at every node, *and this is again due to how large the parameter space is and comes from a point of trying to simplify that as much as possible*
- Now, because there aren't any policies, novel or established, we could directly compare against, we had to build our own baselines by adapting single-cache policies
- We adapted LRU, FIFO and RANDOM replacement but for these there isn't a good way to incorporate costs, so those are cost-unaware and for admission they're paired with the basic Leave Copy Everywhere policy
- We adapted LFU similarly, but also made a cost-aware LFU adaptation which uses a cache benefit metric similar to the one we devised for our aproach
- Least response time forwarding simply means that each node monitors, for each outgoing link, the delay of the last request sent over that link and satisfied, and chooses the link with the smallest delay to forward requests
:::

## Experiment Setting {.smaller}

| Parameter | Description | Value |
|:-:|:-:|:-:|
|$K$|Number of objects in catalog $\mc{K}$|1000|
|$\sum_{\kin} \lambda^k_n$|Total request arrival rate at each $n$ in objects/sec|10|
|$\alpha$|Zipf's law parameter for object popularity distribution|0.75|
|$C_{ab}$|Capacity of each link $\abin$, in objects|10|
|$|\mc{J}_n|$|Number of cache tiers at each $n$|2|
|$L_{n_1}$|Capacity of tier 1 at each $n$, in objects|5|
|$r_{n_1}$|Read rate of tier 1 at each $n$, in objects/sec|20|
|$c^a_{n_1}$, $c^e_{n_1}$| Admission and eviction costs of tier 1 at each $n$|4, 2|
|$r_{n_2}$|Read rate of tier 2 at each $n$, in objects/sec|10|
|$c^a_{n_2}$, $c^e_{n_2}$| Admission and eviction costs of tier 2 at each $n$|2, 1|
: Parameters for all topologies {tbl-colwidths="[10,80,10]"}

::: {.footer}
Experiments
:::

::: {.notes}
- As I said, there are a great number of parameters for these experiments and I'm not going to cover each one in the interest of time, but we have this table for reference if needed
:::

## Results I {.smaller}
**Scenario**: No penalties ($\omega=0$), $L_{n_1} = 5$ and $L_{n_2} = 100$ at each $n$.

![&emsp; Total delay, as fraction of total delay without any caching](images/tops_delay_v3.svg){width=75% height=75% fig-align="center"}

![Cache hits, percentage of hits in the first tier on the left, total number of hits on the right](images/tops_hits_v3.svg){width=75% height=75% fig-align="center"}

::: {.footer}
Experiments
:::

::: {.notes}
- Now I want to move on to our findings
- First, I'll show results from a scenario where we actually ignore penalties, which we can easily do for our approach by setting omega to be zero
- We're comparing our strategy to cost-unaware baselines here
- The figure up top shows the total delay in the network, normalized to the total delay achieved without any caching
- This figure illustrates our point about how managing larger caches is difficult, because as you can see, the "naive" policies are struggling quite a bit, at times showing worse performance than would be achieved with no caches at all
- Our VIP-based approach performs the best across all experiments, though LFU is not that far behind
- The bottom figure reveals more insights as to why the results are as they are because it shows the distribution of cache tiers among the two tiers as well as total cache hits
- Now note that here the first tier can cache 5 objects, while the second tier can cache 100, so the capacity difference is 20-fold
- However, with our approach, anywhere between 15% to 20% of cache hits are occurring on the first tier, much larger than other baselines, showing that we're able to balance the amount of cache hits better
- You'll notice in most cases our approach doesn't even have the highest total number of cache hits, and that's because it is set up to handle the rate of cache hits properly
:::

## Results II {.smaller}
**Scenario**: Cost-unaware policies excluded, $\omega > 0$. $L_{n_1}=5$, best value of $L_{n_2}$ picked.

::: {.body-centered}
Delay vs. penalty ($\omega$ decreases to the right)
:::

::: columns
::: {.column width="50%"}
![&emsp;&emsp; Abilene](images/pen_vs_delay_abilene_v2.svg)
:::
::: {.column width="50%"}
![&emsp;&emsp;&emsp;&emsp; 4x4 Grid](images/pen_vs_delay_grid_v2.svg)
:::
:::

::: {.footer}
Experiments
:::

::: {.notes}
- Now we move on to a scenario with penalties, dropping the "naive" policies and keeping only the cost-aware adaptation of LFU as a baseline
- We'll be looking at how each policy balances the delay vs. penalty trade-off; total delay on the y-axis, total penalty on the x-axis
- Now I want to point out here that different second tier capacities are used for the two policies. This is because I experimented with a range of values for each and picked the best performing round value
- The reason for this connects to the challenges in managing these larger, slower caches.
- I talked about this in the very beginning, and we've also seen how it manifests in the earlier results.
- This in itself is a long discussion, but the gist is that I wanted to represent the best of the competing policy here.
- As we can see, in the Abilene and Grid topologies our approach is generally outperforming the adapted LFU
:::

## Results II {.smaller}
**Scenario**: Cost-unaware policies excluded, $\omega > 0$. Best value of $L_{n_2}$ picked.

::: {.body-centered}
Delay vs. penalty ($\omega$ decreases to the right)
:::

::: columns
::: {.column width="50%"}
![&emsp;&emsp;&emsp; GEANT](images/pen_vs_delay_geant_v2.svg)
:::
::: {.column width="50%"}
![&emsp;&emsp;&emsp; 3-Regular](images/pen_vs_delay_regular_v2.svg)
:::
:::

::: {.footer}
Experiments
:::

::: {.notes}
- The situation is similar for the Geant and 3-regular topologies as well, though in the latter, there is a small operating region where LFU overtakes our approach
:::

# Proposed Work

## Improvements

::: {.fragment}
There is still room for improvement
:::

::: {.fragment}
![](images/big-room.webp){width=65% height=65% fig-align="center"}

... and it's a big room
:::

::: {.footer}
Super Funny Joke
:::

::: {.notes}
- So as much as I'd like to say I'll be pursuing a shiny new goal for the remainder of my time here, there are still aspects of this work that needs to be covered
- So this section will be centered about those improvements I'm planning on pursuing
:::

## Cache Read Bandwidth {.smaller}
Current model assumes read rate of each object is independent of all others. 
```{=tex}
\begin{equation}
    V^k_n(t+1) \leq \Bigg(\Big(V^k_n(t) - \sum\limits_{b \in \mathcal{N}}\mu^k_{nb}(t)\Big)^+ + A^k_n(t)
    + \sum\limits_{a \in \mathcal{N}}\mu^k_{an}(t) - \sum\limits_{j \in \mathcal{J}_n} \color{#C8102E}{r_{n_j}} s^k_{n_j}(t) \Bigg)^+
\end{equation}
```

::: {.footer}
Proposed Work
:::

::: {.notes}
- First and foremost, while the modeling of read rate is essential in this work, the way we do it is not very accurate
- We treat the read rate parameter as if it is independent for each object, which indicates that there is a guaranteeable rate that can be provided per object, no matter how many objects are cached in that tier
- This is an okay assumption, but given the strict rate limitations we're working with, is not great
:::

## Cache Read Bandwidth {.smaller}
Current model assumes read rate of each object is independent of all others.
```{=tex}
\begin{equation}
    \color{#C0C0C0}{V^k_n(t+1) \leq \Bigg(\Big(V^k_n(t) - \sum\limits_{b \in \mathcal{N}}\mu^k_{nb}(t)\Big)^+ + A^k_n(t)
    + \sum\limits_{a \in \mathcal{N}}\mu^k_{an}(t) - \sum\limits_{j \in \mathcal{J}_n}} \color{#C8102E}{r_{n_j}} \color{#C0C0C0}{s^k_{n_j}(t) \Bigg)^+}    
\end{equation}
```
We need to make this more accurate when considering slower caches.
```{=tex}
\begin{equation}
   \color{#C0C0C0}{V^k_n(t+1) \leq \Bigg(\Big(V^k_n(t) - \sum\limits_{b \in \mathcal{N}}\mu^k_{nb}(t)\Big)^+ + A^k_n(t)
   + \sum\limits_{a \in \mathcal{N}}\mu^k_{an}(t) - \sum\limits_{j \in \mathcal{J}_n}} \color{#C8102E}{r^k_{n_j}(t)} \color{#C0C0C0}{s^k_{n_j}(t) \Bigg)^+}
\end{equation}
```
```{=tex}
\begin{equation}
   \sum_{\kin} r^k_{n_j}(t) \leq r_{n_j}, \; \forall t   
\end{equation}
```

::: {.footer}
Proposed Work
:::

::: {.notes}
- To be perfectly honest, this is an oversight on my end. This is something I've known about VIP for a bit, first pointed out to me by my labmate Yuanhao in fact, during one of our conversations about it
- For the original work, it may not matter as much, because there the target was still a small but fast cache, for which this assumption works better
- But in the beginning of this work I was trying to move a bit fast because I wanted to see how the strategy would perform, and at the time I had lofty goals about a real implementation as well
- But after many experiments, I am now convinced this is something we need to address
- And the way we'll do that is simply by enforcing a cache read bandwidth constraint
:::

## Cache Read Bandwidth {.smaller}
Additional control action each time slot, in product form with another control action
```{=tex}
\begin{equation}
\text{maximize} \quad \sum\limits_{\kin} \sum\limits_{\jin} V^k_{n}(t) r^k_{n_j}(t) s^k_{n_j}(t) - p^k_{n_j}(t)
\end{equation}
```
Read rate no longer a constant, VIPs drained via caching represented with a flow variable, like VIP flow terms over links
```{=tex}
\begin{equation}
   F^k_{n_j}(t) \triangleq r^k_{n_j}(t) s^k_{n_j}(t), \qquad f^k_{n_j} = \frac{1}{t} \sum^{t}_{\tau=1} r^k_{n_j}(\tau) s^k_{n_j}(\tau)
\end{equation}
```
**Expectation**: Stability region and its analysis changes significantly; complexity of caching problem solution reduces

::: {.footer}
Proposed Work
:::

::: {.notes}
- Now, how does that change things?
:::

## Different Object Sizes {.smaller}

Equal object sizes assumption makes analysis straightforward, but is not realistic.

Let $z^k$ be the size of object $k$ in bits. Redefine $L_{n_j}$ be the cache capacity of tier $j$ at node $n$ in bits, instead of objects. The caching problem changes as follows:
```{=tex}
\begin{align}
    \text{maximize}
        & \quad \sum\limits_{\kin} \sum\limits_{\jin} r_{n_j} V^k_{n}(t) s^k_{n_j}(t) - \omega p^k_{n_j}(t) \\
    \text{subject to}
        & \quad \sum\limits_{\kin} z^k s^k_{n_j}(t) \leq L_{n_j}, \; \jin \\
        & \quad \sum\limits_{\jin} s^k_{n_j}(t) \leq 1, \; \kin \\
        & \quad s^k_{n_j}(t) \in \{0, 1\}, \; \kin, \; \jin
\end{align}
```
Problem no longer a LAP but a GAP instead (NP-hard).

::: {.footer}
Proposed Work
:::

::: {.notes}
- Now, here's another shortcoming of the model, which I alluded to earlier in the presentation
- (Go over slide content)
:::

## Different Object Sizes {.smaller}

-  Combined with cache bandwidth constraints, solution may still be low-complexity
-  We have to reevaluate how we treat VIP counts:
   -  Copies of a larger object are produced at a slower rate
   -  VIPs for smaller objects can be drained at a higher rate
   -  Without adjustments, service may prioritize smaller objects
-  Combined with multi-tiered caches, this raises the [integrity]{.body-highlight} question, i.e. should we require that nodes cache all chunks of an object in the same tier?
   -  If not, complexity of the caching problem increases dramatically
   -  Chunk-level strategy may serve this scenario better
   -  Prefetching could be utilized[^x]

[^x]: Prefetching is actually one of the techniques G. Rossini, D. Rossi, et al. used to make SSDs viable as part of "content stores"  

::: {.footer}
Proposed Work
:::

## Virtual Plane - Data Plane Gap {.smaller}

::: {.fragment}
![... oops, didn't see you there](images/elephant.webp){width=65% height=65% fig-align="center"}

<audio data-autoplay id="elephant" src="audio/elephant.mp3"></audio>

<script>
  var audio = document.getElementById("elephant");
  audio.volume = 0.1;
</script>
:::

::: {.footer}
Super Funny Callback
:::

::: {.notes}
- Ah, I was hoping we wouldn't run into him
:::

## Virtual Plane - Data Plane Gap {.smaller}

-   Some disparities between the two planes lead to a gap building over time
    -   Virtual plane assumes nodes can immediately cache any object in the network
    -   Data plane requires nodes to respond to requests from cache if available
        - This makes [upward migrations]{.body-highlight}[^*] difficult in data plane
    -   Length of time slots, interest aggregation etc. further extend this gap
-   Amending the gap is difficult, but it must be quantified:
    -   Measure delay between a caching decision in the virtual plane, and when it is realized in data plane
    -   Estimate difference between performance and cost metrics under virtual plane assumptions vs. data plane results

[^*]: When a large amount of unsatisfied demand builds up for an object cached in a slower tier, causing that object to be moved into a faster tier

::: {.footer}
Proposed Work
:::

::: {.notes}

:::

## Cache Exclusivity {.smaller}
```{=tex}
\begin{equation}
\sum_{\jin} s^k_{n_j}(t) \leq 1, \quad \kin
\end{equation}
```
Useful in view of [interest aggregation]{.body-highlight}[^+], but very restrictive:

-  Without interest aggregation, duplicates across tiers could extend bandwidth
-  Removing cache exclusivity could also help with upward migration problem
-  Without this constraint, algorithm operates over individual tiers independently; reduced complexity, adaptations to baselines unnecessary

[^+]: When a node receives an interest for an object while it already is waiting on the data packet from an earlier interest for the same object, it won't send out the new interest. The data that returns for the earlier interest will be used to satisfy the new interest.

::: {.footer}
Proposed Work
:::

## Better Experiments

-  Python is convenient but not highly-scalable for DES frameworks. Julia is a great alternative for both convenience and performance.
-  Per-experiment multi-threading is not effective. Simulator core should be multi-threaded to improve [scale-up]{.body-highlight}; per-experiment [scale-out]{.body-highlight} can also be used.
-  Current codebase has some comments but little documentation and examples. Easily [improvable]{.body-highlight} simulator and [reproducible]{.body-highlight} results crucial.

::: {.footer}
Proposed Work
:::

# Conclusion & Other Contributions

## Conclusion

::: {.incremental}
-   We address a critical and practical problem in scaling high-throughput caching networks
-   Our proposed model and approach achieves our goals and performs decently, but has shortcomings
-   Some improvements can be made by reiterating on technical details, further fine tuning requires experimentation
:::

::: {.footer}
Conclusion & Other Work
:::

## Power & Caching in Wireless HetNets {.smaller}

::: columns
::: {.column width=50%}
**Model**: Arbitrary multi-hop wireless heterogenous network (HetNet) topology:

-   MCs, SCs, and users; MCs and SCs have wireline backhaul connections
-   All wireless transmissions share channel, i.e. no interference management
-   All nodes can be equipped with caches
-   Pre-determined shortest path (in hops) routing

**Goal**: Minimize delay in network by controlling power and caching allocation
:::
::: {.column width=50%}
![HetNet illustration](images/hetnet.svg)
:::
:::

::: {.footer}
Conclusion & Other Work
:::

## Power & Caching in Wireless HetNets {.smaller}

**Optimization**: Minimize total delay of user requests:

-   Wireless link transmission delay dependent on SINR
    ```{=tex}
    \begin{equation}
    \text{SINR}_{vu}(S)=\frac{ G_{vu}s_{vu}}{N_u+  \sum\limits_{j\in V\backslash v}G_{ju}\sum\limits_{w}s_{jw}+G_{vu}\sum\limits_{w\neq u}s_{vw}}
    \end{equation}
    ```
    ```{=tex}
    \begin{equation}
    f(\text{SINR}_{vu}(S)) = \frac{1}{\log_2(1+\text{SINR}_{vu}(S))}
    \end{equation}
    ```

::: {.footer}
Conclusion & Other Work
:::

## Power & Caching in Wireless HetNets {.smaller}

**Optimization**: Minimize total delay of user requests:

-   Wireless link transmission delay dependent on SINR
-   Problem is NP-hard due to integer constraints on caching variables
    ```{=tex}
    \begin{equation}
    D_{(i,p)}^o(X,S)=\sum\limits_{k=1}^{|p|-1}f(\text{SINR}_{p_{k+1}p_k}(S))\prod\limits_{l=1}^k (1-x_{p_l i})
    \end{equation}
    ```
    ```{=tex}
    \begin{equation}
    D^o(X,S)=\sum\limits_{(i,p)\in\mathcal{R}}{\lambda_{(i,p)}{D_{(i,p)}^o(X,S)}}
    \end{equation}
    ```

::: {.footer}
Conclusion & Other Work
:::

## Power & Caching in Wireless HetNets {.smaller}

**Optimization**: Minimize total delay of user requests:

-   Wireless link transmission delay dependent on SINR
-   Problem is NP-hard due to integer constraints on caching variables
-   Convex relaxation on these leads to reduced-complexity formulation (RCF)
    ```{=tex}
    \begin{equation}
    D_{(i,p)}(Y,S)={\sum\limits_{k=1}^{|p|-1}f(\text{SINR}_{p_{k+1}p_k}(S)) g_{p_k i}(Y) }
    \end{equation}
    ```
    ```{=tex}
    \begin{equation}
    g_{p_k i}(Y)=1-\min\Big\{1,\sum\limits_{l=1}^k y_{p_l i}\Big\},\,\quad\forall\, y_{p_li}\in [0, 1]
    \end{equation}
    ```
    ```{=tex}
    \begin{equation}
    D(Y,S)=\sum\limits_{(i,p)\in\mathcal{R}}{\lambda_{(i,p)} D_{(i,p)}(Y,S)}
    \end{equation}
    ```

::: {.footer}
Conclusion & Other Work
:::

## Power & Caching in Wireless HetNets {.smaller}

**Optimization**: Minimize total delay of user requests:

-   Wireless link transmission delay dependent on SINR
-   Problem is NP-hard due to integer constraints on caching variables
-   Convex relaxation on these leads to reduced-complexity formulation (RCF)
    ```{=tex}
    \begin{equation}
    D(Y,S)=\sum\limits_{(i,p)\in\mathcal{R}}{\lambda_{(i,p)} D_{(i,p)}(Y,S)}
    \end{equation}
    ```
-   RCF is not jointly convex in power and caching

::: {.footer}
Conclusion & Other Work
:::

## Power & Caching in Wireless HetNets {.smaller}
::: columns
::: {.column width=50%}
Use projected subgradient method to solve for local minima in general case
```{=tex}
\begin{equation}
\begin{aligned}
    & S^{t+1} = S^t + \xi_S^t(\bar{S}^t - S^t) \\
    & \bar{S}^t = [S^t - w_S^t d_S^t]^+_{\mathcal{D}_S} \\
    & \boldsymbol{y}^{t+1} = \boldsymbol{y}^t + \xi^t_{\boldsymbol{y}}(\boldsymbol{\bar{y}}^t - \boldsymbol{y}^t) \\
    & \boldsymbol{\bar{y}}^t = [\boldsymbol{y}^t - w_Y^t d^t_{\boldsymbol{y}}]^+_{\mathcal{D}_{\boldsymbol{y}}} \\
    & d_S^t = \nabla_S D(Y^t,S^t), \; d^t_{\boldsymbol{y}} \in \partial_{\boldsymbol{y}}D(Y^t,S^t) \\
    & \xi^t_{\boldsymbol{y}} = \frac{D^t - \hat{D}^t}{||d_{\boldsymbol{y}}^t||^2}, \; \xi_S^t = \frac{D^t - \hat{D}^t}{||d_S^t||^2}
\end{aligned}
\end{equation}
```
:::
::: {.column width=50%}
**Projected Subgradient Method**

- Initialize: Choose $S^{0}$, $\boldsymbol{y}^{0}$

- **do**

    - Compute subgradient $d^t_S, d^t_{\boldsymbol{y}}$
    - Determine step sizes $\xi^t_{\boldsymbol{y}}$, $\xi_S^t$
    - Compute projected variables $\boldsymbol{\bar{y}}^t$, $\bar{S}^t$
    - Update $S^{t+1}$ and $\boldsymbol{y}^{t+1}$
    - Let $t=t+1$

- **while** $D^t - D^{t-1} > \epsilon$

- Let $(\boldsymbol{y}^{*}_{sub},S^{*}_{sub}) = (\boldsymbol{y}^{t},S^{t})$

- Rounding
:::
:::

::: {.footer}
Conclusion & Other Work
:::

## Power & Caching in Wireless HetNets {.smaller}
Experimental results under different scenarios compare performance against baseline replacement policies paired with cache-unaware power optimization

![&emsp;&emsp;&emsp;&emsp; Power budget](images/hetnet_result1.svg){width="55%" height="55%"}

::: {.footer}
Conclusion & Other Work
:::

## Power & Caching in Wireless HetNets {.smaller}
Experimental results under different scenarios compare performance against baseline replacement policies paired with cache-unaware power optimization

![&emsp;&emsp;&emsp;&emsp; SC cache capacity](images/hetnet_result3.svg){width="55%" height="55%"}

::: {.footer}
Conclusion & Other Work
:::

## Power & Caching in Wireless HetNets {.smaller}
Experimental results under different scenarios compare performance against baseline replacement policies paired with cache-unaware power optimization

![&emsp;&emsp;&emsp;&emsp; Zipf parameter](images/hetnet_result2.svg){width="55%" height="55%"}

::: {.footer}
Conclusion & Other Work
:::

# Thanks & Questions

# Appendices {visibility="uncounted"}

## Appendix A: Other References {.smaller visibility="uncounted"}
-  *"Toward terabyte-scale caching with SSD in a named data networking router"*, W. So, T. Chung, H. Yuan, D. Oran, M. Stapp, ANCS 2014
-  *"On Designing Optimal Memory Damage Aware Caching Policies for Content-Centric Networks"*, S. Shukla, A. A. Abouzeid, WiOpt 2016

::: {.footer}
Appendices
:::

## Appendix B: Model Notation {.smaller visibility="uncounted"}

| Notation | Definition |
|:--:|:--------|
|$\mathcal{G}$|Directed graph representing the network topology|
|$(\mathcal{N},\mathcal{L})$|Set of nodes and links in $\mathcal{G}$|
|$C_{ab}$|Transmission capacity (in objects/sec) of link $(a,b)$|
|$\mathcal{K}$|Set of data objects in the network|
|$\mc{S}(k)$|Content source node for $k \in \mathcal{K}$|
|$\mathcal{J}_n$ | Set of cache tiers at node $n \in \mathcal{N}$|
|$L_{n_j}$ | Size (in objects) of cache tier $j \in \mathcal{J}_n$ at $n$|
|$r_{n_j}$ | Read rate of tier $j$ at node $n$|
|$c^a_{n_j}$| Admission cost of tier $j$ at node $n$|
|$c^e_{n_j}$| Eviction cost of tier $j$ at node $n$|
: Table of Notations {tbl-colwidths="[20,80]"}

::: {.footer}
Appendices
:::

## Appendix B: Model Notation {.smaller visibility="uncounted"}

| Notation | Definition |
|:--:|:--------|
|$\lambda^k_n$|Exogenous request arrival rate for $k$ at $n$|
|$t$|Time slot referring to time interval $[t, t+1)$|
|$A^k_n(t)$|Number of exogenous requests for $k$ at $n$ during $t$|
|$s^k_{n_j}(t)$ | Caching state of $k$ in tier $j$ at $n$ at the beginning of $t$|
|$p^k_{n_j}(t)$ | Penalty incurred by the choice of $s^k_{n_j}(t)$|
|$p(t)$|Sum penalty incurred during $t$|
|$\omega$|Penalty importance weight|
|$V^k_n(t)$|VIP count for $k$ at $n$ during $t$|
|$\mathbf{V}(t)$|Vector of VIP queue states during $t$|
|$\mu^k_{ab}(t)$|Allocated rate of VIPs for $k$ over $(a,b)$ during $t$|
: Table of Notations {tbl-colwidths="[20,80]"}

::: {.footer}
Appendices
:::

## Appendix C: Analysis Notation {.smaller visibility="uncounted"}

| Notation | Definition |
|:--:|:--------|
|$A^k_{n,max}$| Finite value such that $A^k_n(t) \leq A^k_{n,max}$ for all $t$|
|$A_{n,max}$| Maximum total exogenous arrivals at node $n$ during $t$ across all $\kin$, i.e. $A_{n,max} \triangleq \sum_{\kin}A^k_{n,max}$|
|$\mu^{out}_{n,max}$| Maximum total allocated transmission rate for VIPs across all $(n,b) \in \mc{L}$, i.e. $\mu^{out}_{n,max} \triangleq \sum_{\bin}C_{nb}$|
|$\mu^{in}_{n,max}$| Maximum total allocated transmission rate for VIPs across all $(a,n) \in \mc{L}$, i.e. $\mu^{in}_{n,max} \triangleq \sum_{\bin}C_{an}$|
|$r_{n,max}$| Maximum total rate that can be served by all cache tiers at $n$|
|$\Psi(\boldsymbol{\lambda})$|Minimum time-average sum penalty achievable by a feasible and stabilizing randomized policy|

::: {.footer}
Appendices
:::

## Appendix D: Simulator Snippets {.smaller visibility="uncounted"}

::: {.panel-tabset}

### Node

```{.python}
class Node(object):
   # ...
   def packetReceiver(self, remote_id):
      while True:
         pkt = yield self.in_links[remote_id].get()
         if pkt.isData():
               yield self.env.timeout(1/self.in_links[remote_id].link_cap)
         if pkt.isData() or pkt.isInterest():
               self.pkt_buffer.put((remote_id, pkt))
   def packetProcessor(self):
      while True:
         remote_id, pkt = yield self.pkt_buffer.get()
         if pkt.isInterest():
               self.receiveInterest(remote_id, pkt.request)
         elif pkt.isData():
               self.receiveData(pkt.request)
   # ...
   def forwardInterest(self, request):
        self.sendInterestPacket(self.fib[request.object_id][0], request)
        return

    def decideCaching(self, object_id):
        yield self.env.timeout(0)
        return
   # ...
```

### Cache

```{.python}
class Cache(object):
   def cacheController(self):
      while True:
         task = yield self.task_queue.get()
         if task.type == 'r':
               obj = yield self.env.process(self.readProcess(task.object_id))
               self.out_buffer.put(obj)
         elif task.type == 'w':
               yield self.env.process(self.writeProcess(task.object_id))
   #...
   def readObject(self, object_id):
      self.stats['reads'] += 1            
      task = CacheTask(type = 'r', object_id = object_id)
      tic = self.env.now
      self.task_queue.put(task)
      obj = yield self.out_buffer.get()
      self.stats['read_delay'] += self.env.now - tic
      return obj
   def cacheObject(self, object_id):
      self.stats['writes'] += 1
      self.contents.append(object_id)
      self.cur_size += 1
      task = CacheTask(type = 'w', object_id = object_id)
      self.task_queue.put(task)
      return True
```

### Link

```{.python}
class Link(object):
   def __init__(self, env, link_cap, prop_delay):
      self.env = env
      self.link_cap = link_cap
      self.pipe = sp.Store(env)
   def put(self, pkt):
      self.pipe.put(pkt)
   def get(self):
      return self.pipe.get()

class VipLink(object):
    def __init__(self, env):
        self.env = env
        self.update_pipe = sp.Store(env)
        self.vip_pipe = sp.Store(env)    
    def pushUpdate(self, pkt):
        self.update_pipe.put(pkt)    
    def getUpdate(self):
        return self.update_pipe.get()    
    def pushVips(self, pkt):
        self.vip_pipe.put(pkt)    
    def getVips(self):
        return self.vip_pipe.get()
```

### Caching Policy

```{.python}
class UNIFNode(Node):
   def decideCaching(self, object_id):
      cache = np.random.choice(self.caches)
      if cache.isFull():
         victim_id = np.random.choice(cache.contents)
         yield self.env.process(cache.replaceObject(victim_id, object_id))
      else:
         cache.cacheObject(object_id)
```

### Forwarding Policy

```{.python}
class RoundRobinNode(Node):
   def addFIB(self, fib):
      super().addFIB(fib)
      self.link_queues = {}
      for k in fib:
         self.link_queues[k] = deque(fib[k])
   
   def forwardInterest(self, request):
      object_id = request.object_id
      remote_id = self.link_queues[object_id][0]
      self.link_queues[object_id].rotate(1)
      self.sendInterestPacket(remote_id, request)
```
:::

::: {.footer}
Appendices
:::