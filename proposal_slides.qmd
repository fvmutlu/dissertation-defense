---
title: "Placeholder Title"
subtitle: "Faruk Volkan Mutlu - PhD Proposal Review"
author: "Research Advisor: Edmund Yeh<br>Committee Members: Elif Uysal, Stratis Ioannidis"
format:
  revealjs:
    slide-number: true
    theme: white
    logo: images/neu_logo.svg
    css: styles/logo.css
---

## Outline

The general outline of this presentation is as follows:

1. Context
2. Research Topics
   - Joint Caching and Forwarding in Networks with Hybrid Storage Systems
      - NDN for Data Intensive Science Experiments*
   - Joint Power Control and Caching in Wireless HetNets
3. Proposed Work
4. Conclusion and Acknowledgements

::: {.footer}
Outline
:::

::: {.notes}
- We'll first go over the general context in which my research fits
- We'll then cover two major research directions I've pursued and am pursuing
- The first one makes up the bulk of the content of this presentation since it is work that I am heading
- I will also take an aside while discussing that, to mention a more implementation-oriented project that I am part of, which originated the purpose of my work on the first topic
- The second one is work which I played a major role in building, but was originally proposed by Derya Malak, who was a postdoc in our lab during my first year and a current collaborator 
- The proposed work section will focus on the first of these topics and introduce extensions I plan on pursuing for the remainder of my dissertation work
- Then of course there will be a brief conclusion
:::

::: {.hidden}
$$
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\nin}{n \in \mc{N}}
\newcommand{\kin}{k \in \mc{K}}
\newcommand{\jin}{j \in \mc{J}_n}
\newcommand{\iin}{i \in \mc{I}_n}
\newcommand{\ain}{a \in \mc{N}}
\newcommand{\bin}{b \in \mc{N}}
\newcommand{\abin}{(a,b) \in \mc{L}}
\newcommand{\about}{(a,b) \not\in \mc{L}^k}
\DeclareMathOperator*{\argmax}{arg\,max}
$$
:::

## Outline

For each research topic, we'll cover:

1. Motivation
2. Related Work
3. Contributions
   - Model
   - Approach
   - Results
4. Future Extensions

::: {.footer}
Outline
:::

::: {.notes}
- I will introduce the topic with its motivation, giving you a background on why we even want to investigate the topic and why it is important
- Then we will go over relevant studies that explored similar or adjacent problems and outline what we want address that we believe weren't addressed by these works
- We will then discuss our contributions, breaking it into these three steps
- Finally we'll briefly list some possible future directions for the topic, some of which will reappear in my proposed work section
:::

# Context

## Information Centric Networking

- **Motivation**: The Internet is primarily a data distribution network, yet its principles are those of a communication network
- **Premise**: Make uniquely named data the primary entity of the network, instead of endpoints
- **Advantages**: Improve efficiency and scalability by decoupling data from location and session

::: {.footer}
Context
:::

## In-network Caching

- **Motivation**: Current caching infrastructure is mostly proprietary and centralized
- **Premise**: Enable every router in the network to maintain its own cache
- **Advantages**: Caching becomes decentralized, reducing network congestion and improving scalability

::: {.footer}
Context
:::

## Wireless Edge & HetNets

- Interface between local devices and wider network infrastructure
- Many edge devices are connected wirelessly and through various access technologies (HetNet)
- **Importance**: Edge-computing is gaining prevalance, driving more computation towards the network edge

::: {.footer}
Context
:::

# Research Topic I
<h3>Joint Caching and Forwarding in Networks with Hybrid Storage Systems</h3>

## Toy Example {.smaller}

![](images/toynet.svg){width=60% height=60% fig-align="center"}

::: columns
::: {.column width="50%"}
| Notation | Definition |
|:-:|:---------------|
|$C_{ab}$|Capacity of link $(a,b)$|
|$\mathcal{N}_c$|Set of consumer nodes|
|$\mathcal{K}$|Set of data objects|
|$\alpha$|Zipf's law parameter|
|$L_{a}$|Cache size (in packets) at node $a$|
|$\lambda$|Total incoming request rate at forwarder|
:::
::: {.column width="50%"}
-  Catalog of $|\mathcal{K}|=1000$ single-packet objects, ranked in popularity according to Zipf's law with parameter $\alpha=0.75$
-  Consumer nodes send requests to the forwarder node, which either responds from its cache or forwards them upstream to server
-  Caching at the forwarder is clairvoyant, i.e. most popular $L_{f}$ objects are cached
:::
:::

::: {.footer}
Motivation
:::

## Toy Example {.smaller}

::: columns
::: {.column width="50%"}
![](images/zipf_1k.svg)

::: {.fragment}
-  Most popular 10% of objects make up half of all requests
-  Most popular 1% of objects make up one fifth of all requests
:::
:::
::: {.column width="50%"}
![](images/cache_size.svg){.fragment}

::: {.fragment}
-  Increasing cache sizes can help reduce delay significantly
-  But the gain from size increase has diminishing returns
:::
:::
:::

::: {.footer}
Motivation
:::

## Core Challenge {.incremental}
-  DRAM is the primary cache device since it is fast, i.e. can operate at *line rate*
-  However, DRAM offers small cache sizes at a high and exponentially increasing cost ($3 to $12 / GB)
-  Next best storage element is the NVMe SSD, offering large cache sizes at significantly lower costs ($0.1 / GB)
-  NVMe SSDs can be an order of magnitude slower than DRAM

::: {.footer}
Motivation
:::

## Extended Toy Example {.smaller}

![](images/toynet_cache.svg){width=75% height=75% fig-align="center"}

::: columns
::: {.column width="50%"}
| Notation | Definition |
|:-:|:---------------|
|$r_{n_j}$|Read rate of tier $j$ at $n$|
|$L_{n_j}$|Cache size of tier $j$ at $n$|

Numbered nodes connected by green edges represent potential cache tiers
:::
::: {.column width="50%"}
-  Same catalog and popularity distribution, three possible cache tiers
-  Objects are cached in order of popularity, starting from tier 1 and moving onto next tier when current one is full
:::
:::

::: {.footer}
Motivation
:::

## Key Observations {.incremental .smaller}

::: columns
::: {.column width="50%"}
![](images/two_tiers.svg){.fragment}

::: {.fragment}
-  Although second tier operates slower than line rate, it improves delay at high request rates
-  This is due to the diminishing returns emerging from Zipf's law
:::
:::
::: {.column width="50%"}
![](images/three_tiers.svg){.fragment}

::: {.fragment}
-  Addition of the third tier impacts performance negatively
-  **Lesson**: Cache tiers need to outpace their own *hit rate*
:::
:::
:::

::: {.footer}
Motivation
:::

## Multi-tiered Caching Policy {.incremental}
-  **Goal**: A caching policy suitable for systems with a combination of different storage elements as caches
-  This policy needs to:
   -  Balance frequency of cache hits served by cache tiers based on their transfer rates
   -  Be adaptable to changing traffic patterns in the network
   -  Offer distributed implementation with performance guarantees

::: {.footer}
Motivation
:::

## Related Work {.incremental .smaller}

::: {.footer}
Related Work
:::

## System Model {.smaller .scrollable}

| Notation | Definition |
|:--:|:--------|
|$\mathcal{G}$|Directed graph underlining the network topology|
|$(\mathcal{N},\mathcal{L})$|Set of nodes and links in $\mathcal{G}$|
|$C_{ab}$|Transmission capacity of link $(a,b)$|
|$\mathcal{K}$|Set of data objects in the network|
|$src(k)$|Content source node for $k \in \mathcal{K}$|
|$\mathcal{J}_n$ | Set of cache tiers at node $n \in \mathcal{N}$|
|$L_{n_j}$ | Size of cache tier $j \in \mathcal{J}_n$ at $n$|
|$r_{n_j}$ | Read rate of tier $j$ at node $n$|
|$t$|Time slot referring to interval $[t, t+1)$|
|$V^k_n(t)$|VIP count for $k$ at $n$ during $t$|
|$A^k_n(t)$|Number of exogenous requests for $k$ at $n$ during $t$|
|$\lambda^k_n$|Exogenous VIP arrival rate for $k$ at $n$|
|$\mu^k_{ab}(t)$|Allocated rate of VIPs for $k$ over $(a,b)$ during $t$|
|$s^k_{n_j}(t)$ | Caching state of $k$ in tier $j$ at $n$ at the beginning of $t$|
: Table of Notations {tbl-colwidths="[20,80]"}

::: {.footer}
Contributions
:::

## Queue Dynamics
The VIP queue evolution for object $k$ at node $n$ can be described with the following, where $(x)^+ = max(x,0)$:
```{=tex}
\begin{equation}
\begin{split}
    V^k_n(t+1) \leq & \Bigg(\Big(V^k_n(t) - \sum\limits_{b \in \mathcal{N}}\mu^k_{nb}(t)\Big)^+ + A^k_n(t) \\ 
    & + \sum\limits_{a \in \mathcal{N}}\mu^k_{an}(t) - \sum\limits_{j \in \mathcal{J}_n} r_{n_j} s^k_{n_j}(t) \Bigg)^+    
\end{split}
\end{equation}
```

## Algorithm {.smaller .scrollable}
At the beginning of each time slot $t$, at each node $n$, observe queueues $(V^k_n(t))_{k \in \mathcal{K}, n \in \mathcal{N}}$ then perform caching and forwarding as follows:

-  **Caching**: Choose $s^k_{n_j}(t)$ for each $\kin$ and $j \in \mathcal{J}_n$ to:
```{=tex}
\begin{align}
    \text{maximize}
        & \quad \sum\limits_{\kin} \sum\limits_{\jin} r_{n_j} V^k_n(t) s^k_{n_j}(t) \\
    \text{subject to}
        & \quad \sum\limits_{\kin} s^k_{n_j}(t) \leq L_{n_j}, \; \jin \\
        & \quad s^k_{n_j}(t) \in \{0, 1\}, \; \kin, \; \jin
\end{align}
```
-  **Forwarding**: Let $\mc{L}^k$ be the set of links which are allowed to transmit VIPs of object k, determined by a routing policy. For each $\kin$ and $\abin^k$, choose:
```{=tex}
\begin{equation}
   \mu^k_{ab}(t) =
   \left\{ \begin{array}{ll}
      C_{ba}, & \text{if} \; k = k^*_{ab}(t) \; \text{and} \; W^k_{ab}(t) > 0 \\
      0, & \text{otherwise}
   \end{array} \right.
\end{equation}
```
```{=tex}
\begin{equation}
\begin{split}
   W^k_{ab}(t) & \triangleq V^k_a(t) - V^k_b(t), \\
   k^*_{ab}(t) & \triangleq \argmax\limits_{\{k: \abin^k\}} W^k_{ab}(t)
\end{split}
\end{equation}
```