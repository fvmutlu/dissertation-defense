---
title: "Placeholder Title"
subtitle: "Faruk Volkan Mutlu - PhD Proposal Review"
author: "Research Advisor: Edmund Yeh<br>Committee Members: Elif Uysal, Stratis Ioannidis"
format:
  revealjs:
    slide-number: true
    theme: white
    logo: images/neu_logo.svg
    css: styles/logo.css
    highlight-style: atom-one
---

## Outline {.smaller}

The outline of this presentation is as follows:

1. General Context
2. Joint Caching and Forwarding in Networks with Hybrid Storage Systems
   - Motivation
   - Related Work
   - Contributions
   - Proposed Work
3. Other Contributions
   - Joint Power Control and Caching in Wireless HetNets
   - NDN for Data Intensive Science Experiments*
4. Conclusion and Acknowledgements

::: {.footer}
Outline
:::

::: {.notes}
- We'll first go over the general context in which my research fits
- We'll then cover two major research directions I've pursued and am pursuing
- The first one makes up the bulk of the content of this presentation since it is work that I am heading
- I will also take an aside while discussing that, to mention a more implementation-oriented project that I am part of, which originated the purpose of my work on the first topic
- The second one is work which I played a major role in building, but was originally proposed by Derya Malak, who was a postdoc in our lab during my first year and a current collaborator 
- The proposed work section will focus on the first of these topics and introduce extensions I plan on pursuing for the remainder of my dissertation work
- Then of course there will be a brief conclusion
:::

::: {.hidden}
$$
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\nin}{n \in \mc{N}}
\newcommand{\kin}{k \in \mc{K}}
\newcommand{\jin}{j \in \mc{J}_n}
\newcommand{\kjin}{(k,j) \in \mc{B}_{n,i}}
\newcommand{\iin}{i \in \mc{I}_n}
\newcommand{\ain}{a \in \mc{N}}
\newcommand{\bin}{b \in \mc{N}}
\newcommand{\abin}{(a,b) \in \mc{L}}
\newcommand{\about}{(a,b) \not\in \mc{L}^k}
\newcommand{\betasum}{\sum\limits ^{\sigma_n}_{i=1} \beta_{n,i}}
\newcommand{\betasumnl}{\sum ^{\sigma_n}_{i=1} \beta_{n,i}}
\newcommand{\betasumind}{\sum\limits ^{\sigma_n}_{i=1} \beta_{n,i}\mathbf{1}_{[\kjin]}}
\newcommand{\minpen}{\Psi(\boldsymbol{\lambda})}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
$$
:::

# Context

## Information Centric Networking

- **Motivation**: The Internet is primarily a data distribution network, yet its principles are those of a communication network
- **Premise**: Make uniquely named data the primary entity of the network, instead of endpoints
- **Advantages**: Improve efficiency and scalability by decoupling data from location and session

::: {.footer}
Context
:::

## In-network Caching

- **Motivation**: Current caching infrastructure is mostly proprietary and centralized
- **Premise**: Enable every router in the network to maintain its own cache
- **Advantages**: Caching becomes decentralized, reducing network congestion and improving scalability

::: {.footer}
Context
:::

## Wireless Edge & HetNets

- Interface between local devices and wider network infrastructure
- Many edge devices are connected wirelessly and through various access technologies (HetNet)
- **Importance**: Edge-computing is gaining prevalance, driving more computation towards the network edge

::: {.footer}
Context
:::

# Research Topic I
<h3>Joint Caching and Forwarding in Networks with Hybrid Storage Systems</h3>

## Overview {.smaller}

::: {.incremental}
-  Data volume is growing exponentially, but capacities of in-network caches are stagnant
-  Enabling larger caches in a cost-effective way is not trivial
-  We consider the following:
   -  How beneficial are larger caches?
   -  Are slower and larger storage elements viable as caches?
   -  What are the costs of operating large caches?
   -  How can we get the most out of caches in the network?
-  We develop a joint caching and forwarding strategy that incorporates these considerations
:::

## Toy Example {.smaller}

![](images/toynet.svg){width=60% height=60% fig-align="center"}

::: columns
::: {.column width="50%"}
| Notation | Definition |
|:-:|:---------------|
|$C_{ab}$|Capacity of link $(a,b)$|
|$\mathcal{N}_c$|Set of consumer nodes|
|$\mathcal{K}$|Set of data objects|
|$\alpha$|Zipf's law parameter|
|$L_{a}$|Cache size (in packets) at node $a$|
|$\lambda$|Total incoming request rate at forwarder|
:::
::: {.column width="50%"}
-  Catalog of $|\mathcal{K}|=1000$ single-packet objects, ranked in popularity according to Zipf's law with parameter $\alpha=0.75$
-  Consumer nodes send requests to the forwarder node, which either responds from its cache or forwards them upstream to server
-  Caching at the forwarder is clairvoyant, i.e. most popular $L_{f}$ objects are cached
:::
:::

::: {.footer}
Motivation
:::

## Case for Larger Caches {.smaller}

::: columns
::: {.column width="50%"}
![](images/zipf_1k.svg)

::: {.fragment}
-  Most popular 10% of objects make up half of all requests
-  Most popular 1% of objects make up one fifth of all requests
:::
:::
::: {.column width="50%"}
![](images/cache_size.svg)

::: {.fragment}
-  Increasing cache sizes can help reduce delay significantly
-  Note that the gain from size increase has diminishing returns
:::
:::
:::

::: {.footer}
Motivation
:::

## Core Challenge I

::: {.incremental}
-  DRAM is the primary cache device since it is fast, i.e. can operate at *line rate*
-  However, DRAM offers small cache sizes at a high and exponentially increasing cost ($3 to $12 / GB)
-  Next best storage element is the NVMe SSD, offering large cache sizes at significantly lower costs ($0.1 / GB)
-  NVMe SSDs can be an order of magnitude slower than DRAM
:::

::: {.footer}
Motivation
:::

## Extended Toy Example {.smaller}

![](images/toynet_cache.svg){width=75% height=75% fig-align="center"}

::: columns
::: {.column width="50%"}
| Notation | Definition |
|:-:|:---------------|
|$r_{n_j}$|Read rate of tier $j$ at $n$|
|$L_{n_j}$|Cache size of tier $j$ at $n$|

Numbered nodes connected by green edges represent potential cache tiers
:::
::: {.column width="50%"}
-  Same catalog and popularity distribution, three possible cache tiers
-  Objects are cached in order of popularity, starting from tier 1 and moving onto next tier when current one is full
:::
:::

::: {.footer}
Motivation
:::

## Case for Additional, Slower Caches {.smaller}

::: columns
::: {.column width="50%"}
![](images/two_tiers.svg)

::: {.fragment}
-  Although second tier operates slower than line rate, it improves delay at high request rates
-  This is due to the diminishing returns emerging from Zipf's law
:::
:::
::: {.column width="50%"}
![](images/three_tiers.svg)

::: {.fragment}
-  Addition of the third tier impacts performance negatively
-  **Lesson**: Cache tiers need to outpace their own *hit rate*
:::
:::
:::

::: {.footer}
Motivation
:::

## Core Challenge II

::: {.incremental}
-  Caches are not free to use, every admission and replacement has a cost
-  With SSDs, these are more pronounced:
   -  They wear out over time
   -  Due to their slower speed, cache state transitions take longer
   -  Their per-replacement power consumption is larger*
:::

## Core Challenge III

::: {.incremental}
-  (Talk about joint caching and forwarding)
:::

## Goal

::: {.incremental}
-  We want a caching policy suitable for systems with a combination of storage elements with different performance and cost paremeters as caches
-  This policy needs to:
   -  Balance frequency of requests served by cache tiers based on their transfer rates
   -  Be adaptable to changing traffic patterns in the network
   -  Offer a distributed implementation
:::

::: {.footer}
Motivation
:::

## Related Work {.smaller}

-  *"Multi-Terabyte and multi-Gbps information centric routers"*, G. Rossini, D. Rossi, M. Garetto, E. Leonardi, INFOCOM 2014
   -  Two-layer cache (SSD masked behind DRAM), utilizes "prefetching", trace-driven and synthetic simulation results
-  *"Hierarchical Content Stores in High-Speed ICN Routers: Emulation and Prototype Implementation"*, R. B. Mansilha, L. Saino, M. Barcellos, M. Gallo, E. Leonardi, D. Perino, D. Rossi, ICN 2015
   -  Follow-up to above work, prototype implementation
-  *"Exploiting parallelism in hierarchical content stores for high-speed ICN routers"*, R. B. Mansilha, M. Barcellos, E. Leonardi, D. Rossi, COMNET 2017
   -  Follow-up to above works, integrates HCS design with NDN Forwarding Daemon (NFD)

## Related Work {.smaller}
-  *"Toward terabyte-scale caching with SSD in a named data networking router"*, W. So, T. Chung, H. Yuan, D. Oran, M. Stapp, ANCS 2014

::: {.footer}
Related Work
:::

## Related Work

-  *"VIP: a framework for joint dynamic forwarding and caching in named data networks"*, Edmund M. Yeh, Tracey Ho, Ying Cui, Michael Burd, Ran Liu, Derek Leong, ACM ICN 2014
   -  (Placeholder)

::: {.footer}
Related Work
:::

## System Model

-  (Give an overview and list assumptions)

## System Model {.smaller .scrollable}

| Notation | Definition |
|:--:|:--------|
|$\mathcal{G}$|Directed graph representing the network topology|
|$(\mathcal{N},\mathcal{L})$|Set of nodes and links in $\mathcal{G}$|
|$C_{ab}$|Transmission capacity (in objects/sec) of link $(a,b)$|
|$\mathcal{K}$|Set of data objects in the network|
|$\mc{S}(k)$|Content source node for $k \in \mathcal{K}$|
|$\mathcal{J}_n$ | Set of cache tiers at node $n \in \mathcal{N}$|
|$L_{n_j}$ | Size (in objects) of cache tier $j \in \mathcal{J}_n$ at $n$|
|$r_{n_j}$ | Read rate of tier $j$ at node $n$|
|$c^a_{n_j}$| Admission cost of tier $j$ at node $n$|
|$c^e_{n_j}$| Eviction cost of tier $j$ at node $n$|
|$t$|Time slot referring to interval $[t, t+1)$|
|$V^k_n(t)$|VIP count for $k$ at $n$ during $t$|
|$A^k_n(t)$|Number of exogenous requests for $k$ at $n$ during $t$|
|$\lambda^k_n$|Exogenous VIP arrival rate for $k$ at $n$|
|$\mu^k_{ab}(t)$|Allocated rate of VIPs for $k$ over $(a,b)$ during $t$|
|$s^k_{n_j}(t)$ | Caching state of $k$ in tier $j$ at $n$ at the beginning of $t$|
|$p^k_{n_j}(t)$ | Penalty incurred by the choice of $s^k_{n_j}(t)$|
: Table of Notations {tbl-colwidths="[20,80]"}

::: {.footer}
Contributions
:::

## Queue Dynamics
The VIP queue evolution for object $k$ at node $n$ can be described with the following, where $(x)^+ = max(x,0)$:
```{=tex}
\begin{equation}
\begin{split}
    V^k_n(t+1) \leq & \Bigg(\Big(V^k_n(t) - \sum\limits_{b \in \mathcal{N}}\mu^k_{nb}(t)\Big)^+ + A^k_n(t) \\ 
    & + \sum\limits_{a \in \mathcal{N}}\mu^k_{an}(t) - \sum\limits_{j \in \mathcal{J}_n} r_{n_j} s^k_{n_j}(t) \Bigg)^+    
\end{split}
\end{equation}
```

## Virtual Plane Algorithm {.smaller}
At the beginning of each time slot $t$, at each node $n$, observe queueues $(V^k_n(t))_{k \in \mathcal{K}, n \in \mathcal{N}}$ then perform caching and forwarding as follows:

-  **Caching**: Choose $s^k_{n_j}(t)$ for each $\kin$ and $j \in \mathcal{J}_n$ to:
```{=tex}
\begin{align}
    \text{maximize}
        & \quad \sum\limits_{\kin} \sum\limits_{\jin} r_{n_j} V^k_{n}(t) s^k_{n_j}(t) - p^k_{n_j}(t) \\
    \text{subject to}
        & \quad \sum\limits_{\kin} s^k_{n_j}(t) \leq L_{n_j}, \; \jin \\
        & \quad \sum\limits_{\jin} s^k_{n_j}(t) \leq 1, \; \kin \\
        & \quad s^k_{n_j}(t) \in \{0, 1\}, \; \kin, \; \jin
\end{align}
```
```{=tex}
\begin{equation}
    p^k_{n_j}(t) \triangleq 
    \left\{ \begin{array}{ll}
        c^a_{n_j}, & \text{if} \; s^k_{n_j}(t) - s^k_{n_j}(t-1) = 1 \\
        c^e_{n_j}, & \text{if} \; s^k_{n_j}(t) - s^k_{n_j}(t-1) = -1 \\
       0, & \text{otherwise}
   \end{array} \right.
\end{equation}
```

## Virtual Plane Algorithm {.smaller}
At the beginning of each time slot $t$, at each node $n$, observe queueues $(V^k_n(t))_{k \in \mathcal{K}, n \in \mathcal{N}}$ then perform caching and forwarding as follows:

-  **Forwarding**: Let $\mc{L}^k$ be the set of links which are allowed to transmit VIPs of object k, determined by a routing policy. For each $\kin$ and $\abin^k$, choose:
```{=tex}
\begin{align}
    \text{maximize}
        & \quad \sum\limits_{\kin} \sum\limits_{\abin^k} \mu^k_{ab}(t) (V^k_a(t) - V^k_b(t)) \\
    \text{subject to} 
        & \quad \sum\limits_{\kin} \mu^k_{ab}(t) \leq C_{ba}, \; \abin \\
        & \quad \mu^k_{ab}(t) \geq 0, \; \kin, \; \abin
\end{align}
```

## Analysis {.smaller}

| Notation | Definition |
|:--:|:--------|
|$f^k_{ab}$|Time-average VIP flow for $k$ over $(a,b)$|
|$F^k_{ab}(t)$|Number of VIPs of $k$ sent over $(a,b)$ during slot $t$|

```{=tex}
\begin{equation}
f^k_{ab} = \frac{1}{t}\sum^{t}_{\tau=1} F^k_{ab}(\tau)
\end{equation}
```

$F^k_{ab}(t)$ satisfy the following for all $t \geq 1$:
```{=tex}
\begin{equation}
    F^k_{ab}(t) \geq 0, F^k_{nn}(t) = 0, F^k_{\mc{S}(k)n}(t) = 0, \; \forall a,b,n \in \mc{N}, \kin,
\end{equation}
```
```{=tex}
\begin{equation}
    F^k_{ab}(t) = 0, \; \forall a,b \in \mc{N}, \kin, (a,b) \not \in \mc{L}^k
\end{equation}
```
```{=tex}
\begin{equation}
   \sum\limits_{\kin} F^k_{ab}(t) \leq C_{ba}, \; \forall \abin
\end{equation}
```

## Analysis {.smaller}

| Notation | Definition |
|:--:|:--------|
|$\sigma_n$|The total number of possible ways any number of objects $\kin$ can be placed in cache tiers at $n$|
|$\mc{B}_{n,i}$|i-th among all $\sigma_n$ cache placement sets at $n$|
|$\beta_{n,i}$|Fraction of time objects at $n$ are placed according to $\mc{B}_{n,i}$|

The elements of set $\mc{B}_{n,i}$ are pairs in the form of $(k,j)$, such that if $(k,j) \in \mc{B}_{n,i}$, object $k$ is cached in tier $j$. $\beta_{n,i}$ satisfy the following:
```{=tex}
\begin{equation}
    0 \leq \beta_{n,i} \leq 1, \quad i = 1,\cdots,\sigma_n, \quad \nin
\end{equation}
```
```{=tex}
\begin{equation}
    \betasum = 1, \quad \forall \nin
\end{equation}
```

## Analysis

**Stability Region**: The VIP stability region $\Lambda$ of the network $\mc{G} = (\mc{N},\mc{L})$, is the set $\Lambda$ consisting of all VIP arrival rates $\boldsymbol{\lambda} = (\lambda^k_n)_{\kin,\nin}$ such that the following holds:
```{=tex}
\begin{equation}
\begin{split}
    \lambda^k_n \leq \sum\limits_{\bin} f^k_{nb} & - \sum\limits_{\ain} f^k_{an} + \sum\limits_{\jin} r_{n_j} \betasumind, \\
    & \forall \nin, \; \kin, \; n \not = \mc{S}(k)
\end{split}
\end{equation}
```
When all exogenous request arrival rates abide by the bound described above, there is a feasible forwarding and caching policy in the virtual plane that can stabilize all VIP queues.

## Analysis {.smaller}

| Notation | Definition |
|:--:|:--------|
|$\Psi(\boldsymbol{\lambda})$|(description pending)|
|$N$|Number of nodes in the network, i.e. $N = |\mc{N}|$|
|$A^k_{n,max}$| Finite value such that $A^k_n(t) \leq A^k_{n,max}$ for all $t$|
|$A_{n,max}$| Maximum total exogenous arrivals at node $n$ during $t$ across all $\kin$, i.e. $A_{n,max} \triangleq \sum_{\kin}A^k_{n,max}$|
|$\mu^{out}_{n,max}$| Maximum total allocated transmission rate for VIPs across all $(n,b) \in \mc{L}$, i.e. $\mu^{out}_{n,max} \triangleq \sum_{\bin}C_{nb}$|
|$\mu^{in}_{n,max}$| Maximum total allocated transmission rate for VIPs across all $(a,n) \in \mc{L}$, i.e. $\mu^{in}_{n,max} \triangleq \sum_{\bin}C_{an}$|
|$r_{n,max}$| Maximum total rate that can be served by all cache tiers at $n$, i.e. (pending)|

## Analysis {.smaller}

**VIP Backlog/Penalty Bounds**: Given VIP arrival rate vector $\boldsymbol{\lambda} = (\lambda^k_n)_{\kin,\nin}$, if there exists $\boldsymbol{\epsilon} = (\epsilon^k_n)_{\nin, \kin} \succ \mathbf{0}$ such that $\boldsymbol{\lambda} + \boldsymbol{\epsilon} \in \Lambda$, then the network of VIP queues under the proposed algorithm satisfies the following bounds:
```{=tex}
\begin{equation}
    \lim\limits_{T \rightarrow \infty} \frac{1}{T} \sum\limits^{T-1}_{t=0} \sum\limits_{\nin, \kin} \mathbb{E}[V^k_n(t)] \leq \frac{NB}{\epsilon} + \frac{\omega}{2\epsilon} \minpen
\end{equation}
```
```{=tex}
\begin{equation}
    \lim\limits_{T \rightarrow \infty} \frac{1}{T}\sum\limits^{T-1}_{t=0} \mathbb{E}[p(t)] \leq \frac{2NB}{\omega} + \minpen
\end{equation}
```
```{=tex}
\begin{equation}
    B \triangleq \frac{1}{2N} \sum_{\nin} \bigg((\mu^{out}_{n,max})^2 + 2(\mu^{out}_{n,max})r_{n,max} + \big(\textstyle \sum_{\kin}A^k_{n,max} + \mu^{in}_{n,max} + r_{n,max})^2 \bigg)
\end{equation}
```

**Interpretation**: We can pick a large $\omega$ to push the time average penalty close to $\Psi(\boldsymbol{\lambda})$ at the cost of significantly increasing the average queue backlog and vice-versa.

## Solutions to Optimization Problems {.smaller}
**Caching**: We can rewrite the optimization problem for the caching step.
```{=tex}
\begin{align}
    \text{maximize}
        & \quad \sum\limits_{\kin} \sum\limits_{\jin} r_{n_j} V^k_{n}(t) s^k_{n_j}(t) - p^k_{n_j}(t) \\
    \text{subject to}
        & \quad \sum\limits_{\kin} s^k_{n_j}(t) \leq L_{n_j}, \; \jin \\
        & \quad \sum\limits_{\jin} s^k_{n_j}(t) \leq 1, \; \kin \\
        & \quad s^k_{n_j}(t) \in \{0, 1\}, \; \kin, \; \jin
\end{align}
```
```{=tex}
\begin{equation}
    p^k_{n_j}(t) \triangleq 
    \left\{ \begin{array}{ll}
        c^a_{n_j}, & \text{if} \; s^k_{n_j}(t) - s^k_{n_j}(t-1) = 1 \\
        c^e_{n_j}, & \text{if} \; s^k_{n_j}(t) - s^k_{n_j}(t-1) = -1 \\
       0, & \text{otherwise}
   \end{array} \right.
\end{equation}
```

## Solutions to Optimization Problems {.smaller}
**Caching**: Intuitively, we can think of the *net benefit* of admitting an object as its objective value minus the weighted cost of admission. Similarly, we can think of the net benefit of keeping an object in cache as its objective value plus the weighted cost of eviction.
```{=tex}
\begin{align}
    \text{maximize}
        & \quad \sum\limits_{\kin} \sum\limits_{\jin} b^k_{n_j}(t) s^k_{n_j}(t) \\
    \text{subject to}
        & \quad \sum\limits_{\kin} s^k_{n_j}(t) \leq L_{n_j}, \; \jin \\
        & \quad \sum\limits_{\jin} s^k_{n_j}(t) \leq 1, \; \kin \\
        & \quad s^k_{n_j}(t) \in \{0, 1\}, \; \kin, \; \jin
\end{align}
```
```{=tex}
\begin{equation}
    b^k_{n_j}(t) \triangleq \left\{ \begin{array}{ll}
        r_{n_j} V^k_n(t) - \omega c^a_{n_j}, & \text{if} \; s^k_{n_j}(t-1) = 0 \\
        r_{n_j} V^k_n(t) + \omega c^e_{n_j}, & \text{if} \; s^k_{n_j}(t-1) = 1
    \end{array} \right.
\end{equation}
```

## Solutions to Optimization Problems {.smaller}
**Caching**: Using a transformation of variables, the problem can be written as a linear assignment problem. We can interpret this as deciding on caching variables per one-object cache space associated with each cache tier, rather than per-tier[^1].
```{=tex}
\begin{align}
    \text{maximize}
        & \quad \sum\limits_{\kin} \sum\limits_{\iin} b^k_{n_i}(t) s^k_{n_i}(t) \\
    \text{subject to} 
        & \quad \sum\limits_{\kin} s^k_{n_i}(t) \leq 1, \; \iin \\
        & \quad \sum\limits_{\iin} s^k_{n_i}(t) \leq 1, \; \kin \\
        & \quad s^k_{n_i}(t) \in \{0, 1\}, \; \kin, \; \iin
\end{align}
```
$\mc{I}_n = \{1, 2, ..., \sum_{\jin} L_{n_j}\}$. If $i \in \mc{I}_{n_j} = \{1 + \sum^{j - 1}_{\ell = 1} L_{n_\ell}, ..., \sum^{j}_{\ell = 1} L_{n_\ell}\}$, then $b^k_{n_j}(t) = b^k_{n_i}(t)$ and $s^k_{n_j}(t) = s^k_{n_i}(t)$.

[^1]: Only possible due to same size objects assumption

## Solutions to Optimization Problems {.smaller}

![](images/gap.svg)

## Solutions to Optimization Problems {.smaller}

![](images/rlap.svg)

## Solutions to Optimization Problems {.smaller}
**Forwarding**: The solution to the forwarding optimization problem is the classical *backpressure* solution.
```{=tex}
\begin{align}
    \text{maximize}
        & \quad \sum\limits_{\kin} \sum\limits_{\abin^k} \mu^k_{ab}(t) (V^k_a(t) - V^k_b(t)) \\
    \text{subject to} 
        & \quad \sum\limits_{\kin} \mu^k_{ab}(t) \leq C_{ba}, \; \abin \\
        & \quad \mu^k_{ab}(t) \geq 0, \; \kin, \; \abin
\end{align}
```

## Solutions to Optimization Problems {.smaller}
**Forwarding**: The entire reverse link capacity would be allocated to transmitting VIPs for the object with the highest queue backlog differential between the two nodes.
```{=tex}
\begin{equation}
   \mu^k_{ab}(t) =
   \left\{ \begin{array}{ll}
      C_{ba}, & \text{if} \; k = k^*_{ab}(t) \; \text{and} \; W^k_{ab}(t) > 0 \\
      0, & \text{otherwise}
   \end{array} \right.
\end{equation}
```
```{=tex}
\begin{equation}
\begin{split}
   W^k_{ab}(t) & \triangleq V^k_a(t) - V^k_b(t), \\
   k^*_{ab}(t) & \triangleq \argmax\limits_{\{k: \abin^k\}} W^k_{ab}(t)
\end{split}
\end{equation}
```

## Transition to Data Plane

There are several considerations when applying our approach in the data plane:

-  Per-packet forwarding and caching decisions need to be made
-  Assumption that we have immediate access to any object for caching purposes does not hold 
-  Algorithm can lead to oscillatory behavior for caching decisions

## Data Plane Strategy {.smaller}

| Notation | Definition |
|:--:|:--------|
|$\mc{v}^k_{ab}$|Actual number of VIPs for $k$ transmitted over $(a,b)$ during $t$|
|$T$|Size of a sliding window of time slots|
|$\mc{K}_{n_j}$|Set of objects currently cached in tier $j$ at $n$|

Define the *cache score* metric as follows:
```{=tex}
\begin{equation}
    CS^k_n(t) \triangleq \frac{1}{T} \sum^t_{\tau = t - T + 1} \sum_{(a,n) \in \mc{L}^k} v^k_{an}(\tau)
\end{equation}
```

Define the *eviction target* for tier $j$ at node $n$ as follows:
```{=tex}
\begin{equation}
   k'_{n_j} = \argmin_{\{k \in \mc{K}_{n_j}\}} CS^k_n(t)
\end{equation}
```

## Data Plane Strategy {.smaller}

Define the *cache benefit* metric as follows:
```{=tex}
\begin{equation}
    CB^k_{n_j}(t) = \begin{cases}
        \begin{array}{l}
             r_{n_j}(CS^k_n(t) - CS^{k'_{n_j}}_n(t)) - \omega(c^a_{n_j} + c^e_{n_j}), \text{if} \, j \, \text{is full} \\
             r_{n_j} CS^k_n(t) - \omega c^a_{n_j}, \; \text{otherwise}
        \end{array}
    \end{cases} 
\end{equation}
```

**Data Plane Caching**: When a data object $k \not \in \mc{K}_{n_j}$ arrives at $n$ during slot $t$, caching policy at $n$ behaves as follows.

-  Determine the cache tier which offers the highest cache benefit, i.e. $j^* = \argmax_{\{ \jin \}} CB^k_{n_j}(t)$
-  If $CB^k_{n_{j^*}}(t) > 0$, admit object into tier $j^*$
   -  If $j^*$ is full, i.e. $|\mc{K}_{n_{j^*}}| = L_{n_{j^*}}$, $k$ replaces $k'_{n_{j^*}}$
-  If a replacement happened in $j^* < |\mc{J}_n|$, set $k = k'_{n_{j^*}}$, then start the process over

## Data Plane Strategy

**Data Plane Forwarding**: When a request for object $k \not \in \mc{K}_{n_j}, \; \forall \jin$, arrives at $n$, the request is forwarded to the following node.

```{=tex}
\begin{equation}
    b^k_n(t) = \argmax_{\{ b:(n,b) \in \mc{L}^k \}} \frac{1}{T} \sum\limits^t_{t'=t-T+1} v^k_{nb}(t)
\end{equation}
```

## Simulation Setup

-  Object-level simulation environment built with Python using the SimPy library
-  Modular, object-oriented design to allow future extension
-  Multi-threading allowing for thread-per-experiment execution
-  Experiment parameters and output in JSON format for easy parsing
-  Thorough statistic logging

## Simulation Setup {.smaller}

::: {.panel-tabset}

### Node

```{.python}
class Node(object):
   # ...
   def packetReceiver(self, remote_id):
      while True:
         pkt = yield self.in_links[remote_id].get()
         if pkt.isData():
               yield self.env.timeout(1/self.in_links[remote_id].link_cap)
         if pkt.isData() or pkt.isInterest():
               self.pkt_buffer.put((remote_id, pkt))
   def packetProcessor(self):
      while True:
         remote_id, pkt = yield self.pkt_buffer.get()
         if pkt.isInterest():
               self.receiveInterest(remote_id, pkt.request)
         elif pkt.isData():
               self.receiveData(pkt.request)
   # ...
   def forwardInterest(self, request):
        self.sendInterestPacket(self.fib[request.object_id][0], request)
        return

    def decideCaching(self, object_id):
        yield self.env.timeout(0)
        return
   # ...
```

### Cache

```{.python}
class Cache(object):
   def cacheController(self):
      while True:
         task = yield self.task_queue.get()
         if task.type == 'r':
               obj = yield self.env.process(self.readProcess(task.object_id))
               self.out_buffer.put(obj)
         elif task.type == 'w':
               yield self.env.process(self.writeProcess(task.object_id))
   #...
   def readObject(self, object_id):
      self.stats['reads'] += 1            
      task = CacheTask(type = 'r', object_id = object_id)
      tic = self.env.now
      self.task_queue.put(task)
      obj = yield self.out_buffer.get()
      self.stats['read_delay'] += self.env.now - tic
      return obj
   def cacheObject(self, object_id):
      self.stats['writes'] += 1
      self.contents.append(object_id)
      self.cur_size += 1
      task = CacheTask(type = 'w', object_id = object_id)
      self.task_queue.put(task)
      return True
```

### Link

```{.python}
class Link(object):
   def __init__(self, env, link_cap, prop_delay):
      self.env = env
      self.link_cap = link_cap
      self.pipe = sp.Store(env)
   def put(self, pkt):
      self.pipe.put(pkt)
   def get(self):
      return self.pipe.get()

class VipLink(object):
    def __init__(self, env):
        self.env = env
        self.update_pipe = sp.Store(env)
        self.vip_pipe = sp.Store(env)    
    def pushUpdate(self, pkt):
        self.update_pipe.put(pkt)    
    def getUpdate(self):
        return self.update_pipe.get()    
    def pushVips(self, pkt):
        self.vip_pipe.put(pkt)    
    def getVips(self):
        return self.vip_pipe.get()
```

### Caching Policy

```{.python}
class UNIFNode(Node):
   def decideCaching(self, object_id):
      cache = np.random.choice(self.caches)
      if cache.isFull():
         victim_id = np.random.choice(cache.contents)
         yield self.env.process(cache.replaceObject(victim_id, object_id))
      else:
         cache.cacheObject(object_id)
```

### Forwarding Policy

```{.python}
class RoundRobinNode(Node):
   def addFIB(self, fib):
      super().addFIB(fib)
      self.link_queues = {}
      for k in fib:
         self.link_queues[k] = deque(fib[k])
   
   def forwardInterest(self, request):
      object_id = request.object_id
      remote_id = self.link_queues[object_id][0]
      self.link_queues[object_id].rotate(1)
      self.sendInterestPacket(remote_id, request)
```
:::

## Experiment Setting {.smaller}

-  Experiments over four topologies: Abilene, GEANT, 4x4 Grid, 3-regular graph
-  All nodes have caches, can be content sources and can generate requests
-  For each $k$ a node is selected as $\mc{S}(k)$ uniformly at random
-  Content sources have infinite read rate*
-  Routing scheme: Allow $n$ to forward requests for $k$ to any neighbor on *any* shortest path between $n$ and $\mc{S}(k)$ (in number of hops)
-  Multi-tier adapted cost-unaware replacement baselines: LRU, LFU, FIFO, RANDOM
   -  LRU, FIFO and RANDOM paired with LCE admission
-  Multi-tier adapted cost-aware baseline: Penalty Aware LFU
-  All baseline caching policies paired with Least Response Time forwarding
-  Multi-tier VIP ran with slot length of 1 sec and sliding window of size $T=100$


## Experiment Setting {.smaller .scrollable}

| Parameter | Description | Value |
|:-:|:-:|:-:|
|$K$|Number of objects in catalog $\mc{K}$|1000|
|$\sum_{\kin} \lambda^k_n$|Total request arrival rate at each $n$ in objects/sec|10|
|$\alpha$|Zipf's law parameter for object popularity distribution|0.75|
|$C_{ab}$|Capacity of each link $\abin$, in objects|10|
|$L_{n_1}$|Capacity of tier 1 at each $n$, in objects|5|
|$r_{n_1}$|Read rate of tier 1 at each $n$, in objects/sec|20|
|$c^a_{n_1}$, $c^e_{n_1}$| Admission and eviction costs of tier 1 at each $n$|4, 2|
|$r_{n_2}$|Read rate of tier 1 at each $n$, in objects/sec|10|
|$c^a_{n_2}$, $c^e_{n_2}$| Admission and eviction costs of tier 1 at each $n$|2, 1|
: Parameters for all topologies {tbl-colwidths="[10,80,10]"}

## Results I {.smaller}
**Scenario**: No penalties ($\omega=0$) and $L_{n_2} = 100$ at each $n$.

Total delay, as fraction of total delay without any caching

![](images/tops_delay_v3.svg){width=75% height=75% fig-align="center"}

Cache hits, percentage of hits in the first tier on the left, total number of hits on the right

![](images/tops_hits_v3.svg){width=75% height=75% fig-align="center"}

## Results II {.smaller}
**Scenario**: Exclude cost-unaware policies, vary $\omega$. Pick best-performing $L_{n_2}$ value for each of LFU and VIP.

Delay vs. penalty ($\omega$ decreasing to the right)

::: columns
::: {.column width="33%"}
![](images/pen_vs_delay_abilene_v2.svg)
:::
::: {.column width="33%"}
![](images/pen_vs_delay_grid_v2.svg)
:::
::: {.column width="33%"}
![](images/pen_vs_delay_regular_v2.svg)
:::
:::

## Room for Improvement

::: {.fragment}
![It's a really big room](images/big-room.webp){fig-align="left"}
:::

::: {.footer}
Funny Joke
:::

## Room for Improvement {.smaller}

::: {.incremental}
-  Model has some restrictive or unrealistic assumptions and features:
   -  Per-object read rate of caches
   -  Cache exclusivity assumption
   -  Equal object sizes assumption
   -  Constant, caching-only penalties
   -  Disregards interest aggregation
-  Solution to the caching optimization problem has high computational complexity
-  Experimentation space is very large, results so far only scratch the surface
   -  More topologies and adapted baseline policies
   -  Better tuning of parameters, larger parameter space
   -  Different classes of request patterns, trace-driven simulation
   -  Chunk-level simulation
:::

## Proposed Work {.smaller}
*A reasonable combination of the following can be composed into a precise "proposed work" section.*

-  Impose cache bandwidth constraints (per-tier and total)
-  Provide two versions w.r.t cache exclusivity (included and not)
-  Include unequal object sizes
-  Generalize penalties (not just constant, but dynamic and based on both rate allocations and cache variables)
-  Impose penalty constraints
-  Conduct more experiments, both to expand covered scenarios and to test above revisions
-  Revise simulations to improve performance and reproducibility
-  Develop data plane only model that observes interest aggregation
-  Develop MinDelay-based model and policy as both a baseline and a potential alternative for low traffic scenarios

## My Ideal Contribution {.smaller}

A joint forwarding and caching policy that:

-  Is well suited for actual ICN implementations, i.e. modeling effects of core features like interest aggregation
-  Provides a well-defined network capacity region and guarantees network stability in that region
-  Guarantees a configurable trade-off between delay (proportional with queue backlogs) and some penalty function 
-  Is distributed without any loss of said guarantees
-  Does not separate control plane vs. data plane states, i.e. no-gap
-  Is practical to implement, i.e. computational complexity scales reasonably
