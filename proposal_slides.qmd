---
title: "Placeholder Title"
subtitle: "Faruk Volkan Mutlu - PhD Proposal Review"
author: "Research Advisor: Edmund Yeh<br>Committee Members: Elif Uysal, Stratis Ioannidis"
format:
  revealjs:
    slide-number: true
    theme: black
    logo: images/neu_logo.svg
    css: styles/logo.css
---

## Outline

1. Introduction
   i. Background
   ii. Research Problems
   iii. Related Work
2. Work So Far
   i. Multi-tier (*Working title*)
   ii. N-DISE (*Working title*)
   iii. HetNet (*Working title*)
3. Proposed Work

::: {.footer}
Outline
:::

# Introduction
<h3>Background</h3>

## Information Centric Networking

- **Motivation**: Internet today is primarily a data distribution network, yet its principles are those of a communication network
- **Core Premise**: Make uniquely named data the primary entity of the network, instead of endpoints
- **Advantages**: Improve efficiency and scalability by decoupling data from location and session

::: {.footer}
Background
:::

## In-network Caching

- **Motivation**: Current caching infrastructure is mostly proprietary and centralized
- **Core Premise**: Enable every router in the network to maintain its own cache
- **Advantages**: Caching becomes decentralized, reducing network congestion and improving scalability

::: {.footer}
Background
:::

## Wireless Edge & HetNets

- Interface between local devices and wider network infrastructure
- Many edge devices are connected wirelessly and through various access technologies (HetNet)
- **Importance**: Edge-computing is gaining prevalance, driving more computation towards the network edge

::: {.footer}
Background
:::

# Introduction
<h3>Research Problems</h3>

# Introduction
<h3>Related Work</h3>

# Work So Far
<h3>Multi-tier</h3>

## Motivation - Toy Example {.smaller}

![](images/toynet.svg){width=60% height=60% fig-align="center"}

::: columns
::: {.column width="50%"}
| Notation | Definition |
|:-:|:---------------|
|$C_{ab}$|Capacity of link $(a,b)$|
|$\mathcal{N}_c$|Set of consumer nodes|
|$\mathcal{K}$|Set of data objects|
|$L_{n}$|Cache size at $n$|
|$\alpha$|Zipf parameter|
:::
::: {.column width="50%"}
-  Catalog of $|\mathcal{K}|=1000$ objects, ranked according to Zipf's law with parameter $\alpha=0.75$
-  Consumer nodes send requests to the forwarder node, which either responds from its cache or forwards them upstream to server
-  Caching at the forwarder is clairvoyant, i.e. most popular objects are cached
:::
:::

## Motivation - Toy Example {.smaller}

::: columns
::: {.column width="50%"}
![](images/zipf_1k.svg){.fragment}

::: {.fragment}
-  Most popular 10% of objects make up half of all requests
-  Most popular 1% of objects make up one fifth of all requests
:::
:::
::: {.column width="50%"}
![](images/cache_size.svg){.fragment}

::: {.fragment}
-  Increasing cache sizes can help reduce delay significantly
-  But the gain from size increase has diminishing returns
:::
:::
:::

## Core Challenge {.incremental}
-  DRAM is the primary cache device since it is fast, i.e. can operate at *line rate*
-  However, DRAM offers small cache sizes at a high and exponentially increasing cost ($3 to $12 / GB)
-  Next best storage element is the NVMe SSD, offering large cache sizes at significantly lower costs ($0.1 / GB)
-  NVMe SSDs can be an order of magnitude slower than DRAM (but they are improving rapidly)

## Key Observation


## System Model - Table of Notations {.smaller .scrollable}

| Notation | Definition |
|:---:|:-----------|
|$\mathcal{G}$|Directed graph underlining the network topology|
|$(\mathcal{N},\mathcal{L})$|Set of nodes and links in $\mathcal{G}$|
|$C_{ab}$|Transmission capacity of link $(a,b)$|
|$\mathcal{K}$|Set of data objects in the network|
|$src(k)$|Content source node for $k \in \mathcal{K}$|
|$t$|Time slot referring to interval $[t, t+1)$|
|$V^k_n(t)$|VIP count for $k$ at $n$ during $t$|
|$A^k_n(t)$|Number of exogenous requests for $k$ at $n$ during $t$|
|$\lambda^k_n$|Exogenous VIP arrival rate for $k$ at $n$|
|$\mu^k_{ab}(t)$|Allocated rate of VIPs for $k$ over $(a,b)$ during $t$|
|$\mathcal{J}_n$ | Set of cache tiers at node $n \in \mathcal{N}$|
|$L_{n_j}$ | Size of cache tier $j \in \mathcal{J}_n$ at $n$|
|$s^k_{n_j}(t)$ | Caching state of $k$ in tier $j$ at $n$ at the beginning of $t$|
|$\alpha^k_{n_j}(t)$ | Caching action for $k$ in tier $j$ at $n$ during $t$ |
|$r_{n_j}$ | Readout rate of tier $j$ at node $n$|
|$\rho^r_{n_j}$ | Read penalty of tier $j$ at $n$|
|$\rho^w_{n_j}$ | Write penalty of tier $j$ at $n$|
|$p^k_{n_j}(t)$ | Penalty incurred for $k$ in tier $j$ at $n$ during $t$|
|$\omega$ | Penalty importance weight|
