## Setup {.smaller}

-  **Content Sources:** $\mc{S}(k)$ are selected uniformly at random.
-  **Routing:** *Any shortest path* (in number of hops) between $n$ and $\mc{S}(k)$.
-  **Cache Tiers:** Fixed two cache tiers at every node, fast and slow.
-  **Requests:** Requests generated for 100 simulation seconds, run terminates when all requests are resolved.
    - Poisson arrivals with rate $\lambda$ at each node; popularity distribution is Zipf with parameter $\gamma$.
- **Baselines:** LCE admission, LRT forwarding, adapted replacements:
    - LRU, FIFO, UNIF, LFU + single-tier VIP.
    - Penalty Aware LFU (PA-LFU) for trade-off baseline; uses *cache benefit* metric with frequency.

::: {.footer}
Experiments
:::

## Setup {.smaller visibility="uncounted"}

::: columns
::: {.column width="50%"}
![GÃ‰ANT GN4-3N (34 nodes) [^geant]](images/geant.svg){width="100%"}
:::
::: {.column width="50%" style="font-size:75%;"}
| Parameter | Value |
|:-:|:-:|
|Number of objects in catalog $\mc{K}$|2000|
|Bandwidth of each link (objects/sec) |10|
|Read / write rate of tier 1 (objects/sec) |25, 25|
|Read / write rate of tier 2 (in objects/sec) |10, 5|
|Admission / eviction costs of tier 1|4, 2|
|Admission / eviction costs of tier 2|2, 1|
: Common parameters for all scenarios {#tbl-params tbl-colwidths="[80,20]"}

[Cache capacity budget]{.body-highlight} $\Delta L = \Delta L_1 + \Delta L_2$.

$L_{n_1} = 5 + \Delta L_1$, $L_{n_2} = 25 \times \Delta L_2$.

$\Delta L = \Delta L_1$ for single-tier; $\Delta L = \Delta L_2$ for multi-tier in direct comparisons.
:::
:::

[^geant]: The GN4 Phase 3 Network project, https://network.geant.org/gn4-3n/

::: {.footer}
Experiments
:::

::: {.notes}
- And here are some of the common parameters used in the experiments.
- There are some notable things here I'll point out.
- Firstly, the fast cache tier is more than twice as fast as the slow one, which has read bandwidth equivalent to a single link.
- But we also model write rates, and following general characteristics of flash, the slow tier's is equivalent to half its read rate.
- This has implications regarding frequent replacements and is part of the reason we want to penalize them.
- Secondly, to enable comparisons with single-tiered implementations we're introducing a notion of cache capacity budget.
- This relates to purchasing costs for memory and flash, and allows the evaluation of what sort of distribution of investment can yield better results for a given policy.
- We're also seeing the Geant topology which will be the representative one we use for looking at some trends.
:::

## Setup {.smaller visibility="uncounted"}

::: columns
::: {.column width="50%"}
![Abilene (11 nodes)](images/abilene.svg){width="100%" style="display: block; margin-top: 120px;"}
:::
::: {.column width="50%" style="font-size:75%;"}
| Parameter | Value |
|:-:|:-:|
|Number of objects in catalog $\mc{K}$|2000|
|Bandwidth of each link (objects/sec) |10|
|Read / write rate of tier 1 (objects/sec) |25, 25|
|Read / write rate of tier 2 (in objects/sec) |10, 5|
|Admission / eviction costs of tier 1|4, 2|
|Admission / eviction costs of tier 2|2, 1|
: Common parameters for all scenarios {#tbl-params tbl-colwidths="[80,20]"}

[Cache capacity budget]{.body-highlight} $\Delta L = \Delta L_1 + \Delta L_2$.

$L_{n_1} = 5 + \Delta L_1$, $L_{n_2} = 25 \times \Delta L_2$.

$\Delta L = \Delta L_1$ for single-tier; $\Delta L = \Delta L_2$ for multi-tier in direct comparisons.
:::
:::

::: {.footer}
Experiments
:::

::: {.notes}
- Along with the smaller Abilene topology.
:::


## Across Topologies {.smaller}

![Delay reduction factor normalized to delay with no caching ($\omega=0$). $\gamma=0.75$, $\lambda=15$.](images/alltops_075_15.svg){width=70% height=70% fig-align="center"}

![Cache capacity allocation at minimum delay point within budget.<br>All combinations ($\Delta L_1$,$\Delta L_2$) within maximum budget of $\Delta L=8$ tested.](images/alltops_075_15_table.svg){width=70% height=70% fig-align="center"}

::: {.footer}
Experiments
:::

::: {.notes}
- Though of course we ran experiments over many different topologies.
- This slide shows a much expanded version of results that were seen in the proposal, where we've tested each possible combination of capacity allocations across the two tiers given the maximum budget.
- We're disregarding penalty weights for the time being here, and the values represent the reduction in delay normalized to delay with no caching.
- One takeaway from the figure is that our approach which is listed as MVIP outperforms most baselines across the board, with LFU being a close competitor.
- Another important takeaway is seen from the table, where we observe MVIP tends to hit its best delay point with a larger allocation in the second tier compared to baselines.
:::

## Request Pattern Parameters

::: columns
::: {.column width="50%"}
![$\gamma=0.75$](images/mt_req_geant_delay.svg){width=90% height=90% fig-align="center"}
:::
::: {.column width="50%"}
![$\lambda=15$](images/zipf_geant_15_delay.svg){width=90% height=90% fig-align="center"}
:::
:::

::: {style="font-size: 80%; text-align: center"}
Geant; delay with increasing $\lambda$ and $\gamma$ ($\omega=0$).

$\Delta L_1 = 8$; $\Delta L_2$ chosen based on minimum delay points for two-tier.
:::

::: {.footer}
Experiments
:::

::: {.notes}
- Moving on, we also observed trends regarding the impact of traffic volume and request pattern.
- Here, single-tiered implementations of baseline policies are also compared, VIP included.
- The comparison is such that maximum capacity budget is allocated for these implementations.
- In contrast, for two-tiered counterpats, only the second tier is expanded, until it is no longer effective in reducing delay for the given policy.
- In this way, we observed our approach again leads in delay performance across the board.
- A notable outcome is that, given the same budget, it can obtain roughly 30-40\% improved delay compared to single-tiered VIP.
:::


## Delay-Penalty Trade-off

::: columns
::: {.column width="50%"}
![$\gamma=0.75$](images/mt_pen_geant_15_075.svg)
:::
::: {.column width="50%"}
![$\gamma=0.5$](images/mt_pen_geant_15_05.svg)
:::
:::

::: {style="font-size: 80%; text-align: center"}
Geant best delay vs. penalty curves for MVIP and PA-LFU policies.

$\lambda=15$. $\omega > 0$, decreases to the right.
:::

::: {.footer}
Experiments
:::

::: {.notes}
- Now, onto a setting where penalty importance weight is considered. These results are likely familiar from the proposal review.
- Here we're comparing the MVIP approach with the penalty-aware adaptation of the LFU policy in the Geant network.
- While we see pretty even curves between the two for a more skewed popularity distribution, we note that MVIP maintains a wider edge when the Zipf parameter is smaller.
- Another notable outcome is how at a certain point, reducing the importance of penalties actually impacts delay negatively.
- This is seen more clearly for the LFU adaptation.
- Basically, at such a point, we're no longer just allowing higher value replacements for the benefit of better performance, but we're allowing too many low benefit replacements which overwhelm the slower caches with constant rewrites, causing increased delay. 
:::

## Delay-Penalty Trade-off {visibility="uncounted"}

::: columns
::: {.column width="50%"}
![$\gamma=0.75$](images/mt_pen_abilene_15_075.svg)
:::
::: {.column width="50%"}
![$\gamma=0.5$](images/mt_pen_abilene_15_05.svg)
:::
:::

::: {style="font-size: 80%; text-align: center"}
Abilene best delay vs. penalty curves for MVIP and PA-LFU policies.

$\lambda=15$. $\omega > 0$, decreases to the right.
:::

::: {.footer}
Experiments
:::

::: {.notes}
- We can observe similar trends in the Abilene topology as well.
:::

## Interest Aggregation

::: columns
::: {.column width="50%"}
![$\gamma=0.75$](images/mt_req_ia_vip_geant_delay.svg)
:::
::: {.column width="50%"}
![$\lambda=30$](images/zipf_ia_geant_delay.svg)
:::
:::

::: {style="font-size: 80%; text-align: center"}
VIP vs. MVIP, with and without interest aggregation (IA).

$L_{n_1}=5$, $L_{n_2}=200$ for MVIP and $L_{n_1}=13$ for VIP.
:::

::: {.footer}
Experiments
:::

## Interest Aggregation {visibility="uncounted"}

::: columns
::: {.column width="50%"}
![$\gamma=0.75$](images/mt_req_ia_only_vip_geant_delay.svg)
:::
::: {.column width="50%"}
![$\lambda=30$](images/zipf_ia_only_vip_geant_30_delay.svg)
:::
:::

::: {style="font-size: 80%; text-align: center"}
VIP vs. MVIP, with and without interest aggregation (IA).

$L_{n_1}=5$, $L_{n_2}=200$ for MVIP and $L_{n_1}=13$ for VIP.
:::

::: {.footer}
Experiments
:::

## Interest Aggregation

::: columns
::: {.column width="50%"}
![VIP/MVIP vs. LFU](images/size_comp_ia_geant_075_50_delay.svg)
:::
::: {.column width="50%"}
![Other baselines](images/size_comp_ia_geant_075_50_baselines_delay.svg)
:::
:::

::: {style="font-size: 80%; text-align: center"}
Delay with increasing cache capacities under IA ($\omega=0$). 

Geant, $\gamma=0.75$, $\lambda=50$.
:::

::: {.footer}
Experiments
:::

::: {.notes}
- Placeholder.
:::

## Interest Aggregation

::: columns
::: {.column width="50%"}
![Cache hits](images/size_comp_ia_geant_075_50_hits.svg)
:::
::: {.column width="50%"}
![Second tier hits vs. reads](images/size_comp_ia_geant_075_50_hits_reads_tier2.svg)
:::
:::

::: {style="font-size: 80%; text-align: center"}
Cache hits and second tier reads issued under IA for multi-tiered cases.

Geant; $\lambda=50$, $\gamma=0.75$.
:::

::: {.footer}
Experiments
:::

::: {.notes}
- Placeholder.
:::

## Interest Aggregation

::: columns
::: {.column width="50%"}
![$\gamma=0.75$](images/mt_pen_ia_geant_30_075.svg)
:::
::: {.column width="50%"}
![$\gamma=0.5$](images/mt_pen_ia_geant_30_05.svg)
:::
:::

::: {style="font-size: 80%; text-align: center"}
Delay vs. penalty curves, MVIP vs. PA-LFU with and without IA.

$\Delta L_2$ based on min. delay point, $\lambda=30$. $\omega$ decreases to the right.
:::

::: {.footer}
Experiments
:::

::: {.notes}
- Placeholder.
:::

## Lessons Learned

- Interest aggregation is powerful; added tier can still have benefits.
- When using slower tiers, more cache hits is not always better.
- Penalizing cache operations not only minimizes costs, but *can improve* performance.
- MVIP is an effective policy outperforming baselines; LFU with LRT is very close.

::: {.footer}
Experiments
:::