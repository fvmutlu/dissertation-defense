## Setup {.smaller}

-  **Content Sources:** $\mc{S}(k)$ are selected uniformly at random.
-  **Routing:** *Any shortest path* (in number of hops) between $n$ and $\mc{S}(k)$.
-  **Cache Tiers:** Fixed two cache tiers at every node, fast and slow.
-  **Requests:** Requests generated for 100 simulation seconds, run terminates when all requests are resolved.
    - Poisson arrivals with rate $\lambda$ at each node; popularity distribution is Zipf with parameter $\gamma$.
- **Baselines:** LCE admission, LRT forwarding, adapted replacements:
    - LRU, FIFO, UNIF, LFU + single-tier VIP.
    - Penalty Aware LFU (PA-LFU) for trade-off baseline; uses *cache benefit* metric with frequency.

::: {.footer}
Experiments
:::

::: {.notes}
- Let's begin by outlining some basic mechanics of the experiments.
- We have content sources distributed for each object uniformly at random around the network.
- We fix routing to *any shortest path*.
- We deploy the same two cache tiers at each node, one fast and one slow tier.
- Request arrivals are Poisson processes with rate $\lambda$ at each node, where the requested object for each arrival is determined by the Zipf distribution with parameter $\gamma$.
- We have several caching policy baselines, both in their traditional single-tier versions including VIP, and their adapted counter parts for the two-tiered cases.
- The LFU baseline has a further adaptation to be penalty-aware, using a cache benefit metric similar to our policy but based on its frequency measure.
- The non-VIP baselines are paired with a Least Response Time forwarding scheme.
:::

## Setup {.smaller visibility="uncounted"}

::: columns
::: {.column width="50%"}
![GÃ‰ANT GN4-3N (34 nodes) [^geant]](images/geant.svg){width="100%"}
:::
::: {.column width="50%" style="font-size:75%;"}
| Parameter | Value |
|:-:|:-:|
|Number of objects in catalog $\mc{K}$|2000|
|Bandwidth of each link (objects/sec) |10|
|Read / write rate of tier 1 (objects/sec) |25, 25|
|Read / write rate of tier 2 (in objects/sec) |10, 5|
|Admission / eviction costs of tier 1|4, 2|
|Admission / eviction costs of tier 2|2, 1|
: Common parameters for all scenarios {#tbl-params tbl-colwidths="[80,20]"}

[Cache capacity budget]{.body-highlight} $\Delta L = \Delta L_1 + \Delta L_2$.

$L_{n_1} = 5 + \Delta L_1$, $L_{n_2} = 25 \times \Delta L_2$.

$\Delta L = \Delta L_1$ for single-tier; $\Delta L = \Delta L_2$ for multi-tier in direct comparisons.
:::
:::

[^geant]: The GN4 Phase 3 Network project, https://network.geant.org/gn4-3n/

::: {.footer}
Experiments
:::

::: {.notes}
- And here are some of the common parameters used in the experiments.
- There are some notable things here I'll point out.
- Firstly, the fast cache tier is more than twice as fast as the slow one, which has read bandwidth equivalent to a single link.
- But we also model write rates, and following general characteristics of flash, the slow tier's is equivalent to half its read rate.
- This has implications regarding frequent replacements and is part of the reason we want to penalize them.
- Secondly, to enable comparisons with single-tiered implementations we're introducing a notion of cache capacity budget.
- This relates to purchasing costs for memory and flash, and allows the evaluation of what sort of distribution of investment can yield better results for a given policy.
- Of course, the same investment would get us a lot more capacity in the slow tier.
- We're also seeing the Geant topology which will be the representative one we use for looking at some trends.
:::

## Setup {.smaller visibility="uncounted"}

::: columns
::: {.column width="50%"}
![Abilene (11 nodes)](images/abilene.svg){width="100%" style="display: block; margin-top: 120px;"}
:::
::: {.column width="50%" style="font-size:75%;"}
| Parameter | Value |
|:-:|:-:|
|Number of objects in catalog $\mc{K}$|2000|
|Bandwidth of each link (objects/sec) |10|
|Read / write rate of tier 1 (objects/sec) |25, 25|
|Read / write rate of tier 2 (in objects/sec) |10, 5|
|Admission / eviction costs of tier 1|4, 2|
|Admission / eviction costs of tier 2|2, 1|
: Common parameters for all scenarios {#tbl-params tbl-colwidths="[80,20]"}

[Cache capacity budget]{.body-highlight} $\Delta L = \Delta L_1 + \Delta L_2$.

$L_{n_1} = 5 + \Delta L_1$, $L_{n_2} = 25 \times \Delta L_2$.

$\Delta L = \Delta L_1$ for single-tier; $\Delta L = \Delta L_2$ for multi-tier in direct comparisons.
:::
:::

::: {.footer}
Experiments
:::

::: {.notes}
- Along with the smaller Abilene topology.
:::


## Across Topologies {.smaller}

![Delay reduction factor normalized to delay with no caching ($\omega=0$). $\gamma=0.75$, $\lambda=15$.](images/alltops_075_15.svg){width=70% height=70% fig-align="center"}

![Cache capacity allocation at minimum delay point within budget.<br>All combinations ($\Delta L_1$,$\Delta L_2$) within maximum budget of $\Delta L=8$ tested.](images/alltops_075_15_table.svg){width=70% height=70% fig-align="center"}

::: {.footer}
Experiments
:::

::: {.notes}
- Though of course we ran experiments over many different topologies.
- This slide shows a much expanded version of results that were seen in the proposal, where we've tested each possible combination of capacity allocations across the two tiers given the maximum budget.
- We're disregarding penalty weights for the time being here, and the values represent the reduction in delay normalized to delay with no caching.
- One takeaway from the figure is that our approach which is listed as MVIP outperforms most baselines across the board, with LFU being a close competitor.
- Another important takeaway is seen from the table, where we observe MVIP tends to hit its best delay point with a larger allocation in the second tier compared to baselines.
:::

## Request Pattern Parameters

::: columns
::: {.column width="50%"}
![$\gamma=0.75$](images/mt_req_geant_delay.svg){width=90% height=90% fig-align="center"}
:::
::: {.column width="50%"}
![$\lambda=15$](images/zipf_geant_15_delay.svg){width=90% height=90% fig-align="center"}
:::
:::

::: {style="font-size: 80%; text-align: center"}
Geant; delay with increasing $\lambda$ and $\gamma$ ($\omega=0$).

$\Delta L_1 = 8$; $\Delta L_2$ chosen based on minimum delay points for two-tier.
:::

::: {.footer}
Experiments
:::

::: {.notes}
- Moving on, we also observed trends regarding the impact of traffic volume and request pattern.
- Here, single-tiered implementations of baseline policies are also compared, VIP included.
- The comparison is such that maximum capacity budget is allocated for these implementations.
- In contrast, for two-tiered counterpats, only the second tier is expanded, until it is no longer effective in reducing delay for the given policy.
- In this way, we observed our approach again leads in delay performance across the board.
- A notable outcome is that, given the same budget, it can obtain roughly 30-40\% improved delay compared to single-tiered VIP.
:::


## Delay-Penalty Trade-off

::: columns
::: {.column width="50%"}
![$\gamma=0.75$](images/mt_pen_geant_15_075.svg)
:::
::: {.column width="50%"}
![$\gamma=0.5$](images/mt_pen_geant_15_05.svg)
:::
:::

::: {style="font-size: 80%; text-align: center"}
Geant best delay vs. penalty curves for MVIP and PA-LFU policies.

$\lambda=15$. $\omega > 0$, decreases to the right.
:::

::: {.footer}
Experiments
:::

::: {.notes}
- Now, onto a setting where penalty importance weight is considered. These results are likely familiar from the proposal review.
- Here we're comparing the MVIP approach with the penalty-aware adaptation of the LFU policy in the Geant network.
- While we see pretty even curves between the two for a more skewed popularity distribution, we note that MVIP maintains a wider edge when the Zipf parameter is smaller.
- Another notable outcome is how at a certain point, reducing the importance of penalties actually impacts delay negatively.
- This is seen more clearly for the LFU adaptation.
- Basically, at such a point, we're no longer just allowing higher value replacements for the benefit of better performance, but we're allowing too many low benefit replacements which overwhelm the slower caches with constant rewrites, causing increased delay. 
:::

## Delay-Penalty Trade-off {visibility="uncounted"}

::: columns
::: {.column width="50%"}
![$\gamma=0.75$](images/mt_pen_abilene_15_075.svg)
:::
::: {.column width="50%"}
![$\gamma=0.5$](images/mt_pen_abilene_15_05.svg)
:::
:::

::: {style="font-size: 80%; text-align: center"}
Abilene best delay vs. penalty curves for MVIP and PA-LFU policies.

$\lambda=15$. $\omega > 0$, decreases to the right.
:::

::: {.footer}
Experiments
:::

::: {.notes}
- We can observe similar trends in the Abilene topology as well.
:::

## Interest Aggregation

::: columns
::: {.column width="50%"}
![$\gamma=0.75$](images/mt_req_ia_vip_geant_delay.svg)
:::
::: {.column width="50%"}
![$\lambda=30$](images/zipf_ia_geant_delay.svg)
:::
:::

::: {style="font-size: 80%; text-align: center"}
VIP vs. MVIP, with and without interest aggregation (IA).

$L_{n_1}=5$, $L_{n_2}=200$ for MVIP and $L_{n_1}=13$ for VIP.
:::

::: {.footer}
Experiments
:::

## Interest Aggregation {visibility="uncounted"}

::: columns
::: {.column width="50%"}
![$\gamma=0.75$](images/mt_req_ia_only_vip_geant_delay.svg)
:::
::: {.column width="50%"}
![$\lambda=30$](images/zipf_ia_only_vip_geant_30_delay.svg)
:::
:::

::: {style="font-size: 80%; text-align: center"}
VIP vs. MVIP, with aggregation (IA).

$L_{n_1}=5$, $L_{n_2}=200$ for MVIP and $L_{n_1}=13$ for VIP.
:::

::: {.footer}
Experiments
:::

## Interest Aggregation

::: columns
::: {.column width="50%"}
![Cache hits](images/size_comp_ia_geant_075_50_hits.svg)
:::
::: {.column width="50%"}
![Second tier hits vs. reads](images/size_comp_ia_geant_075_50_hits_reads_tier2.svg)
:::
:::

::: {style="font-size: 80%; text-align: center"}
Cache hits and second tier reads issued under IA for multi-tiered cases.

Geant; $\lambda=50$, $\gamma=0.75$.
:::

::: {.footer}
Experiments
:::

::: {.notes}
- But there's another reason I wanted to highlight interest aggregation specifically.
- In the traditional, line-rate view of caching where caches yield data more or less immediately, it was shown in literature that caching can offset the benefits of aggregation, especially with increasing capacities.
- This makes sense, as caching will eliminate the likelihood of interests waiting in queues and getting combined as a result.
- But we've underlined the non-blocking operation of the slow tiers in our model, which actually allows aggregation opportunities since new requests for an object can arrive while that object is already being read from the cache in the background.
- So on the left here we're seeing a comparison of total cache hits for the multi-tiered versions of evaluated policies as second tier cache capacity increases.
- And on the right, we're seeing the second tier hits, along with the reads issued to respond to those hits.
- What the right figure indicates is that such aggregation does indeed occur in the slow tier.
- For some baselines, we can see that a great number of hits were satisfied with a much smaller amount of reads.
- For MVIP and LFU, we observe that while this ratio is smaller, it does grow with increasing cache capacity.
- So given slow cache tiers, the interplay of caching and aggregation can be positive.
:::

## Interest Aggregation

::: columns
::: {.column width="50%"}
![VIP/MVIP vs. LFU](images/size_comp_ia_geant_075_50_delay.svg)
:::
::: {.column width="50%"}
![Other baselines](images/size_comp_ia_geant_075_50_baselines_delay.svg)
:::
:::

::: {style="font-size: 80%; text-align: center"}
Delay with increasing cache capacities under IA ($\omega=0$). 

Geant, $\gamma=0.75$, $\lambda=50$.
:::

::: {.footer}
Experiments
:::

::: {.notes}
- Unfortunately, however, this positive impact does not necessarily yield immense benefits as we can see in these delay figures from the same results.
- In fact, the baselines with the largest read aggregation ratios are also the ones doing worst, in fact doing worse than no caching at all.
- Because a large potential for aggregation is also a direct indicator for large queueing delays, which can be debilitating as we said at the beginning.
- So while aggregation is a positive in our scenario, it is not a cure to the transfer rate limitations.
:::

## Interest Aggregation

::: columns
::: {.column width="50%"}
![$\gamma=0.75$](images/mt_pen_ia_geant_30_075.svg)
:::
::: {.column width="50%"}
![$\gamma=0.5$](images/mt_pen_ia_geant_30_05.svg)
:::
:::

::: {style="font-size: 80%; text-align: center"}
Delay vs. penalty curves, MVIP vs. PA-LFU with and without IA.

$\Delta L_2$ based on min. delay point, $\lambda=30$. $\omega$ decreases to the right.
:::

::: {.footer}
Experiments
:::

::: {.notes}
- We'll cap our exploration of aggregation with a comparison of delay-penalty trade-offs.
- Here we're comparing cases with and without aggregation, and aside from the order of magnitude reduction in delay, we also notice a shift in the trends.
- Specifically, we see that the regime where penalties increase along with delay due to redundant replacements is extended.
:::

## Lessons Learned

- Interest aggregation is powerful; added tier can still have benefits.
- When using slower tiers, more cache hits is not always better.
- Penalizing cache operations not only minimizes costs, but *can improve* performance.
- MVIP is an effective policy outperforming baselines; LFU with LRT is very close.

::: {.footer}
Experiments
:::