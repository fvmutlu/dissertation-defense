## Setup {.smaller}

::: {.incremental}
-  **Content Sources:** $\mc{S}(k)$ are selected uniformly at random.
-  **Routing:** *Any* shortest path (in number of hops) between $n$ and $\mc{S}(k)$.
-  **Cache Tiers:** Fixed two cache tiers at every node, fast and slow.
-  **Requests:** Requests generated for 100 simulation seconds, run terminates when all requests are resolved.
    - Poisson arrivals with rate $\lambda$ at each node; popularity distribution is Zipf with parameter $\gamma$.
- **Baselines:** Leave Copy Everywhere admission, Least Response Time forwarding, adapted replacements:
    - Least Recently Used (LRU), First-In-First-Out (FIFO), Uniform Random (UNIF), Least Frequently Used (LFU).
    - Penalty Aware LFU (PA-LFU) for trade-off baseline; uses *cache benefit* metric with frequency.
:::

::: {.footer}
Experiments
:::

## Setup {.smaller visibility="uncounted"}

![GÉANT GN4-3N (34 nodes) [^geant]](images/geant.svg){width="50%" height="50%"}

[^geant]: The GN4 Phase 3 Network project, https://network.geant.org/gn4-3n/

::: {.footer}
Experiments
:::

::: {.notes}
- Alright, so when it comes to experiment settings, I first have to stress that the experimentation space for this work is enormous
- So the results I'll show here are those that I think most succinctly show that our approach can deliver on our goals
:::

## Parameter Reference Table {.smaller}

*Cache capacity budget* $\Delta L = \Delta L_1 + \Delta L_2$ to enable comparisons with single-tier cases; based on the large purchasing cost ratio of DRAM to flash.

- Initial capacities are $L_{n_1} = 5$, $L_{n_2} = 0$.
- Added capacity for $\Delta L > 0$ is $L_{n_1} = 5 + \Delta L_1$, $L_{n_2} = 25 \times \Delta L_2$.

| Parameter | Value |
|:-:|:-:|
|Number of objects in catalog $\mc{K}$|2000|
|Bandwidth of each link $\abin$, in objects|10|
|Read rate of tier 1 at each $n$, in objects/sec|25|
|Admission and eviction costs of tier 1 at each $n$|4, 2|
|Read rate of tier 2 at each $n$, in objects/sec|10|
|Admission and eviction costs of tier 2 at each $n$|2, 1|
: Common parameters for all scenarios {#tbl-params tbl-colwidths="[85,15]"}

::: {.footer}
Experiments
:::

::: {.notes}
- As I said, there are a great number of parameters for these experiments and I'm not going to cover each one in the interest of time, but we have this table for reference if needed
:::

## Cache Capacity

![](images/size_comp_geant_all_075_15.svg){width=55% height=55% fig-align="center"}

::: {style="font-size: 80%; text-align: center"}
Delay with increasing cache capacities. Geant; $\gamma=0.75$, $\lambda=15$.

$\Delta L = \Delta L_1$ for single-tier; $\Delta L = \Delta L_2$ for multi-tier.
:::

::: {.footer}
Experiments
:::

::: {.notes}
- Placeholder.
:::

## Cache Capacity

::: columns
::: {.column width="47%"}
![Cache hits](images/size_comp_geant_075_15_hits.svg)
:::
::: {.column width="53%"}
![Second tier read delays](images/size_comp_geant_075_15_hit_delays.svg)
:::
:::

::: {style="font-size: 80%; text-align: center"}
Cache hits and delay due to cache reads. Geant; $\gamma=0.75$, $\lambda=15$.

$\Delta L = \Delta L_1$ for single-tier; $\Delta L = \Delta L_2$ for multi-tier.
:::

::: {.footer}
Experiments
:::

::: {.notes}
- Placeholder.
:::

## Request Pattern Parameters

::: columns
::: {.column width="50%"}
![$\gamma=0.75$](images/mt_req_geant_delay.svg){width=90% height=90% fig-align="center"}
:::
::: {.column width="50%"}
![$\lambda=15$](images/zipf_geant_15_delay.svg){width=90% height=90% fig-align="center"}
:::
:::

::: {style="font-size: 80%; text-align: center"}
Delay with increasing $\lambda$ and $\gamma$.

Geant; $L_{n_2}$ chosen based on minimum delay points for multi-tier.
:::

::: {.footer}
Experiments
:::

## More Topologies {.smaller}

![Delay reduction factor normalized to delay with no caching.](images/alltops_075_15.svg){width=70% height=70% fig-align="center"}

![Cache capacity allocation in minimum delay point within budget.<br>All combinations ($\Delta L_1$,$\Delta L_2$) within maximum budget of $\Delta L=8$ tested.](images/alltops_075_15_table.svg){width=70% height=70% fig-align="center"}

::: {.footer}
Experiments
:::

::: {.notes}
- Placeholder.
:::

## Delay-Penalty Trade-off

::: columns
::: {.column width="50%"}
![$\gamma=0.75$](images/mt_pen_geant_15_075.svg)
:::
::: {.column width="50%"}
![$\gamma=0.5$](images/mt_pen_geant_15_05.svg)
:::
:::

::: {style="font-size: 80%; text-align: center"}
Delay vs. penalty curves for MVIP and PA-LFU policies. Geant; $\lambda=15$.

$\omega$ decreases to the right.
:::

::: {.footer}
Experiments
:::

::: {.notes}
- Placeholder.
:::

## Interest Aggregation

::: {.incremental}
• Further requests (interests) are aggregated if there is already one in-flight.

[• Returning data satisfies all aggregated requests.]{.fragment fragment-index=2}
:::

::: {.r-stack}
![](images/ia_1.svg){width="648" height="360" fig-align="center"}

![](images/ia_2.svg){.fragment fragment-index=2 width="648" height="360" fig-align="center"}
:::

::: {.footer}
Experiments
:::

## Interest Aggregation {visibility="uncounted"}

• Further requests (interests) are aggregated if there is already one in-flight.

• Returning data satisfies all aggregated requests.

• Traditional view (*line-rate*) of caching suggests caching offsets aggregation benefit and effect is proportional with capacity[^agg].

[^agg]: "Characterizing interest aggregation in content-centric networks.", IFIP Networking, 2016 @dabirmoghaddam2016characterizing.

::: {.footer}
Experiments
:::

## Interest Aggregation

::: columns
::: {.column width="50%"}
![$\gamma=0.75$](images/mt_req_ia_vip_geant_delay.svg)
:::
::: {.column width="50%"}
![$\lambda=30$](images/zipf_ia_geant_delay.svg)
:::
:::

::: {style="font-size: 80%; text-align: center"}
VIP vs. MVIP, with and without interest aggregation (IA).

Geant; $L_{n_1}=5$, $L_{n_2}=200$ for MVIP and $L_{n_1}=13$ for VIP.
:::

::: {.footer}
Experiments
:::

::: {.notes}
- The scale here is a bit deceptive, but in terms of ratios, overall improvement from multi-tiered caching is roughly 40% with no aggregation and 20% with aggregation.
:::

## Interest Aggregation

::: columns
::: {.column width="50%"}
![VIP/MVIP vs. LFU](images/size_comp_ia_geant_075_50_delay.svg)
:::
::: {.column width="50%"}
![Other baselines](images/size_comp_ia_geant_075_50_baselines_delay.svg)
:::
:::

::: {style="font-size: 80%; text-align: center"}
Delay with increasing cache capacities under IA. Geant; $\gamma=0.75$, $\lambda=50$.

$\Delta L = \Delta L_1$ for single-tier; $\Delta L = \Delta L_2$ for multi-tier.
:::

::: {.footer}
Experiments
:::

::: {.notes}
- Placeholder.
:::

## Interest Aggregation

::: columns
::: {.column width="50%"}
![Cache hits](images/size_comp_ia_geant_075_50_hits.svg)
:::
::: {.column width="50%"}
![Second tier hits vs. reads](images/size_comp_ia_geant_075_50_hits_reads_tier2.svg)
:::
:::

::: {style="font-size: 80%; text-align: center"}
Number of cache hits and cache reads issued under IA.

Geant; $\lambda=50$, $\gamma=0.75$.
:::

::: {.footer}
Experiments
:::

::: {.notes}
- Placeholder.
:::

## Interest Aggregation

::: columns
::: {.column width="50%"}
![$\gamma=0.75$](images/mt_pen_ia_geant_30_075.svg)
:::
::: {.column width="50%"}
![$\gamma=0.5$](images/mt_pen_ia_geant_30_05.svg)
:::
:::

::: {style="font-size: 80%; text-align: center"}
Delay vs. penalty curves, MVIP vs. PA-LFU with and without IA.

Geant; $\lambda=30$, $\omega$ decreases to the right.
:::

::: {.footer}
Experiments
:::

::: {.notes}
- Placeholder.
:::