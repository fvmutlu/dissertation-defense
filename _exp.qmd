## Setup {.smaller}

-  **Content Sources:** $\mc{S}(k)$ are selected uniformly at random.
-  **Routing:** *Any shortest path* (in number of hops) between $n$ and $\mc{S}(k)$.
-  **Cache Tiers:** Fixed two cache tiers at every node, fast and slow.
-  **Requests:** Requests generated for 100 simulation seconds, run terminates when all requests are resolved.
    - Poisson arrivals with rate $\lambda$ at each node; popularity distribution is Zipf with parameter $\gamma$.
- **Baselines:** LCE admission, LRT forwarding, adapted replacements:
    - Least Recently Used (LRU), First-In-First-Out (FIFO), Uniform Random (UNIF), Least Frequently Used (LFU).
    - Penalty Aware LFU (PA-LFU) for trade-off baseline; uses *cache benefit* metric with frequency.

::: {.footer}
Experiments
:::

## Setup {.smaller visibility="uncounted"}

::: columns
::: {.column width="50%"}
![GÃ‰ANT GN4-3N (34 nodes) [^geant]](images/geant.svg){width="100%"}
:::
::: {.column width="50%" style="font-size:75%;"}
| Parameter | Value |
|:-:|:-:|
|Number of objects in catalog $\mc{K}$|2000|
|Bandwidth of each link (objects/sec) |10|
|Read / write rate of tier 1 (objects/sec) |25, 25|
|Admission / eviction costs of tier 1|4, 2|
|Read / write rate of tier 2 (in objects/sec) |10, 5|
|Admission / eviction costs of tier 2|2, 1|
: Common parameters for all scenarios {#tbl-params tbl-colwidths="[80,20]"}
:::
:::

[^geant]: The GN4 Phase 3 Network project, https://network.geant.org/gn4-3n/

::: {.footer}
Experiments
:::

::: {.notes}
- Alright, so when it comes to experiment settings, I first have to stress that the experimentation space for this work is enormous
- So the results I'll show here are those that I think most succinctly show that our approach can deliver on our goals
:::

## Parameter Reference Table {.smaller}

| Parameter | Value |
|:-:|:-:|
|Number of objects in catalog $\mc{K}$|2000|
|Bandwidth of each link $\abin$, in objects|10|
|Read / write rate of tier 1 at each $n$, in objects/sec|25, 25|
|Admission / eviction costs of tier 1 at each $n$|4, 2|
|Read / write rate of tier 2 at each $n$, in objects/sec|10, 5|
|Admission / eviction costs of tier 2 at each $n$|2, 1|
: Common parameters for all scenarios {#tbl-params tbl-colwidths="[85,15]"}

::: {.footer}
Experiments
:::

::: {.notes}
- As I said, there are a great number of parameters for these experiments and I'm not going to cover each one in the interest of time, but we have this table for reference if needed
:::

## Cache Capacity

![](images/size_comp_geant_all_075_15.svg){width="100%" fig-align="center"}

::: {style="font-size: 80%; text-align: center"}
Delay with increasing cache capacities ($\omega=0$). $\gamma=0.75$, $\lambda=15$. 
:::

::: {style="font-size: 60%; text-align: center"}
[Cache capacity budget]{.body-highlight} $\Delta L = \Delta L_1 + \Delta L_2$. $L_{n_1} = 5 + \Delta L_1$, $L_{n_2} = 25 \times \Delta L_2$.

$\Delta L = \Delta L_1$ for single-tier; $\Delta L = \Delta L_2$ for multi-tier.
:::

::: {.footer}
Experiments
:::

::: {.notes}
- At the beginning we mentioned that due to the slow speeds of the added tier more capacity may not necessarily be beneficial.
- So one of the most important questions for our experimentation is, how large of a cache can we benefit from? This setting explores that.
- For the time being we ignore the penalty importance weight, we'll get to that later.
- We compare all baselines, with both single and two-tiered versions.
- We see that trends differ across policies, and for multi-tiered versions of some baselines, there's a minimum delay point after which the added capacity becomes detrimental.
- While such a point would also exist for MVIP, in this setting it is not met within the budget.
- The minimum delay point comes at a higher allocation budget for MVIP compared with LRU and LFU, but the total delay is roughly 15% better than LFU's minimal point.
- Another significant result is that, with the same budget, our multi-tiered VIP approach has about 40% reduced delay compared to the single-tier counterpart. 
:::

## Cache Capacity

::: columns
::: {.column width="47%"}
![Cache hits](images/size_comp_geant_075_15_hits.svg)
:::
::: {.column width="53%"}
![Second tier read delays](images/size_comp_geant_075_15_hit_delays.svg)
:::
:::

::: {style="font-size: 80%; text-align: center"}
Cache hits and delay due to cache reads for multi-tiered versions.
:::

::: {.footer}
Experiments
:::

::: {.notes}
- Placeholder.
:::

## Request Pattern Parameters

::: columns
::: {.column width="50%"}
![$\gamma=0.75$](images/mt_req_geant_delay.svg){width=90% height=90% fig-align="center"}
:::
::: {.column width="50%"}
![$\lambda=15$](images/zipf_geant_15_delay.svg){width=90% height=90% fig-align="center"}
:::
:::

::: {style="font-size: 80%; text-align: center"}
Delay with increasing $\lambda$ and $\gamma$ ($\omega=0$).

$\Delta L_1 = 8$; $\Delta L_2$ chosen based on minimum delay points for multi-tier.
:::

::: {.footer}
Experiments
:::

::: {.notes}
- Placeholder.
:::

## More Topologies {.smaller}

![Delay reduction factor normalized to delay with no caching ($\omega=0$). $\gamma=0.75$, $\lambda=15$.](images/alltops_075_15.svg){width=70% height=70% fig-align="center"}

![Cache capacity allocation at minimum delay point within budget.<br>All combinations ($\Delta L_1$,$\Delta L_2$) within maximum budget of $\Delta L=8$ tested.](images/alltops_075_15_table.svg){width=70% height=70% fig-align="center"}

::: {.footer}
Experiments
:::

::: {.notes}
- Placeholder.
:::


## Delay-Penalty Trade-off

::: columns
::: {.column width="50%"}
![$\gamma=0.75$](images/mt_pen_geant_15_075.svg)
:::
::: {.column width="50%"}
![$\gamma=0.5$](images/mt_pen_geant_15_05.svg)
:::
:::

::: {style="font-size: 80%; text-align: center"}
Delay vs. penalty curves for MVIP and PA-LFU policies. $\lambda=15$.

$\omega > 0$, decreases to the right.
:::

::: {.footer}
Experiments
:::

::: {.notes}
- Placeholder.
:::

## Interest Aggregation

::: columns
::: {.column width="50%"}
![$\gamma=0.75$](images/mt_req_ia_vip_geant_delay.svg)
:::
::: {.column width="50%"}
![$\lambda=30$](images/zipf_ia_geant_delay.svg)
:::
:::

::: {style="font-size: 80%; text-align: center"}
VIP vs. MVIP, with and without interest aggregation (IA).

$L_{n_1}=5$, $L_{n_2}=200$ for MVIP and $L_{n_1}=13$ for VIP.
:::

::: {.footer}
Experiments
:::

## Interest Aggregation {visibility="uncounted"}

::: columns
::: {.column width="50%"}
![$\gamma=0.75$](images/mt_req_ia_only_vip_geant_delay.svg)
:::
::: {.column width="50%"}
![$\lambda=30$](images/zipf_ia_only_vip_geant_30_delay.svg)
:::
:::

::: {style="font-size: 80%; text-align: center"}
VIP vs. MVIP, with and without interest aggregation (IA).

$L_{n_1}=5$, $L_{n_2}=200$ for MVIP and $L_{n_1}=13$ for VIP.
:::

::: {.footer}
Experiments
:::

## Interest Aggregation

::: columns
::: {.column width="50%"}
![VIP/MVIP vs. LFU](images/size_comp_ia_geant_075_50_delay.svg)
:::
::: {.column width="50%"}
![Other baselines](images/size_comp_ia_geant_075_50_baselines_delay.svg)
:::
:::

::: {style="font-size: 80%; text-align: center"}
Delay with increasing cache capacities under IA. $\gamma=0.75$, $\lambda=50$.

$\Delta L = \Delta L_1$ for single-tier; $\Delta L = \Delta L_2$ for multi-tier.
:::

::: {.footer}
Experiments
:::

::: {.notes}
- Placeholder.
:::

<!-- ## Interest Aggregation

::: columns
::: {.column width="50%"}
![Cache hits](images/size_comp_ia_geant_075_50_hits.svg)
:::
::: {.column width="50%"}
![Second tier hits vs. reads](images/size_comp_ia_geant_075_50_hits_reads_tier2.svg)
:::
:::

::: {style="font-size: 80%; text-align: center"}
Cache hits and second tier reads issued under IA for multi-tiered cases.

Geant; $\lambda=50$, $\gamma=0.75$.
:::

::: {.footer}
Experiments
:::

::: {.notes}
- Placeholder.
::: -->

## Interest Aggregation

::: columns
::: {.column width="50%"}
![$\gamma=0.75$](images/mt_pen_ia_geant_30_075.svg)
:::
::: {.column width="50%"}
![$\gamma=0.5$](images/mt_pen_ia_geant_30_05.svg)
:::
:::

::: {style="font-size: 80%; text-align: center"}
Delay vs. penalty curves, MVIP vs. PA-LFU with and without IA.

$\Delta L_2$ based on min. delay point, $\lambda=30$. $\omega$ decreases to the right.
:::

::: {.footer}
Experiments
:::

::: {.notes}
- Placeholder.
:::

## Lessons Learned

- When using slower added tiers, more cache hits is not always better.
- MVIP is an effective policy outperforming baselines; LFU with LRT is very close.
- Penalizing cache operations not only minimizes costs, but can *improve* performance.
- Interest aggregation is powerful; less cache is needed, but multi-tier can still have benefits.

::: {.footer}
Experiments
:::

::: {.notes}
- In the settings we cover, LFU with LRT performs closely with our approach, though MVIP takes a slight edge in general, with the margins being larger for less skewed popularity distributions.
:::