## Motivation: Data-Intensive Science {.smaller}

::: columns
::: {.column width="50%"}
![](images/data_plot.svg)

-  Data-intensive science experiments process huge amounts of data
-  Data growth predictions keep moving the bar up

:::
::: {.column width="50%"}
![](images/lhcopen_data.svg)

-  Data is continuously distributed across the world for research
-  In one year of LHC running, over an exabyte of data is accessed

:::
:::

::: {.footer}
Introduction
:::

::: {.notes}
- To expand on my motivation, I'll first talk about the use case that initially inspired my work.
- As you know, large science programs in fields like high-energy physics, genomics etc. deal with experiments that produce huge amounts of data.
- For instance, for the LHC, in one year, over an exabyte of data is accessed.
- This data also has to constantly be distributed across the world for research, which is a significant networking challenge.
:::

## Motivation: Data-Intensive Science {.smaller}

*"N-DISE: NDN-based Data Distribution for Large-Scale Data-Intensive Science"*:

-   20 gigabytes ($2 \times 10^{10}$ bytes) of DRAM cache space at each node
-   "Increasing capacities via NVMe SSDs require novel caching strategies" (NDN-DPDK forwarder paper[^ndndpdk])

::: {.footer}
Introduction
:::

::: {.notes}
- To address this, our lab led an effort to build a state-of-the-art, Named Data Networking based data distribution system, which you can read about in the paper we published in ICN 2022.
- The point here is, even in this state-of-the-art prototype, we were only able to allocate a small amount of cache space at each router. As you can see it is a drop in the bucket compared to the volumes we expect to be working with.
- I also want to briefly mention that, in a separate paper that describes a core technology that enabled the N-DISE system (the NDN-DPDK forwarder), the problem of scaling cache capacities and the need for specialized policies for operating larger caches was specifically outlined as part of future outlook.
- So there's a gap here that motivated my work.
:::

## Motivation: Data-Intensive Science

-   20 gigabytes ($2 \times 10^{10}$ bytes) of DRAM cache space at each node
-   "Increasing capacities via NVMe SSDs require novel caching strategies" (NDN-DPDK forwarder paper[^ndndpdk])

[^ndndpdk]: "NDN-DPDK: NDN forwarding at 100 Gbps on commodity hardware", J. Shi, D. Pesavento, L. Benmohamed, ACM ICN 2020

::: {.footer}
Introduction
:::

::: {.notes}
- To address this, our lab led an effort to build a state-of-the-art, Named Data Networking based data distribution system, which you can read about in the paper we published in ICN 2022.
- The point here is, even in this state-of-the-art prototype, we were only able to allocate a small amount of cache space at each router. As you can see it is a drop in the bucket compared to the volumes we expect to be working with.
- I also want to briefly mention that, in a separate paper that describes a core technology that enabled the N-DISE system (the NDN-DPDK forwarder), the problem of scaling cache capacities and the need for specialized policies for operating larger caches was specifically outlined as part of future outlook.
- So there's a gap here that motivated my work.
:::