## Related Work: ICN

-  *"Multi-Terabyte and multi-Gbps information centric routers"*, G. Rossini, D. Rossi, M. Garetto, E. Leonardi, INFOCOM 2014
-  *"Hierarchical Content Stores in High-Speed ICN Routers: Emulation and Prototype Implementation"*, R. B. Mansilha, L. Saino, M. Barcellos, M. Gallo, E. Leonardi, D. Perino, D. Rossi, ICN 2015
-  *"Exploiting parallelism in hierarchical content stores for high-speed ICN routers"*, R. B. Mansilha, M. Barcellos, E. Leonardi, D. Rossi, COMNET 2017

::: {.footer}
Related Work
:::

::: {.notes}
- Now, to conclude the introduction, I want to point out some related work.
- This connected series of papers you see on the screen, which come from the same group that authored the one about coupling caching and forwarding that I just discussed, is the closest related work in my opinion.
- These papers lay out similar concerns about cache capacities and the relevant challenges.
- However, their work targets the system layer. They deal with lower level implementation details of how to integrate SSDs into the actual data structures that make up the caches in ICNs.
- In these works, while the authors rely on traditional cache admission and replacement policies, they also point out that specialized policies may result in better performant systems.
- They also do not consider the coupling of forwarding decisions.
:::

## Related Work: More ICN {.smaller}
-  *"A reality check for content centric networking"*, D. Perino, M. Varvello, SIGCOMM 2011
-  *"Toward terabyte-scale caching with SSD in a named data networking router"*, W. So, et al., ANCS 2014
-  *"On Designing Optimal Memory Damage Aware Caching Policies for Content-Centric Networks"*, S. Shukla, A. A. Abouzeid, WiOpt 2016
-  *"HCaching: High-Speed Caching for Information-Centric Networking"*, H. Li, et al., GLOBECOM 2017
-  *"A Split Architecture Approach to Terabyte-Scale Caching in a Protocol-Oblivious Forwarding Switch"*, L. Ding, et al., ToN 2017
-  *"NB-cache: non-blocking in-network caching for high-speed content routers"*, T. Pan, et al., IWQoS 2019

::: {.footer}
Related Work
:::

::: {.notes}
- Obviously, there are a lot more works that explore similar problems from different angles. I can't go into everything at the moment, but I still wanted to put these other works from the ICN literature in
:::

## Related Work: HSS

-  *"AutoTiering: automatic data placement manager in multi-tier all-flash datacenter"*, Z. Yang, et al., IPCCC 2017
-  *"Sibyl: Adaptive and extensible data placement in hybrid storage systems using online reinforcement learning"*, G. Singh, et al., ISCA 2022 

::: {.footer}
Related Work
:::

::: {.notes}
- As well as these works from the more tangentially related hybrid storage systems literature
:::

## Related Work: VIP

*"VIP: a framework for joint dynamic forwarding and caching in named data networks"*, E. M. Yeh, T. Ho, Y. Cui, M. Burd, R. Liu, D. Leong, ACM ICN 2014

-   Couples caching and forwarding
-   Models read rate of caches
-   Virtual and data plane separation makes algorithm simpler to operate
-   Lyapunov drift analysis integrates easily with costs

::: {.footer}
Related Work
:::

::: {.notes}
- Lastly, perhaps the single most important related work to cover here is the VIP paper by Prof. Yeh and co-authors, as my work directly extends this paper.
- So let me go over why that is.
- Recall the earlier paper I mentioned about coupling caching and forwarding; coincidentally the VIP paper also appeared in the same conference and proposed a strategy that does exactly that, which suits our goals.
- Furthermore, this paper includes the read capacities of cache devices in its system model, which is one of the parameters of primary concern to us.
- The core feature of this paper is a separation of the data plane mechanisms from the "virtual plane", where control decisions take place, which we also adopt as it makes building these strategies simpler.
- Finally, there is a technical benefit here because the analysis for VIP is based on Lyapunov drift, which has an extension called drift-plus-penalty, that gives us a direct venue for incorporating cache utilization costs.
:::